************************************************************************
cd
git clone http://github.com/czender/prv
************************************************************************
for drc in cns grd hire iusd job jrn ltr mny poetry rvw pr sdn trv www \
aca anl anv bxm c c++ ck cld crm crr dead dot elisp f fsf hdf idl idx_rfr linux lsm map match matlab mie mk ncl perl sh slr_spc sltsbl tex time \
ess ess_acc ess_atm ess_bnd ess_ccc ess_ccp ess_cnt ess_gng ess_gpc ess_hyd ess_lsp ess_phz ess_prc ess_rdn ess_sco uci \
aeroce aeronet afgl arese avhrr dmr dst esmf icr idea igpp ncep toms \
phd ppr_BiZ03 ppr_BiZ04 ppr_CaZ08 ppr_CaZ09 ppr_CaZ10 ppr_FlZ05 ppr_FlZ06 ppr_FZR07 ppr_GrZ04 ppr_HZM12 ppr_ZeK05 ppr_ZeM07 ppr_Zen08 ppr_Zen12 ppr_Zen14 ppr_ZeT06 ppr_ZeT06b ppr_ZFM07 ppr_ZGD09 ppr_ZKT12 ppr_ZMT04 prp prp_access prp_acme prp_ans prp_arl prp_axs prp_esdr prp_gpe prp_ids prp_IGPP06 prp_sdci prp_si2; do
    /bin/rm -r -f ~/${drc}
done

mkdir ~/prv
for drc in cns grd hire iusd job jrn ltr mny poetry rvw pr sdn trv www; do
    cp -r ~/${drc} ~/prv
    cd  ~/prv/${drc}
    /bin/rm -r -f .svn TAGS .gdb_history .cvsignore *.mod gmon.out *.brf *.idx *.ilg *.ind *.out *.nc
done

mkdir ~/sw
for drc in aca anl anv bxm c c++ ck cld crm crr dead dot elisp f fsf hdf idl idx_rfr linux lsm map match matlab mie mk ncl perl sh slr_spc sltsbl tex time; do
    cp -r ~/${drc} ~/sw
    cd  ~/sw/${drc}
    /bin/rm -r -f .svn TAGS .gdb_history .cvsignore *.mod gmon.out *.brf *.idx *.ilg *.ind *.out *.nc :${drc}
done

mkdir ~/acd
for drc in ess ess_acc ess_atm ess_bnd ess_ccc ess_ccp ess_cnt ess_gng ess_gpc ess_hyd ess_lsp ess_phz ess_prc ess_rdn ess_sco uci; do
    cp -r ~/${drc} ~/acd
    cd  ~/acd/${drc}
    /bin/rm -r -f .svn TAGS .gdb_history .cvsignore *.mod gmon.out *.brf *.idx *.ilg *.ind *.out *.nc :${drc}
done

mkdir ~/rsr
for drc in aeroce aeronet afgl arese avhrr dmr dst esmf icr idea igpp ncep toms; do
    cp -r ~/${drc} ~/rsr
    cd  ~/rsr/${drc}
    /bin/rm -r -f .svn TAGS .gdb_history .cvsignore *.mod gmon.out *.brf *.idx *.ilg *.ind *.out *.nc :${drc}
done

mkdir ~/pnp
for drc in phd ppr_BiZ03 ppr_BiZ04 ppr_CaZ08 ppr_CaZ09 ppr_CaZ10 ppr_FlZ05 ppr_FlZ06 ppr_FZR07 ppr_GrZ04 ppr_HZM12 ppr_ZeK05 ppr_ZeM07 ppr_Zen08 ppr_Zen12 ppr_Zen14 ppr_ZeT06 ppr_ZeT06b ppr_ZFM07 ppr_ZGD09 ppr_ZKT12 ppr_ZMT04 prp prp_access prp_acme prp_ans prp_arl prp_axs prp_esdr prp_gpe prp_ids prp_IGPP06 prp_sdci prp_si2; do
    cp -r ~/${drc} ~/pnp
    cd  ~/pnp/${drc}
    /bin/rm -r -f .svn TAGS .gdb_history .cvsignore *.mod gmon.out *.brf *.idx *.ilg *.ind *.out *.nc :${drc}
done
************************************************************************
Hi Ping,

Thank you for raising the high cirrus issue (pun intended).
Due to your comments I re-read the relevant parts of EMIT.
I want to make sure you noticed the following parts of the
proposal before we add this as a PMW:

Their atmospheric correction lead is Dave Thompson.
Thompson (and PI Green) have written multiple and recent papers on
atmospheric corrections with B.-C. Gao. EMIT will use (according to
Table E. 4-1) the current ATREM code that it sounds like Gao
developed, and which they are already using for AVIRIS-NG and HyspIRI 
studies (p. J.9-30). The proposal mentions (p. 1, J.11-2) cirrus 
removal using the 1360-1400 nm band.

Given this, do you still want to designate as a PMW that the proposal
is "relatively weak in the aspect of atmospheric correction
(specifically, the removal of high cloud contamination)...."?
This is not my area of expertise and I'm happy to defer to you. 

Best,
Charlie
************************************************************************
Hi Charles,

PyNCO, the Python wrappers for NCO, are at

http://github.com/nco/pynco

Please take a look and let us know whether anything about the package
needs to change before it can be distributed with UV-CDAT. The goal is

from nco import Nco

works in any Python script that uses UV-CDAT. Joe Hamman and I will
do the work to make the changes, we just need to know what would make
integration/distribution with UVCDAT as smooth as possible.

Obviously NCO needs to be installed for PyNCO to do anything.
However, I thought starting this issue/ticket with the smaller and
more familiar task of adding a Python library might get the ball
rolling, and we can tackle any issues that arise with installing
NCO as they arise from installing testing PyNCO. Or if you'd prefer
a different strategy that's fine too...

Thanks!
Charlie
************************************************************************
I think this page is ready for prime-time. Thanks for your input, everyone. Tagging so they are aware of its contents. I would like to inform the CESM community about this, as theses issues affect their gridfiles and mapfiles too. Suggestions on how to do that? 
************************************************************************
Hi Dave et al,

Thanks for reading the rather lengthy regridder pages.
Sorry for the delayed responses, which are interspersed below.

c

> Hi Susannah, Peter, Charles and Charlie,
> Reading through this page and comment thread,
> https://acme-climate.atlassian.net/wiki/display/ATM/Validation+and+Benchmarking+of+Regridders?focusedCommentId=32374835#comment-32374835
> I see lots of progress and cooperation, and I want to thank you all. 
>   However, it leaves me with three questions:
> 1. "Is the "Conclusion" section still valid?

To my knowledge, yes.

> 2. Are there instructions/documentation available for finding and using 
> the correct versions of the code on machines that are broadly accessible 
> (Rhea, Cades, ??), and pointers and instructions for building the source 
> on other machines for the more "typical" ACME participant - (meaning me 
> or someone like me)

The one-stop-shop for regridding is
https://acme-climate.atlassian.net/wiki/display/ATM/How+to+Regrid+Data

This contains links to the NCO and UVCDAT regridder instructions.
The locations of up-to-date NCO binaries (on rhea, pileus, edison,
cooley), and instructions to install/build your own should you wish,
are also shown/linked there.   

> 3. Are the tools sufficiently stable that they can be tested, accepted 
> and maintained by the Workflow group as part of the v0 workflow, if this 
> is indeed the approach the Workflow GL plan to use?

I designate the "two-dot"-releases of NCO (most recently NCO 4.5.2) as 
stable and put them on the above DOE machines. I update these as
necessary due to important new features or bugfixes. I'm unsure
whether this qualifies as "tested, accepted and maintained by the
Workflow group". I'm not certain what the Workflow GL plan to use for
v0 workflow. I am certain I'll continue to maintain/support NCO for
ACME users and do my best to smooth integration into the v0 workflow.

Does this answer your questions?

Best,
c
************************************************************************
git fetch
git checkout master
git pull
git checkout devel file1 file2
git status
git diff --cached
git commit
git push
************************************************************************
also want to add a non-scientific perspective:
the "careful" method breaks the (elegant, IMHO) processing hierarchy
that peter and we envisaged when discussing this months ago.  
the hierarchy is that monthly climos depend only on months, seasonal
climos depend only on monthly, annual climos depend only on seasonal.
if you can live with non-careful approximations, then the hierarchy
naturally extends to longer scales, and obviates the need to look at
level N-2 (and N-3, to avoid terminator migration) data when computing
climos for level N. 
************************************************************************
Charles and Jeff this page is on regridding anomalies and how to fix (some of) them. Would appreciate any comments before announcing to a wider audience.
************************************************************************
Dear Christine and Feng,

I just completed the first test of SLD remapping with NCO.
I regridded the AIRS test data Feng sent some time ago.
The TSurfStd field looks OK. Yay!

The three main issues that are my highest priority are
preserving global min/max features, working better with
integer fields (TSurfStd_ct), and finishing a script that
can do most of the tedious typing for the user.
There are other issues, but those are the showstoppers.

You'll need at least one more piece of software to get
this stuff working (in addition to updating NCO to the
bleeding edge or 4.5.3-alpha02 if you prefer tags).
The software is the ESMF package.

You may already have ESMF---type ESMF_RegridWeightGen
to find out if you do. It's a simple package install on
many systems (e.g., MacPorts: port install esmf).
Or you could install it from source.

There are four conceptual steps in the process:
NCO generates the required source+destination grids
(from the SLD file and user specifications, respectively).
ESMF uses those generate remapping weights.
Then NCO applies the remapping weights.
I'm writing a script to simplify all this, so the user
only needs to know the input file(s) and desired output grid 
and nothing about the black magic in between.
The script needs ESMF_RegridWeightGen to complete.

If you're interested in testing and giving feedback on early
versions of this stuff, then please install ESMF and update NCO.
But don't bother updating NCO until Tuesday AM, as I will
be polishing stuff until then.

Best,
c
************************************************************************
Yes, would be good to hear what the relevant scientists want. At least two of the variables are Aerosol Optical Depths (AODs). My experience is that scientists don't quote AODs to more than two significant figures. I suspect the computed climos accurately represent the model to that level, and the experiment you suggest could test that. Unless such an experiment disproves the two-digit hypothesis, then it seems to me the climos provide two things that I think aerosol scientists want:  1. an answer good to two digits 2. that they don't have to derive themselves. 
************************************************************************
ncks -O -D 5 -t 1 --rgr nfr=y --rgr grid=${DATA}/sld/rgr/grd_src_airs.nc ${DATA}/sld/raw/AIRS.2014.10.01.202.L2.TSurfStd.Regrid010.1DLatLon.nc ~/foo.nc 2>&1 | m
ncks -M -m -v grid_area,grid_corner_lat -d grid_size,0 ${DATA}/sld/rgr/grd_src_airs.nc

ncks -O -D 1 -t 1 --rgr grd_ttl='Equi-angular grid 40x40' --rgr grid=${DATA}/sld/rgr/grd_dst_airs.nc --rgr lat_nbr=40 --rgr lon_nbr=40 --rgr lat_sth=30.0 --rgr lat_nrt=70.0 --rgr lon_wst=-130.0 --rgr lon_est=-90.0 ~/nco/data/in.nc ~/foo.nc 2>&1 | m
ncks -M -m -v grid_area,grid_corner_lat -d grid_size,0 ${DATA}/sld/rgr/grd_dst_airs.nc

# plain
ESMF_RegridWeightGen -s ${DATA}/sld/rgr/grd_src_airs.nc -d ${DATA}/sld/rgr/grd_dst_airs.nc -w ${DATA}/sld/rgr/map_airs_to_dst_aave.nc  --method conserve --src_regional --dst_regional --ignore_unmapped
ESMF_RegridWeightGen -s ${DATA}/sld/rgr/grd_src_airs.nc -d ${DATA}/sld/rgr/grd_dst_airs.nc -w ${DATA}/sld/rgr/map_airs_to_dst_bilin.nc --method bilinear --src_regional --dst_regional --ignore_unmapped
ESMF_RegridWeightGen -s ${DATA}/sld/rgr/grd_src_airs.nc -d ${DATA}/sld/rgr/grd_dst_airs.nc -w ${DATA}/sld/rgr/map_airs_to_dst_aave_ua.nc  --method conserve --src_regional --dst_regional --ignore_unmapped --user_areas
# tempest...

ncks -D 5 -t 1 -O --rnr=0.5 --map=${DATA}/sld/rgr/map_airs_to_dst_bilin.nc ${DATA}/sld/raw/AIRS.2014.10.01.202.L2.TSurfStd.Regrid010.1DLatLon.nc ${DATA}/sld/rgr/dogfood.nc
ncks -D 5 -t 1 -O --xtn=TSurfStd_ct --map=${DATA}/sld/rgr/map_airs_to_dst_bilin.nc ${DATA}/sld/raw/AIRS.2014.10.01.202.L2.TSurfStd.Regrid010.1DLatLon.nc ${DATA}/sld/rgr/dogfood.nc

ncks -D 5 -t 1 -O --rnr=0.5 --map=${DATA}/sld/rgr/map_airs_to_dst_aave.nc ${DATA}/sld/raw/AIRS.2014.10.01.202.L2.TSurfStd.Regrid010.1DLatLon.nc ${DATA}/sld/rgr/dogfood.nc
ncks -D 5 -t 1 -O --rnr=0.5 --map=${DATA}/sld/rgr/map_airs_to_dst_aave_ua.nc ${DATA}/sld/raw/AIRS.2014.10.01.202.L2.TSurfStd.Regrid010.1DLatLon.nc ${DATA}/sld/rgr/dogfood.nc

ncks -M -m -v latitude,longitude ${DATA}/sld/rgr/dogfood.nc
************************************************************************
My understanding of this issue continues to evolve with discussion. It was only yesterday when I first realized this problem affects not only polar regions (due to monthly changes in polar night) but in fact afflicts every gridpoint (poles-to-tropics) because duration of local night varies monthly everywhere. I was dismayed to realize this is a global not a regional problem, which I'm sure some of you already realized.

And yes, @Susannah, I agree with your estimates of the throughput-cost of the "careful" solution. 

What Phil calls the "careful" and "supercareful" options would be (or could be, depending on implementation) equivalent except for interannual terminator changes. Phil's statements that the "supercareful" option employed by UVCDAT "uses tallies to account for missing data at any timescale" ... "is the best way to do it as it handles all cases" could be mis-interpreted to mean that the "supercareful" method is a solution. To be clear, no amount of monthly bookkeeping can provide the tallies necessary to solve the problem anywhere. At best monthly bookkeeping can reduce additional biases currently introduced into climos on supra-monthly timescales. The sub-monthly timescale will always contain biases of (at least) the same order of magnitude as those that careful/supercareful monthly bookkeeping can eliminate. Timestep-level tallies from CAM are the only means that have been discussed that could solve the problem. Nothing else comes close. This concludes today's regularly scheduled horse-beating :)

************************************************************************
nco christine smit:
ncgen -b -o ~/christine.nc christine.cdl
ncra -O -d time,1,1 -d time,3,3 ~/christine.nc ~/foo.nc
ncra -O -d time,1,1 -d time,3,3 ~/nco/data/in.nc ~/foo.nc
ncra -O -C -v one_dmn_rec_var -d time,1,1 -d time,2,2 ~/nco/data/in.nc ~/foo.nc
************************************************************************
nco sean davis:
ncrcat -O -4 -L 1 ~/hus_Amon_reanalysis_CFSR_197901-197912.nc ~/hus_Amon_reanalysis_CFSR_198001-198012.nc ~/foo.nc
************************************************************************
Four currently available options have been discussed for ANN:
1. Compute from DJF, MAM, JJA, SON climos. Neglect intraseasonal
changes in missing value presence. AKA NCO method. (4 files, always)
2. Compute from DJF, MAM, JJA, SON climos. Account for intraseasonal
changes in missing value presence. AKA UVCDAT method. (4 files, always)
3. Compute from D,J,F,M,A,M,J,J,A,S,O,N climos (12 files always)
4. Compute from individual months (# of year times 12 files always)

Options 1-4 always produce wrong climatological answers for any
location where the number of missing values depends on month.  
In practice this means _all_ gridpoints (not just polar regions) since  
all CAM missing values depend on solar radiation and nighttime length
varies with month everywhere.

Options 2, 3, and 4 are equivalent except for points where the
terminator occupies different locations/timesteps in different years.
This is rare. Option 4 does not scale well so in my mind (and it
sounds like Susannah's too) Options 2 and 3 are preferable to 4.  

Options 2 and 3 differ in speed, complexity, and memory. Option 3
scales as Susannah describes. I could implement it fairly quickly.


************************************************************************
Dear All,

The discussion of Recommendation #1 from two years ago is drifting.
I encourage everyone to consider the text of the Recommendation as is:

"We recommend that HDF5 files be designed to maximize netCDF4
interoperability by making Earth Science data files in HDF5 format
fully accessible from the netCDF4 API."

and to refrain from expanding it to encompass areas (such as CF) that
we can consider as independent recommendations this year.

As I said earlier, I am unaware of any dataset content that _requires_ 
using an HDF5 feature that cannot be made accessible through the
netCDF4 API with a little effort on the part of the HDF5 dataset
designer. If members are aware of any such content, they should post
it to the group immediately, and verify that they have indeed followed 
interoperability best practices in creating the content. Additionally
they should post the program/script/method of creation so the group
can examine it do determine whether/how to improve the best practices
to allow netCDF4 access to the full content.

It is counterfactual to assert that such content exists until it has
been shown to exist. Joe Lee provided a dataset that cannot be read
by current versions of ncdump/ncks. Whether this is a limitation of
ncdump/ncks, of netCDF4, or of the practices used to create it has not 
yet been established.

Peter and I would like this group to operate by consensus.
However, that may not always be possible. Our group is focused on
Interoperability. Recommendation #1 means that only HDF5 features
accessible through the netCDF4 API should be used in future
NASA-distributed datasets. Kent Yang and Joe Lee have suggested
appending "if applicable" to Recommendation #1, i.e., 

"We recommend that HDF5 files be designed to maximize netCDF4
interoperability by making Earth Science data files in HDF5 format
fully accessible from the netCDF4 API if applicable."

In my opinion this modification undermines the Recommendation.
He raised these concerns (in quotes) to me/DIWG, and I answer (for
myself) interspersed:

"Supposed if there is a valid HDF5 file that is created by HDF5
APIs (and can be accessed by HDF5 APIs and  tools successfully) and it 
is supposed to be accessed by NetCDF-4 APIs, but it is not."

This presumes the dataset creator has followed best practices.

"This may be due to  a bug or something else that ncdump cannot dump
this HDF5 file successfully. Doesn't that mean that product cannot be
released or the data producer has to twist its design to make ncdump
happy? I think you may agree with me that such an HDF5 file should be
released regardless when the bugs get fixed.

You may say at least the bug will be fixed."

If it's a netCDF4 problem, then the dataset can be released without
modification because it is or will be (when the bug is fixed)
fully accessible through the netCDF4 API. If however, the problem
was caused by not following best practices, then the onus is on the
dataset producer to follow best practices and make it fully accessible
through the netCDF4 API _before NASA releases the dataset_.

"Now let's go to a step further, what if the bug or the issue cannot
be fixed due to some reasons. Will this product get released?  I
certainly hope so."

Such a situation has not been shown to exist, and if it did exist,
would presumably be handled on a case-by-case basis. DIWG cannot
always 
************************************************************************
ncks --rgr grd_ttl='CAM FV-scalar grid 7x12' --rgr grid=${DATA}/grids/7x12_SCRIP.20150901.nc --rgr lat_nbr=7 --rgr lon_nbr=12 --rgr lat_typ=cap --rgr lon_typ=grn_ctr ~zender/nco/data/in.nc ~/foo.nc
ncks --rgr grd_ttl='CAM FV-velocity grid 6x12' --rgr grid=${DATA}/grids/6x12_SCRIP.20150901.nc --rgr lat_nbr=6 --rgr lon_nbr=12 --rgr lat_typ=uni --rgr lon_typ=grn_wst ~zender/nco/data/in.nc ~/foo.nc
ncks --rgr grd_ttl='Uniform grid 6x12' --rgr grid=${DATA}/grids/6x12_SCRIP.20150901.nc --rgr lat_nbr=6 --rgr lon_nbr=12 --rgr lat_typ=uni --rgr lon_typ=grn_ctr ~zender/nco/data/in.nc ~/foo.nc
************************************************************************
Dear All,

Is there an analytic formula for the area of a triangle on the sphere where two sides are great circle arcs and the third is a small circle parallel to the equator, i.e., an arc of constant latitude? I'm familiar with the formulae for spherical triangles, but those don't apply here. I solved (I think) the case with an arc of constant latitude and two rhumb lines, but I really need an analytic general solution to a triangle with one arc of constant latitude and the other two sides great circle arcs. Otherwise may have to numerically integrate. Uggh.

Thanks!
cz
************************************************************************
New Rhea build
Intel:
export LINUX_CC='icc -std=c99 -D_DEFAULT_SOURCE'
export LINUX_CXX='icpc'
export LINUX_FC='ifort'
module add intel gsl
export LD_LIBRARY_PATH='/ccs/proj/cli900/sw/redhat/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4/lib:/sw/redhat6/szip/2.1/rhel6.6_gnu4.8.2/lib':${LD_LIBRARY_PATH}
export NETCDF_ROOT='/ccs/proj/cli900/sw/redhat/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4'
export PATH='/ccs/proj/cli900/sw/redhat/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4/bin':${PATH}
cd ~/nco/bld;make ANTLR_ROOT=${HOME} NETCDF_ROOT='/ccs/proj/cli900/sw/redhat/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4' UDUNITS_INC='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/include' UDUNITS_LIB='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/lib' OPTS=D OMP=Y allinone;cd -

cd ~/nco/bld;make ANTLR_ROOT=${HOME} LDFLAGS='-L/ccs/compilers/intel/rh6-x86_64/14.0.4/composer_xe_2013_sp1.4.211/compiler/lib/intel64 -lirc -limf' NETCDF_ROOT='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4' SZ=Y SZ_LIB='/sw/redhat6/szip/2.1/rhel6.6_gnu4.8.2/lib' UDUNITS_INC='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/include' UDUNITS_LIB='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/lib' OPTS=D OMP=Y allinone;cd -

-L /ccs/compilers/intel/rh6-x86_64/14.0.4/composer_xe_2013_sp1.4.211/compiler/lib/intel64 -lirc -limf

************************************************************************
hey man,
i think the non-spherical triangle on a sphere area is tractable.
if so, then exact regridding with lat/lon grid is only a few weeks away.
this is the first algorithm that came to me.
i'm sure there are better ways but for starters...

1. triangle has one side (the base, say) on constant latitude circle:
vertices a,b,c are (lat1,lon1), (lat1=lat2,lon2), (lat3,lon3)
2. integration of triangular area proceeds as in first year calculus,
with the differential area element cos(lat)*dlon*dlat and lat
integral runs from lat1 to lat3. only complication is lon integral
which has variable lower and upper bounds that depend on lat.
lower lon bound is (lon2-lon1)*(lat-lat3)/(lat1-lat3).

i want a great circle arc to connect two points on a sphere.
the endpoints are (lat1,lon1) and (lat2,lon2).
is the arc described by the parameter Z the great circle arc?

Z is in [0,1]:
lat=lat1+Z*(lat2-lat1)
lon=lon1+Z*(lon2-lon1)

i think that whether this must be true or false could be revealed
by careful thought and scribbling but but i cannot visualize it.

************************************************************************
ncks -O -D 5 -v FSNT --map ${DATA}/maps/map_ne30np4_to_fv257x512_aave.20150823.nc ${DATA}/ne30/rgr/famipc5_ne30_v0.3_00003.cam.h0.1979-01.nc ${DATA}/ne30/rgr/fv_FSNT.nc
ncks -O -D 5 -v FSNT --map ${DATA}/maps/map_fv257x512_to_conusx4v1np4_chevrons_bilin.20150901.nc ${DATA}/ne30/rgr/fv_FSNT.nc ${DATA}/ne30/rgr/dogfood.nc
ncks -H -s %20.15e -v area -d ncol,0 ${DATA}/ne30/rgr/dogfood.nc
ncks -H -s %20.15e -v grid_area -d grid_size,0 ${DATA}/grids/conusx4v1np4_chevrons_scrip_c150815.nc

ncol=0 on conus chevrons file:
3.653857995295246e-05 raw GLL weight
3.653857995294302e-05 matlab N-2 triangles
3.653857995294301e-05 matlab N   triangles
3.653857995294258e-05 new NCO (haversine)
3.653857995289623e-05 old NCO (acos)
************************************************************************
2)      Question for you: Can you submit your written reviews via NSPIRES by the end of August?
a.       The attached forms are the review forms. NSPIRES will give you pop ups, but you can use the Word forms for convenience and copy and paste.
b.      Leads: write your review as though it was a panel summary, and weâ€™ll use that as the draft to start from.
c.      Seconds (readers): Bullet point your comments. In fact, briefer the better to make it easy for the leads.
d.      Note the differences between Form A (science) and B (can they achieve the science with this method?). Some people miss the distinction

3)      At our telcon, we'll identify the major weaknesses, which I will send the PIs in mid-September for a quick turnaround response.  Leads: if you can, log into NSPIRES and read the Reader comments before the meeting.
************************************************************************
ncks -H -v area
************************************************************************
Queen-size futon bed-set free on driveway of 3 Whistler Court:

Queen-size futon
Queen-size box-spring
Queen-size steel box-spring frame (gives ~5" vertical floor clearance)
~72" x 48" x 3/4" plywood sheet used to give extra futon support
************************************************************************
The flaws explained above can be thought of as fuzziness at the level of a few tenths of a degree in georeferencing regridded data to the native model grid. These location errors produce only small (<< 1%) errors in regional or global statistics. So, why migrate? One aim is that diagnostics and observational evaluations with regridded data (often much more intuitive to visually evaluate than native SE grids) produce the same answers (to double precision whenever possible) as statistics computed on the native model grid. Without migration, agreement between native and regridded statistics beyond single precision is a matter of luck and coincidence, not determinism and reproducibility. As ACME grids shift to ~1/4 degree and finer, it becomes even more important to exploit the full double precision accuracy that software can guarantee when supplied with accurate grids.

Names of ACME gridfiles and mapfiles are date-stamped. The new grid and mapfiles were produced on or after 20150724. Cubed-sphere gridfiles produced before 20150724 are still considered good. Most other atmosphere gridfiles produced before 20150724 contain the flaws mentioned above. Gridfile flaws propagate to mapfiles so in most cases mapfiles must be updated too.

Flawed ACME Gridfiles  	   
129x256_SCRIP.130510.nc	   
257x512_SCRIP.130510.nc	   
801x1600_SCRIP.130510.nc   

Corrected ACME Gridfiles:
129x256_SCRIP.20150901.nc
257x512_SCRIP.20150901.nc
801x1600_SCRIP.20150901.nc

Flawed ACME Mapfiles:
map_ne30np4_to_fv129x256_aave.150418.nc
map_ne30np4_to_fv257x512_bilin.150418.nc
map_ne120np4_to_fv257x512_aave.150418.nc
map_ne120np4_to_fv801x1600_bilin.150418.nc

Corrected ACME Mapfiles:
map_ne30np4_to_fv129x256_aave.20150901.nc
map_ne30np4_to_fv257x512_bilin.20150901.nc
map_ne120np4_to_fv257x512_aave.20150901.nc
map_ne120np4_to_fv801x1600_bilin.20150901.nc

Corrected Other Gridfiles:
180x360_SCRIP.20150820.nc
90x180_SCRIP.20150820.nc
t42_SCRIP.20150901.nc
t62_SCRIP.20150901.nc
t85_SCRIP.20150901.nc

Generating Gridfiles:
The netCDF Operator ncks generates accurate and complete SCRIP-format gridfiles for select grid types, including equi-angular, FV, and Gaussian rectangular latitude/longitude grids. Full documentation is at http://nco.sf.net/nco.html#grid. Options pertinent to the grid geometry and metadata are passed to NCO via key-value pairs prefixed by "--rgr". Pass at least six key-value pair arguments to create a fully explicitly specified, well-annotated grid. The six arguments, and their corresponding keys, are: grid title (grd_ttl), grid filename (grid), number of latitudes (lat_nbr), number of longitudes (lon_nbr), latitude grid type (lat_typ), and longitude grid type (lon_typ). Four other arguments (the NSEW bounding box) are necessary to construct regular regional (non-global) grids, but for now we focus on global grids.

The lat_typ options for global grids are "eqa" for Equi-angular, "fv" for Finite Volume, and "gss" for Gaussian. An FV grid is nearly identical to an Equi-angular grid, with some important data and metadata differences at the poles. FV grids must have an odd number of latitudes because the first and last latitudes have special properties (and span half the width
of the other latitudes), whereas the Equiangular grid may have any number of latitudes. Gaussian grids must have an even number of latitudes. The lon_typ options for global grids are "grn_ctr" and "180_ctr" for the first gridcell centered at Greenwich or 180 degrees, respecitvely. And "grn_wst" and "180_wst" for Greenwich or 180 degress lying on the western edge of the first gridcell.

These keywords allow one to generate grids commonly used by ACME, CESM, and other models. New FV grids have replaced the flawed grids on the ACME inputdata server: https://acme-svn2.ornl.gov/acme-repo/acme/mapping/grids. The newer FV grids were generated by NCO (version 4.5.2 or later) with:

ncks -O -D 7 --rgr grd_ttl='CAM FV-scalar grid 129x256' --rgr grid=${DATA}/grids/129x256_SCRIP.20150901.nc --rgr lat_nbr=129 --rgr lon_nbr=256 --rgr lat_typ=cap --rgr lon_typ=grn_ctr ~zender/nco/data/in.nc ~/foo.nc
ncks -O -D 7 --rgr grd_ttl='CAM FV-scalar grid 257x512' --rgr grid=${DATA}/grids/257x512_SCRIP.20150901.nc --rgr lat_nbr=257 --rgr lon_nbr=512 --rgr lat_typ=cap --rgr lon_typ=grn_ctr ~zender/nco/data/in.nc ~/foo.nc
ncks -O -D 7 --rgr grd_ttl='CAM FV-scalar grid 801x1600' --rgr grid=${DATA}/grids/801x1600_SCRIP.20150901.nc --rgr lat_nbr=801 --rgr lon_nbr=1600 --rgr lat_typ=cap --rgr lon_typ=grn_ctr ~zender/nco/data/in.nc ~/foo.nc

ncks -O -D 7 --rgr grd_ttl='T42 Gaussian grid' --rgr grid=${DATA}/grids/t42_SCRIP.20150901.nc --rgr lat_nbr=64 --rgr lon_nbr=128 --rgr lat_typ=gss --rgr lon_typ=Grn_ctr ~zender/nco/data/in.nc ~/foo.nc
ncks -O -D 7 --rgr grd_ttl='T62 Gaussian grid' --rgr grid=${DATA}/grids/t62_SCRIP.20150901.nc --rgr lat_nbr=94 --rgr lon_nbr=192 --rgr lat_typ=gss --rgr lon_typ=Grn_ctr ~zender/nco/data/in.nc ~/foo.nc
ncks -O -D 7 --rgr grd_ttl='T85 Gaussian grid' --rgr grid=${DATA}/grids/t85_SCRIP.20150901.nc --rgr lat_nbr=128 --rgr lon_nbr=256 --rgr lat_typ=gss --rgr lon_typ=Grn_ctr ~zender/nco/data/in.nc ~/foo.nc

For convenience, all grids created for this migration are filename date-stamped with 20150901 to facilitate recognizing updated gridfiles and mapfiles. Full creation metadata is in the file header. Any valid netCDF file may be named as the source (e.g., in.nc). It will not be altered. The destination file (foo.nc) will be overwritten. Its contents are immaterial. The "-D 7" option increases the output verbosity so one can easily check, e.g., the precision of normalization of area and latitude weights. The API for creating grids is primitive (e.g., having to repeat "--rgr") because it was quickly bolted-on to NCO. We may improve and extend the NCO API to specify other grids and maps in the future. Important updates will be noted here.

How to generate accurate Mapfiles:

ESMF_RegridWeightGen -s ${DATA}/grids/ne30np4_pentagons.091226.nc -d ${DATA}/grids/129x256_SCRIP.20150901.nc -w ${DATA}/maps/map_ne30np4_to_fv129x256_aave.20150901.nc --method conserve
ESMF_RegridWeightGen -s ${DATA}/grids/ne30np4_pentagons.091226.nc -d ${DATA}/grids/257x512_SCRIP.20150901.nc -w ${DATA}/maps/map_ne30np4_to_fv257x512_bilin.20150901.nc --method bilinear
ESMF_RegridWeightGen -s ${DATA}/grids/ne120np4_pentagons.100310.nc -d ${DATA}/grids/257x512_SCRIP.20150901.nc -w ${DATA}/maps/map_ne120np4_to_fv257x512_aave.20150901.nc --method conserve
ESMF_RegridWeightGen -s ${DATA}/grids/ne120np4_pentagons.100310.nc -d ${DATA}/grids/801x1600_SCRIP.20150901.nc -w ${DATA}/maps/map_ne120np4_to_fv801x1600_bilin.20150901.nc --method bilinear

************************************************************************
Regional grids:
ncks -O -D 1 --rgr grd_ttl='Regional grid 10x10' --rgr grid=${DATA}/grids/10x10_SCRIP.20150901.nc --rgr lat_nbr=10 --rgr lon_nbr=10 --rgr lat_sth=-5.0 --rgr lat_nrt=5.0 --rgr lon_wst=-5.0 --rgr lon_est=5.0 ~zender/nco/data/in.nc ~/foo.nc
************************************************************************
Thanks for the info, Jeff. Your understanding of climo_nco.sh and your point about neglecting some monthly missing value information in the construction of the annual climatology are both correct. Eliminating biases associated with missing value statistics reminds me of the aphorism that perfect is the enemy of good. The main reasons that annual climatologies are built-up from seasonal climatologies rather than straight from monthly (which avoids the problem) are that going back to the raw monthly data does not buy much accuracy, yet it does cost a lot of speed and possibly disk space.

Computing an annual climatology that eliminates the bias Jeff refers to requires either tracking the "tally" of missing values for each field at each location through the intermediary seasonal stage, thus increasing seasonal memory use and/or disk space by 50-100% (an additional short or int per field that contains missing values), or, alternatively, computing the annual means straight from the monthly data, e.g., average 60 files (5 years * 12 months) instead of 4 files (4 seasons, as NCO & AMWG currently do) for a 5-year climatology. To speed things up, such a "single-stage" climatology could be done on only those fields with missing values, combining them with the multi-stage climatology for other variables at the end.

While it is tempting to jump ahead and implement such "exact" methods in the name of accuracy, one must first understand that any accuracy gained has fundamental limitations. Even after doing the bookkeeping to construct annual means that respect each month's missing value mask, i.e., the UVCDAT method, such means are still significantly biased. Consider AODVIS, for example. The raw monthly output at a point is the average of the timesteps when this field is valid in the model. Whether a valid monthly AODVIS was valid for only 1 timestep or for the entire month is not archived, and its annual mean from CAM monthly output includes time-biases no matter how the annual climatology is constructed. Accuracy beyond that is illusory. And remember that the current method employed by NCO (and AMWG) is already perfectly accurate for the ~660/675 model fields in CAM without missing data. 

If one's goal is exact temporal weighting of data that may contain missing values, the proper solutions are to compute statistics for desired timescales on-line (while the model runs) or to output the timestep-level "tally" of every variable to the monthly output. If and when models output that, NCO could use it as a weight. Hence, the proper fix to this problem requires changing model output routines and to store the "timestep tally" for each missing value field.

More pragmatically, constructing the type of mean for missing values that Jeff has implemented in UVCDAT could be done by modifying how NCO is called in climo_nco.sh without any NCO source code modification. It could be be implemented quickly as an optional method of construction in climo_nco.sh. It could be done by running the variables with missing values through their own, single-stage, climatology. Or it could be implemented by storing missing-value tallies in the seasonal files and using those as weights in the annual climatology. Either method would be more statistically defensible than what NCO (and AMWG) does now, though not by much, and similar biases would remain (because the timestep-level tallies are unavailable).

To summarize, the UVCDAT method of constructing annual climatologies would solve the problem of timestep-level weighting if it operated on instantaneous timestep-level missing value tallies. Until models such output timestep tallies, climos constructed from monthly or even daily-mean data will suffer biases from inaccurate accounting of missing value duration. It is unclear how much accuracy is gained by a partial solution such as UVCDAT has implemented. The drawbacks of the method include speed, complexity, and, potentially, disk-space (0-100% more, varies by implementation).    
************************************************************************
matlab -nodisplay
format long
lat1 =[
0.341215693632680,
0.341147888120055,
0.346916154821447,
0.347227105822592,
0.347227105822592,
0.347227105822592]

lon1 =[
3.815996778621659,
3.822830543733946,
3.819605294803501,
3.813060805743338,
3.813060805743338,
3.813060805743338]

a = grtcircdist_latlon(lat1(1),lon1(1),lat1(2),lon1(2));
b = grtcircdist_latlon(lat1(2),lon1(2),lat1(3),lon1(3));
c = grtcircdist_latlon(lat1(3),lon1(3),lat1(1),lon1(1));
s = (a+b+c)/2;
A1 = sum(4atan(sqrt(tan(s/2).tan((s-a)/2).tan((s-b)/2).tan((s-c)/2))));

a = grtcircdist_latlon(lat1(1),lon1(1),lat1(3),lon1(3));
b = grtcircdist_latlon(lat1(4),lon1(4),lat1(3),lon1(3));
c = grtcircdist_latlon(lat1(4),lon1(4),lat1(1),lon1(1));

s = (a+b+c)/2;
A2 = 4atan(sqrt(tan(s/2).tan((s-a)/2).tan((s-b)/2).tan((s-c)/ ...
2)));
************************************************************************
The NCO algorithm (i.e., N-2 triangles) always works for N=3 (obviously), and for N>=4 convex polygons. The NCO algorithm can be made to work for N=4 concave polygons by starting at the vertex that contains the concave angle. The NCO algorithm would break-down into a morass of if/thens for concave polygons N>=5.

The centroid N-triangle algorithm always works for N<=4, for concave and convex, and always for N>=5 for convex. It breaks down for (some) concave polygons N>=5. It does work for the N=6 chevron you drew. What other concave shapes are used/important?

Correct me if I'm wrong but we expect fewer triangles (i.e., N-2) to be (slightly) more accurate than more triangles because of less round-off error. (We still do not understand why NCO and matlab produce such different results for ncol=0 on the conus grid). 

The general case for N>=5 requires lots of trig, for both algorithms. I'm not interested in NCO solving the general case of polygonal area. I am interested in NCO solving polygonal area for all shapes that arise in climate modeling grids used by funding agencies :)

One way NCO could proceed is to require a "check_concave" switch. When not activated, NCO would use its current fast (N-2 triangle), accurate (less round-off error) method. When activated, NCO would check for concave angles for all N>=4 polygons. This requires solving all interior angles (whereas for convex polygons NCO only solves for side/arc lengths). If polygon is convex, use L'Huillier N-2 triangle solution. If polygon is concave, use N-triangle centroid solution. Does Girard's formula actually work for concave polygons? If so that is also an option, though it sacrifices precision. And if Girard works in the general case, then it might be "good enough" if restricted to the (presumably very few) concave polygons on an otherwise predominantly convex grid.

I'm most interested in getting this right for the Cubed Sphere (CS) grid at the moment, with RRM grids (and concave shapes like chevrons etc.) to follow. My understanding is that all shapes on the default CS grid (i.e., default dual grid for CAM SE) are convex, mainly quadrilaterals, with maybe pentagons/hexagons at the edges. Is this correct? 
************************************************************************
To summarize, my understanding is that:
Matlab starts with the GL weight and shifts vertices around.
Computes area based on centroid and N triangles.
Also the "areas" listed in the SCRIP file are actually GL weights, 
and the vertices in the SCRIP file are the polygons that best fit
those weights (and are known to be hard/impossible to improve).
Yes?

NCO assumes the vertices in the SCRIP file are gospel and
computes (via L'Huillier) the area using N-2 (not N-1) triangles.
Is L'Huillier applied to N centroid-based triangles more
accurate than applied to N-2 vertice-based triangles?
Because vertice-based triangles are "sharper" than centroid-based? 
Is this known "for sure" or maybe a coincidence for this point?
Surpised that using fewer triangles appears to be less accurate!
Would naively expect less round-off noise from fewer triangles.

What, if anything should be done differently?
NCO could compute a centroid and use N triangles, like Matlab.
Or use N-2 triangles with a side-angle-side formula.
Or keep N-2 triangles and L'Huillier.
However, if the centroid algorithm is known to be more accurate, then
NCO should just use that.
************************************************************************
3.653857995294302e-05 area computed via matlab routine
3.653857995295246e-05 (GL weight)
relative error between the above two numbers: 3e-13.

Relative error between NCKS and Matlab area calculation: 1.3e-12.

For the record: the matlab routine searches for a dual grid with
spherical area that matches the GL weight. It uses a Newton iteration,
and if we run more iterations it wont converge to better than 3e-13. 

The matlab area is computed via a convoluted process:
0. assume the SCRIP control volume is a "n-gon"
1. convert lat/lon coordinates to cartesian
2. compute a mean of all the coordinates (centroid of the n-gon)
3. using the centroid, and each of the edges, make n triangles
4. sum the area of the n triangles (via L'Huillier's formula)

So the matlab code is converting to Cartesian, and then in
L'Huillier's formula, converting back to lat/lon to compute great
circle distances). And our matlab algorithm is also forming n
triangles, and ncks might make triangles out of just vertices and
you'd only have n-1 triangles? 

This brings up another possibility: some of the cells in our dual grid
are (slightly) not convex, and ncks may be assuming they are convex
when breaking up into triangles in order to compute the spherical
area? That's why we switched to using a centroid, since it's a little
more robust.
************************************************************************
For ${DATA}/grids/conusx4v1np4_chevrons_scrip_c150815.nc, NCO-diagnosed and raw grid area of first gridcell: 
zender@roulee:~$ ncks -H -s %20.15e -v area -d ncol,0 ${DATA}/ne30/rgr/dogfood.nc
3.653857995289623e-05
zender@roulee:~$ ncks -H -s %20.15e -v grid_area -d grid_size,0 ${DATA}/grids/conusx4v1np4_chevrons_scrip_c150815.nc
3.653857995295246e-05

Ratio is 3.653857995289623/3.653857995295246=0.99999999999846107867
so relative precision meets 1.0e-11, almost 1.0e-12.
Better than the ne30 grid previously tested, yet underwhelming.
Would expect ~1.0e-14 agreement if NCO and the Matlab script both
implement L'Huillier's formula corrrectly yet with different order
of operations. 
************************************************************************
************************************************************************
github.txt
Hi Justin,

Thanks, there is one more thing...
Just noticed that someone created an "NCO" account identical to the
(old) "nco" account at about the same time, and with my NCO imagery. 
Perhaps a bot created both by skimming SourceForge projects? 
It frustrates me that people abuse GitHub this way, given the
resources GitHub generously provides to the free software community. 

In any case, I would like to get the "NCO" uppercase account as well.
It might make a better organization name than "nco" (lowercase).
And I am worried that the "hacker" who created it might otherwise
someday do something unpleasant with "NCO" that confuses my users.
would appreciate it if you could send this message to the registrar of
the NCO account (or just transfer it to me outright):

Hello GitHub user "NCO",

I hope you are well. My name is Charlie Zender and I am the author and
maintainer of the netCDF Operator (NCO) software suite. A few months
ago the authors of other NCO-related software (python wrappers for
NCO, and the Debian and Fedora NCO maintainers) and I agreed to place
our NCO-related software under the same GitHub organization. We would
like to use the "NCO" name for this, and hope you will allow GitHub to
transfer the "NCO" username to me for that purpose.

NCO is free software written by and given to students, scientists, and
researchers involved in geophysics and global change research.
We would greatly appreciate the enhanced identity that the "NCO"
organization name would provide to our efforts.
Your generosity in transferring this name would be greatly appreciated.
Please contact me personally at zender@uci.edu or 949 891-2429 if you
would like to discuss this.

Thank you for your consideration,
Charlie
************************************************************************
Ruby yes
Max yes
Sandrine yes
Rachel
Esther no
Chiara
Luca
Francesca no
Lucy yes
Ryan Alavi yes
Aidan and Mateo
Sebastian
Dominic (not invited yet)

nadiadickson@gmail.com,aileen.lanch@gmail.com,adrian.lanch@gmail.com,lgrant@uci.edu,mcanness@uci.edu,isabella.velicogna@gmail.com,erignot@uci.edu,lolajp@sbcglobal.net,augustiniak@gmail.com,zender.michelle@gmail.com,john@fusiontechnology.com,yjia@gatan.com,lzender@zendergroup.org,monaruth2000@yahoo.com,lzender@att.net,drewmlin@gmail.com,eyeirish@yahoo.com,sharianderson69@gmail.com,carolrufenach@gmail.com,clrufenach@gmail.com

Dear Friends and Family of Ruby,

Ruby turns 10 this Saturday, 8/29, and she's hoping you or your
designated offspring will jump off the walls with us at SkyHigh,
the trampoline place in Costa Mesa, at 2970 Airway Ave.
http://www.jumpskyhigh.com
The party runs from 2:30-4:30 and includes jumping, pizza, and cake.
There's paperwork involved so sign the waivers online or come a bit
early to sign to get the full two hours of jumping. 
She's really excited to spring with your spawn all afternoon long.

Please RSVP to either

Charlie 949 891 2429 zender@uci.edu
Robynn  949 823 9344 robynn.zender@gmail.com

and we hope to see you then!
************************************************************************
Hi Christine, Feng, and Steven,

Just finished looking into the lon_bnds problem you reported.
Turns out this is due to the library (named Tempest) that I
used (past tense) to create the mapfiles (weightfiles) for regridding.
I've informed the Tempest authors of the problem.
Thanks for bringing it to my attention.

In the meantime, I implemented the capability to generate new
grids directly in NCO, and used these grids to create
(with a different mapping library named ESMF) new mapfiles:

http://glace.ess.uci.edu/tmp/map_180x360_to_90x180.20150820.nc
http://glace.ess.uci.edu/tmp/map_90x180_to_180x360.20150820.nc

The NCO you already have installed (no recompilation necessary)
should create plottable regridded files with these mapping files, i.e., 

ncks -O --map=${DATA}/maps/map_180x360_to_90x180.20150820.nc ${DATA}/rgr/dst_1x1.nc ${DATA}/rgr/dst_2x2.nc
ncks -O --map=${DATA}/maps/map_90x180_to_180x360.20150820.nc ${DATA}/rgr/dst_2x2.nc ${DATA}/rgr/dst_1x1.nc

This shows the virtue of placing the date in the mapfile name.
Always use the latest mapfile :)

The new grid generation capability in NCO is part of the work
necessary to support Swath-Like Data (SLD) in NCO. 

Best,
Charlie
************************************************************************
Subject: RLL longitude interface problem
Hi Tempestuous ones,

I used Tempest to create RLL mapping files like 1x1<->2x2 degree, then I distributed these mapfiles to a few users who later notified me that they could not plot the resultant regriddings. It turns out that the Tempest longitude interfaces (but not the latitude interfaces) are bad. 

I constructed the Tempest RLL mapfile with:
GenerateRLLMesh --lat 180 --lon 360 --file ${DATA}/rgr/msh_180x360.g
GenerateRLLMesh --lat 90 --lon 180 --file ${DATA}/rgr/msh_90x180.g
GenerateOverlapMesh --a ${DATA}/rgr/msh_180x360.g --b ${DATA}/rgr/msh_90x180.g --out ${DATA}/rgr/msh_ovr_180x360_to_90x180.g # works
GenerateOfflineMap --in_mesh ${DATA}/rgr/msh_180x360.g --out_mesh ${DATA}/rgr/msh_90x180.g --ov_mesh ${DATA}/rgr/msh_ovr_180x360_to_90x180.g --out_map ${DATA}/maps/map_180x360_to_90x180.20150818.nc

This produces these interfaces:
zender@roulee:/data/zender/grids$ ncks -H -v xv_a map_180x360_to_90x180.20150818.nc | /bin/more
n_a[0] nv_a[0] xv_a[0]=0 
n_a[0] nv_a[1] xv_a[1]=1 
n_a[0] nv_a[2] xv_a[2]=0 
n_a[0] nv_a[3] xv_a[3]=0 
...

Tempest puts three zero-longitude interfaces around the first gridcell where there should only be two. I verified this by constructing the same grid (in SCRIP format) using the new SCRIP-grid-generation feature of NCO (present since version 4.5.2-alpha7):
zender@roulee:/data/zender/grids$ ncks -O -D 1 --rgr grd_ttl='Offset grid 180x360' --rgr grid=${DATA}/grids/180x360_SCRIP.20150820.nc --rgr lat_nbr=180 --rgr lon_nbr=360 --rgr lat_typ=ngl_eqi_fst --rgr lon_typ=Grn_wst ~/nco/data/in.nc ~/foo.nc
zender@roulee:/data/zender/grids$ ncks -H -v grid_corner_lon ${DATA}/grids/180x360_SCRIP.20150820.nc | /bin/more
grid_size[0] grid_corners[0] grid_corner_lon[0]=0 
grid_size[0] grid_corners[1] grid_corner_lon[1]=1 
grid_size[0] grid_corners[2] grid_corner_lon[2]=1 
grid_size[0] grid_corners[3] grid_corner_lon[3]=0
...

Running the NCO-generated grid through ESMF_RegridWeightGen produces the expected results and correct interface longitudes. So it looks like a Tempest bug in the longitude interfaces for RLL grids.

Best,
c
************************************************************************
Hi Dave,

> 1. "Is the "Conclusion" section still valid?

> 2. Are there instructions/documentation available for finding and using 
> the correct versions of the code on machines that are broadly accessible 
> (Rhea, Cades, ??), and pointers and instructions for building the source 
> on other machines for the more "typical" ACME participant - (meaning me 
> or someone like me)

I maintain up-to-date NCO on rhea, pileus, cooley, and edison/hopper. 
Also happy to install at other centers.

Many people independently install NCO on their own machines.
Most of UCI's maintenance efforts to update NCO are focused
on high performance computing centers (HPCCs) where we must
manually shepherd new releases through the gates of the system
administrators who produce the "modules" available to all.
Standard Linux distros (Fedora, Ubuntu, Debian) all have 
NCO packages no more than six months old that users can get
with a one-line install.

> 3. Are the tools sufficiently stable that they can be tested, accepted 
> and maintained by the Workflow group as part of the v0 workflow, if this 
> is indeed the approach the Workflow GL plan to use?

I can't speak for what the Workflow Group leaders plan to use.

> Really soon, we plan to start the next round of both low and high 
> resolution v0.1 experiments



> Dave
************************************************************************
This page is under construction...and links will be added in the coming week.

There are numerous problems with grids employed by ACME (and CESM) prior to 20150901. These problems arise from flaws or limitations in the geometry of grids supplied to the utility (ESMF_RegridWeightGen) that generates the weights that regridders apply to convert between the source and destination maps. All tested regridders correctly apply the weights the are supplied, and migrating to improved grids (and to the mapfiles generated from those grids by ESMF_RegridWeightGen or Tempest) improves both the numerical accuracy and the data and metadata completeness and consistency of the regridding procedure. None of the problems described below affect the accuracy of the model results on the native grid. The affected grids include all the FV (plain and staggered grids), Gaussian grids for spectral models, mapfiles produced from those grids, and all mapfiles employing bilinear interpolation. These improved grids improve the accuracy of diagnostics and the aesthetics of plots produced from regridded files.

The four issues identified and fixed are:
1. ACME uses flawed FV grids that omit a small strip of longitude to the east of Greenwich. For FV 129x256, this amounts to 0.2% of global area. The maps based on the flawed grids somehow reapportion area so that total area is conserved (4*pi sr), yet this necessarily redistributes weights from their true positions. This may cause the behavior you noticed (if you are looking at FV grids). Fixed FV grids and maps (with suffix .20150724.nc) are in   
/lustre/atlas/proj-shared/cli115/zender/[grids,maps] on rhea
With these grids, "area"- and "gw"-weighted statistics should agree to double-precision. This problem was identified independently by Charles Doutriaux and myself. Together with the Gaussian grid problems described below, it this shows that ACME (and CESM) should migrate to more accurate structured 2D grids.  

2. SCRIP introduced, and CESM and ACME inherited, a format of storing all coordinates in double precision (yay!). Unfortunately, every Gaussian grid I have examined has grid center latitudes (= sine of the Gaussian quadrature points) accurate to no greater than ~8 digits. This goes all the way back to grandaddy SCRIP. NCO now generates SCRIP-format Gaussian grids accurate to 16 digits.

3. The Gaussian grids employed by SCRIP/CESM/ACME that I have examined (T42, T62, and T85) infer gridcell interfaces as midpoints between the Gaussian quadrature points/angles. Gridcell areas are inferred from the area between gridcell interfaces. The ~single-precision quadrature weights are inconsistent with area determined by the the midpoint rule for interfaces. This may be the problem you noticed (if you are working on your own with Gaussian grids). NCO now uses Newton-Raphson iteration (instead of the quadrature midpoints) to determine the gridcell interface location that exactly matches the area determined by the (now double-precision) Gaussian weights. With these grids, "area"- and "gw"-weighted statistics do agree to double-precision.

4. It is thought that staggered FV grids produced by NCL and fed to ESMF_RegridWeightGen utilize the continuous form of the weighting function (i.e., cosine(lat)) evaluated on the discretized grid, rather than the exact discretized weight function (which depends on the difference of the sine of the latitudes). This leads to a problem similar to the problem with the interface location on the Gaussian grid.

The new grids and mapfiles address these problems, which have always existed in ACME and its predecessors (CESM, CCSM, CCM)...

************************************************************************
mark regridding:
equi-angle polar grids (odd numbers of latitudes) use interface-based area for weight
why don't equi-angle offset grids use interface-based area for weight?

defining interface latitudes for gaussian grids by area-equivalent latitude OK?
any negative downstream effects?


************************************************************************
Thank you Justin, please send them this:

Hello GitHub user "nco",

I hope you are well. My name is Charlie Zender and I am the author and
maintainer of the netCDF Operator (NCO) software suite. A few months
ago the authors of other NCO-related software (python wrappers for
NCO, and the Debian and Fedora NCO maintainers) and I agreed to place
our NCO-related software under the same GitHub organization. We would
like to use the "nco" name for this, and hope you will allow GitHub to
transfer the "nco" username to me for that purpose.

NCO is free software written by and given to students, scientists, and
researchers involved in geophysics and global change research.
We would greatly appreciate the enhanced identity that the "nco"
organization name would provide to our efforts.
Your generosity in transferring this name would be greatly appreciated.
Please contact me personally at zender@uci.edu or 949 891-2429 if you
would like to discuss this.

Thank you for your consideration,
Charlie
************************************************************************
Hello Val,

One way to demonstrate how the netCDF module on rhea is misconfigured is
to use it to try to build NCO. Have you tried that?
Here's a report from a user who built NCO at ALCF, OLCF, and NERSC.
He gives the steps required at each location.
His experience mirrors my own. The netCDF module on OLCF/rhea
requires gymnastics to work-around to complete the NCO build.

Thanks!
c

Hi Charlie,

I built alpha8 on Cooley, Rhea, and (as a bonus) on Edison. Iâ€™ve updated climo_nco.sh to work in all cases. Iâ€™m leaving Pileus for you to update because I donâ€™t have a login there. Iâ€™ve attached the scripts of what I had to do to get it working. Rhea was a real pain because the modules are inconsistent and request files that no longer existâ€¦

So vacation happily,
Peter
************************************************************************
nco_build_cooley-caldwell0608125.txt

Download/install antlr:
--------------------------
1. got antlr 2.7.7 from http://nco.sourceforge.net/#bld
2. followed instructions in INSTALL.txt
3. ./configure --prefix=/home/caldwell/bin/antlr --disable-examples
4. make
5. make test (complained about examples missing... as expected)
6. make install
7. export ANTLR_ROOT=/home/caldwell/bin/antlr #note this is for bash.

Download/install UDUnits:
--------------------------
1. get from http://www.unidata.ucar.edu/software/udunits
2. ./configure -prefix=/home/caldwell/bin/udunits
3. make
4. make install
5. export UDUNITS2_PATH=/home/caldwell/bin/udunits

Specify netcdf locations:
--------------------------
export NETCDF_INC=/soft/libraries/unsupported/netcdf-4.3.2/include
export NETCDF_LIB=/soft/libraries/unsupported/netcdf-4.3.2/lib
export NETCDF_ROOT=/soft/libraries/unsupported/netcdf-4.3.2/

Download nco:
--------------------------
git clone https://github.com/czender/nco.git
cd nco
git pull
./configure --prefix=/home/caldwell/bin/nco
make
make test
make install

may want GSL which exists at /soft/libraries/unsupported/gsl/1.9/gnu
************************************************************************
nco_build_edison-caldwell060815.txt

To build NCO on Edison:

module load netcdf
module load szip
module load gsl

antlr is in /usr/bin, /usr/lib64, /usr/include...

export ANTLR_ROOT=/usr
export NETCDF_ROOT=/opt/cray/netcdf/4.3.0/GNU/48
export NETCDF_LIB=${NETCDF_ROOT}/lib
export NETCDF_INC=${NETCDF_ROOT}/include

git clone https://github.com/czender/nco.git
cd ~/nco
./configure --prefix=/global/homes/p/petercal
make
make test
make install
************************************************************************
nco_build_rhea-caldwell060815.txt

Steps for building NCO on Rhea:
-------------------------------

ATTEMPT TO USE DEFAULT MODULES: 
#==============================
Note: this didn't work because one of these modules (netcdf?) 
was built with hard links to /sw/redhat6/szip/2.1/rhel6_intel11.1.072
which doesn't exist anymore. Charlie's versions of these tools don't
have this problem...

bash #start a new, clean bash shell with nothing defined.
source $MODULESHOME/init/sh
module load netcdf 
module load antlr2
module load udunits
module load gsl
module load PE-intel
module load szip

#need to undefine GSL_LIB: gsl module defines it to be a bunch 
#of include flags, but NCO expects an actual library. If empty,
#NCO creates this command correctly by itself. Note that szip module
#also defines SZIP_LIB in a similar weird way, but NCO doesn't use 
#this info...
export GSL_LIB=""

export ANTLR_ROOT=/autofs/na4_sw/rhea/antlr2/2.7.7/rhel6.6_gnu4.4.7
export UDUNITS_PATH=/sw/rhea/udunits/2.1.24/rhel6.6_gnu4.4.7
export NETCDF_ROOT=/sw/rhea/netcdf/4.1.3/rhel6.4_intel13.1.3
export NETCDF_LIB=${NETCDF_ROOT}/lib
export NETCDF_INC=${NETCDF_ROOT}/include

git clone https://github.com/czender/nco.git ~/nco
cd ~/nco
./configure --prefix=/autofs/nccs-svm1_home1/caldwep/
make
make test
make install

ATTEMPT TO USE CHARLIE'S VERSIONS OF ANTLR, NETCDF, ETC (worked in the end):
#=============================================================
Notes:
1. I get errors about 'can't find libimf.so' (which seems to be an intel math library) when building stuff related to hdf5 unless I 'module load PE-intel'. This seems odd since netcdf and szip say in their paths that they use gcc. But it seems to work.
2. 'module load gsl' defines GSL_LIB to be a list of -I, -i, -L, and -l stuff, but NCO expects it to be a directory. By unsetting it, NCO figures out the correct value. SZIP_LIB is similarly weirdly defined by module load, but NCO doesn't use it.

bash
export ANTLR_ROOT=/ccs/home/zender
export UDUNITS2_PATH=/ccs/home/zender
export NETCDF_ROOT=/sw/redhat6/netcdf/4.3.3.1/rhel6.6_gcc4.8.2--with-dap+hdf4
export NETCDF_LIB=${NETCDF_ROOT}/lib
export NETCDF_INC=${NETCDF_ROOT}/include

source ${MODULESHOME}/init/sh # PMC added 20150607: needed to find module command	
module load PE-intel
module load gsl
export GSL_LIB=""

export PATH='/ccs/home/zender/bin':${PATH}
export LD_LIBRARY_PATH='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_gcc4.8.2--with-dap+hdf4/lib:/sw/redhat6/szip/2.1/rhel6.6_gnu4.8.2/lib:/ccs/home/zender/lib'\:${LD_LIBRARY_PATH}

git clone https://github.com/czender/nco.git ~/nco
cd ~/nco
./configure --prefix=/autofs/nccs-svm1_home1/caldwep/
make
make test
make install
************************************************************************
Hello GitHub,

My username is czender and my most popular repository is czender/nco.
Other users want me to create an "nco" organization where they can
store their nco-related software. I'm happy to oblige but the "nco"
username is already taken. Fortunately, it appears to be owned by me,
because I registered it fours years ago but never really used it.
Now I am unable to login as "nco".

I cannot figure out how to reset the "nco" password.
When I ask for password reset as username "nco" GitHub sends me a
password reset link for "czender" instead.
Would someone at GitHub please help me reclaim the "nco" username
to be used as a new organization for nco-related software?
We would like to distribute from github.com/nco not
githu.com/czender/nco.

Thanks much,
Charlie
************************************************************************
hi henry,
last i heard your official start date is 8/15.
sorry it took so long, that's bureaucracy for you.
in cany case, welcome back!

the top priority for now is to clear out any ncap2 issues.
start with the bug reports by maksym petrenko---he's at NASA
and is an important NCO user. he uses it on big data analysis
workflows of the sort that pay our bills. he reports mainly
to the sf bugs page, not to the forums.

talk to you soon,
c
************************************************************************
bali: contract modifications

Hi Molly,

Most of what's there applies to my courses and can be retained.
Removing the part about NGOs is fine.

Somewhere I want an agreement for TA support.
Students need adequate turnaround and feedback on their assignments.
My goal is a TA (who can work remotedly from UCI) for each course. 
Please try to work this out with Marcia.

I'm traveling until 8/18, and if this contract is the place for such
TA agreements, then perhaps it's best to wait until I return to finish
it so that I can be more involved and attentive.

Thanks!
Charlie

Existing:
Provide opportunities to experience and understand local culture and
perspective on environmental circumstances (including, but not limited
to possible examples are visits to coral reef restoration project,
turtle hatchery, NGOs related to disaster preparedness and relief). 
Suggested:
Provide opportunities to experience and understand local culture and
perspective on environmental circumstances (including, but not limited
to, visits to coral reef restoration project, an active volcano,
NGOs related to disaster preparedness and relief, and guest talks by
by local environmental experts).
************************************************************************
9899. nco: tell pdn@hawaii.edu and ajonko@lanl.gov about nco for cmip

Pedro DiNezio <pdn@hawaii.edu>
Alex Jonko <ajonko@lanl.gov>

Dear Pedro and Alex,

I read with interest, and answered, your survey of needs for CMIP6.
I write to inform you about a software package (which I wrote) that
addresses many  users' needs for handling CMIP-style data.
It is the netCDF Operators (NCO), http://nco.sf.net.
NCO has been used by hundreds or thousands of researchers to analyze
(and prepare for submission to PCMDI) CMIP datasets since CMIP3.
A section of the manual, http://nco.sf.net/nco.html#cmip, is even
devoted to describing advanced processing techniques for CMIP data.

NCO is largely a volunteer project with sporadic funding from NSF,
DOE, and NASA. It is not base-funded like NCL (NCAR), UV-CDAT (DOE),
PRISM (EU), or CDO (MPI). Hence NCO does not have permanent staff
resources to promulgate itself as a solution in every venue. 
Yet the history and wide userbase of NCO on CMIP data attest to its
usefulness. Please consider these factors in any characterizations
you write of software needs and solutions for handling CMIP data.
In particular, consider NCO when requesting/allocating resources
to better handle CMIP6 needs.

If you have any questions about how to use NCO for particular tasks,
do not hesitate to contact me.

Best Regards,
Charlie
************************************************************************
Dear Migration Expert,
We seek a refund for transaction on 20150726, but we are traveling and
email is our only means to fill-in the refund form we obtained; we are
unable to print it and fill-it-in by hand.

Client #: AU-21729806
Full Name: Charles Sutton Zender and Sharilyn Anderson
Telephone Number: 1-949-891-2429
Service Paid for: Electronic Travel Authority
Reasons for requesting a refund:
Migration Expert did not provide our ETAs. We learned we needed ETAs
for US->Australia while trying to obtain boarding passes for our
flight to Australia on the date of travel, 20150726 (~9 PM Pacific).
With only an hour remaining until our flight was to board, we paid
Migration Expert USD $80 for two ETAs. We thought, incorrectly, that a
private business would expedite our ETAs more quickly than the
government immigration website. Migration Expert took our payment
instantly but never provided our ETAs. Time to make our flight
dwindled, and, eventually we had no choice but to attempt to secure
ETAs from the Australian government website. This we did and we
received our (government-issued) ETAs instantly, which allowed us
to obtain our boarding passes, board our flight, and fly to Australia
where we are now. I immediately called Migration Expert, explained
the situation, and requested a refund. Instead I was sent this form.
Migration Expert took our payment, never provided any services, and
our reliance on this company almost led us to miss our flight.
Please refund our payment. Thank you for your consideration.
Sincerely,
/s/ Charlie and Shari
Credit Card #: 5524790038764207 
Expiry Date: 07/19
Cardholder's Name + Billing address:
Charles S. Zender
3 Whistler Ct.
Irvine, CA 92617
************************************************************************
Phil, The issue you raise is now understood to arise from two or three distinct problems with grids, not with the regridders per se. In other words, the regridders are correctly applying the given weights, but there are flaws with the FV grids that (at least) ACME uses, and with the Gaussian grids that CESM and ACME use. NCO 4.5.2-alpha7 (which _is_ in my home directory on rhea) contains the fixes, and I'll update this regridder page and write a self-contained Confluence page on migrating to correct grids when I return.

The three issues identified and fixed are:
1. ACME uses flawed FV grids that omit a small strip of longitude to the east of Greenwich. For FV 129x256, this amounts to 0.2% of global area. The maps based on the flawed grids somehow reapportion area so that total area is conserved (4*pi), yet this necessarily redistributes weights from their true positions. This may cause the behavior you noticed (if you are looking at FV grids). Fixed FV grids and maps (with suffix .20150724.nc) are in   
/lustre/atlas/proj-shared/cli115/zender/[grids,maps] on rhea
With these grids, "area"- and "gw"-weighted statistics should agree to double-precision. This problem was identified independently by Charles Doutriaux and myself. Together with the Gaussian grid problems described below, it this shows that ACME (and CESM) should migrate to more accurate structured 2D grids.  

2. SCRIP introduced, and CESM and ACME inherited, a format of storing all coordinates in double precision (yay!). Unfortunately, every Gaussian grid I have examined has grid center latitudes (= sine of the Gaussian quadrature points) accurate to no greater than ~8 digits. This goes all the way back to grandaddy SCRIP. NCO now generates SCRIP-format Gaussian grids accurate to 16 digits.

3. The Gaussian grids employed by SCRIP/CESM/ACME that I have examined (T42, T62, and T85) infer gridcell interfaces as midpoints between the Gaussian quadrature points/angles. Gridcell areas are inferred from the area between gridcell interfaces. The ~single-precision quadrature weights are inconsistent with area determined by the the midpoint rule for interfaces. This may be the problem you noticed (if you are working on your own with Gaussian grids). NCO now uses Newton-Raphson iteration (instead of the quadrature midpoints) to determine the gridcell interface location that exactly matches the area determined by the (now double-precision) Gaussian weights. With these grids, "area"- and "gw"-weighted statistics do agree to double-precision.

4. There is a potential issue with the staggered FV grid, similar to the problem with the interface location on the Gaussian grid. I will address that soon.

I learned all this shortly before embarking on travels and alerted Peter and Mark only to #1. It was suggested and I agreed to write a Confluence page on basically, why and howto migrate to new grids. Because it is hard to retract flawed- and/or migrate to better grid/mapfiles, I hesitate to publish more about the new grid/mapfiles on a public page before testing them more thoroughly and tweaking the metadata a little. The curious reader will find the exact commands in the "history" attribute of the referenced grid/mapfiles. 

Since these problems have existed for so long, perhaps their exhumation can wait a few more weeks when I can be more attentive. I welcome any feedback from early adopters of the new grids, and suggestions for a migration strategy and accompanying announcement.  

c
************************************************************************
I've finished adding missing-value support to climatology2.py; it wasn't there in time for Peter's tests.  Yesterday I ran climo_test.sh and looked at three variables - CLDLOW, BURDENDUST, and AODVIS.  All three climatology codes agreed on the first two to O(1.e-7).  They all disagreed on AODVIS.  For AODVIS, the greatest disagreement between NCO and UVCDAT was at index 36360, so that's what I looked at closely.
At that index, AODVIS has a March value for all three years 1980-1982, but April and May values are always missing.  My definition of the "correct" average for this situation is (AODVIS[March,1980]+AODVIS[March,1981]+AODVIS[March,1982])/3 - that is, completely ignore the missing data.  I computed this in basic interactive Python, and got the same value had been written by climatology2.py, call it AODVIScdat.
The other two codes, call them amwg and nco, differed.  The averages were related as follows:
92 * AODVISamwg = 31* AODVIScdat ;  i.e.,
3*92* AODVISamwg = 93* AODVIScdat
92 * AODVISnco  = 93 * AODVIScdat
It looks as though all three codes add weighted values the same way, i.e. 31*AODVIS[March,1980]+31*AODVIS[March,1981]+31*AODVIS[March,1982].  But they divide, i.e. "normalize", by a different total weight.  AMWG uses 3*(March+April+May), where the month name is shorthand for its number of days.  Thus AMWG completely ignores missing values when normalizing.  NCO uses (March+April+May).  The new climatology2, and my hand calculation, use 3*March.
Conclusions:
AMWG-computed climatologies should not be used for variables with missing values.
NCO has a small error, a factor of 1/93, in this probably-worst case.  It is likely to be an easily-fixed bug.  It's like using a row of the weight matrix when a column should be used.
UV-CDAT (climatology2.py) is right on this matter, although I welcome corrections!
************************************************************************
ncks -O -C -d time,1 -v one_dmn_rec_var_flt_mss ~/nco/data/in.nc ~/foo1.nc
ncap2 -O -h -s 'one_dmn_rec_var_flt_mss/=2' ~/foo1.nc ~/foo1.nc
ncks -O -C -d time,0 -v one_dmn_rec_var_flt_mss ~/nco/data/in.nc ~/foo2.nc
ncks -O -C -d time,0 -v one_dmn_rec_var_flt_mss ~/nco/data/in.nc ~/foo3.nc
ncra -O -D 6 -w 3,2,1 ~/foo1.nc ~/foo2.nc ~/foo3.nc ~/foo.nc
ncks -C -H -s '%g' -v one_dmn_rec_var_flt_mss ~/foo.nc

ncks -O -C -d time,0 -v one_dmn_rec_var_flt_mss ~/nco/data/in.nc ~/foo1.nc
ncks -O -C -d time,1 -v one_dmn_rec_var_flt_mss ~/nco/data/in.nc ~/foo2.nc
ncks -O -C -d time,2 -v one_dmn_rec_var_flt_mss ~/nco/data/in.nc ~/foo3.nc
ncra -O -D 6 -w 1,2,3 ~/foo1.nc ~/foo2.nc ~/foo3.nc ~/foo.nc
ncks -C -H -s '%g' -v one_dmn_rec_var_flt_mss ~/foo.nc

ncks -O -C -d time,0 -v one_dmn_rec_var_flt ~/nco/data/in.nc ~/foo1.nc
ncks -O -C -d time,1 -v one_dmn_rec_var_flt ~/nco/data/in.nc ~/foo2.nc
ncks -O -C -d time,2 -v one_dmn_rec_var_flt ~/nco/data/in.nc ~/foo3.nc
ncra -O -D 6 -w 1,2,3 ~/foo1.nc ~/foo2.nc ~/foo3.nc ~/foo.nc
ncks -C -H -s '%g' -v one_dmn_rec_var_flt ~/foo.nc
************************************************************************
climo_nco.sh -c famipc5_ne30_v0.3_00003 -s 1980 -e 1982 -i ${DATA}/ne30/raw -o /Users/zender/data/ne30/clm > ~/climo_nco.txt 2>&1 &
ncks -C -H -d ncol,36360 -v AODVIS /Users/zender/data/ne30/raw/famipc5_ne30_v0.3_00003.cam.h0.1980-03.nc 
ncks -C -H -d ncol,36360 -v AODVIS /Users/zender/data/ne30/raw/famipc5_ne30_v0.3_00003.cam.h0.1981-03.nc 
ncks -C -H -d ncol,36360 -v AODVIS /Users/zender/data/ne30/raw/famipc5_ne30_v0.3_00003.cam.h0.1982-03.nc 
ncks -C -H -d ncol,36360 -v AODVIS ${DATA}/ne30/clm/famipc5_ne30_v0.3_00003_03_198003_198203_climo.nc
ncks -C -H -d ncol,36360 -v AODVIS ${DATA}/ne30/clm/famipc5_ne30_v0.3_00003_MAM_198003_198205_climo.nc

ncra -D 6 -O -C -d ncol,36360 -w 31,30,31 -v AODVIS ${DATA}/ne30/clm/famipc5_ne30_v0.3_00003_03_198003_198203_climo.nc ${DATA}/ne30/clm/famipc5_ne30_v0.3_00003_04_198004_198204_climo.nc ${DATA}/ne30/clm/famipc5_ne30_v0.3_00003_05_198005_198205_climo.nc ~/foo.nc

ncra -O -D 6 -w 1,2,3 ~/foo1.nc ~/foo2.nc ~/foo3.nc ~/foo.nc

************************************************************************
Jeff pointed out a previously unnoticed bug in the NCO climos. It was due to the "ncra -w" missing value treatment introduced in ncra in 4.4.9 in May. Thanks, Jeff. The more eyes that look at this, the better.

This flaw has now been fixed. Unevenly weighted inputs (like MAM, where the number of days varies by month) did not correctly normalize missing values. So seasonal and annual climo files (MAM, JJA, SON, DJF, and ANN) were corrupted. This produced, in affected files, answers up to ~1% wrong at gridcells with missing values. About ~15 CAM fields have missing values, and ~650 fields do not. The monthly files (01, 02, ... 12) were unaffected for all fields (as Peter's evaluation implies) because the weights were even.

How did we miss this in testing? As Peter said, we had three models that disagreed, and did not have a known good answer. We focused on looking at the global means that we understood (because if the global means match to double precision you know the methods agree). In hindsight, it would have smart to also look at the single gridcells with the greatest anomalies, as Jeff did. A good lesson to learn.

The fix is in 4.5.2-alpha8 and later. It does not significantly alter climo performance or total memory use. I have verified the specific example Jeff noted. If anyone notices any other anomalies please let me know. Your feedback and suggestions are always welcome!
************************************************************************
PS: I compareed NCO climo performance on pileus vs rhea two weeks ago.
Ran ~165 years of ne30 climo from a B1850 run in Marcia's directory.
Took ~18m00s on one, ~17m30s on the other, nearly indistinguishable.
For ne120 climos, NCO uses load balancing where 12 nodes is optimal.
That's easy to get on rhea not pileus, which has < 12 nodes.
IOW, the NCO balancing for pileus may need to be specially optimized
for 6-8 nodes. But, doesn't look like pileus is sig. faster than rhea.
I would be cautious about abandoning rhea for pileus since rhea has
so much more room and redundancy.
************************************************************************
Hi All,

The level-of-detail and depth of analysis here are impressive, Peter.
Once again, you have helped put a number of disparate tools in a
coherent framework, and the results should help focus our efforts.
Your report has uncovered some hitherto unknown characteristics of the
AMWG diagnostics that are good to know, understand, and move beyond.

I'm confident that NCO improves on AMWG answers, and I'd bet that
UV-CDAT is also more accurate than AMWG in most cases, because AMWG
weighting is accurate only to single precision. You've identified some
aspects of AMWG climos that we may never understand. It's gratifying
that NCO can reproduce AMWG answers when instructed to use their
flawed weights and normalization. That bolsters confidence that we
can abandon their answers as the standard, and focus on improved
accuracy and speed.

Please let me know if there is anything else I can do to get fast
and accurate climatologies into the hands of ACME researchers.

c
************************************************************************
hopper:
module add netcdf
************************************************************************
nersc.txt
Please submit a new user account application form at: nim.nersc.gov
Please request a regular MPP account (account type: standard).
In the description, please put:
work with Helen on installing NCO on NERSC systems.
desired repo: mpccc1, PI: Sudip Dosanji.
The Account Support team will process your request, and send you a link to setup your password.
************************************************************************
Uber $15-20

Train, walk via Central Station Platform 24: 23min, $16
International Airport Station: 12min - every 10 minutes $13

Central Station Platform 24
Line T4 Eastern Suburbs & Illawarra Line
4min - every 5 minutes $3

Martin Place Station Platform: 5min - 1623 feet
************************************************************************
Fly
Bound for Australia we fling ourselves through darkness without end
Splayed on seats like a fat chimp tribe retreated to sparse foliage
Limbs akimbo, jostling on saplings too narrow, perched and restless
Stars throw our shadows on nodes of clouds adrift in unseen gyres below


************************************************************************
Hi Mark and Peter,

I depart Sunday for three weeks of travel and there's an ACME-related
issue I'd like to apprise you of before I disappear. 
The default rectantangular SCRIP grids for regridding are flawed.
Charles Doutriaux and I both noticed this independently.
This issue definitely affects plotting programs, and it can affect
accuracy of diagnostics not performed on the native grid.
We discussed this briefly on a telecon and I've since looked into it.
Here are my conclusions:

The flawed grids lack a small wedge of longitude west of Greenwich.
The xc=grid_center_lon variable is correct, and the xv=grid_corner_lon  
variable is incorrect because xv[0] coincides with xc[0] at Greenwich.
Software that employs the xv values can propagate the errors.
For example, xv and yv are often used to construct gridcell area.
Due to the missing longitude wedge, the fv129x256 grid is about 0.2%
shy of the true global area.

As of 4.5.2-alpha6, NCO can produce its own SCRIP files for most 2D
rectangular grids. I've generated candidates to replace the grids and 
mapfiles affected by this problem. They seem to perform as expected.

Flawed grids are hard to "recall", so I hope you might inspect these 
candidate grids and mapfiles before they see wider distribution.
Perhaps when I return from travels we can discuss whether/how to
migrate towards the correct grids/maps, which are on rhea in

/lustre/atlas/proj-shared/cli115/zender/[maps,grids]

Generating the grids requires the latest NCO (in ~zender/[bin,lib]):

Grids:
ncks -O -D 1 --rgr grd_ttl='FV-scalar grid 129x256' --rgr grid=${DATA}/grids/129x256_SCRIP.20150724.nc --rgr lat_nbr=129 --rgr lon_nbr=256 --rgr lat_typ=ngl_eqi_pol --rgr lon_typ=Grn_ctr  ~/nco/data/in.nc ~/foo.nc
ncks -O -D 1 --rgr grd_ttl='FV-scalar grid 257x512' --rgr grid=${DATA}/grids/257x512_SCRIP.20150724.nc --rgr lat_nbr=257 --rgr lon_nbr=512 --rgr lat_typ=ngl_eqi_pol --rgr lon_typ=Grn_ctr  ~/nco/data/in.nc ~/foo.nc
ncks -O -D 1 --rgr grd_ttl='FV-scalar grid 801x1600' --rgr grid=${DATA}/grids/801x1600_SCRIP.20150724.nc --rgr lat_nbr=801 --rgr lon_nbr=1600 --rgr lat_typ=ngl_eqi_pol --rgr lon_typ=Grn_ctr  ~/nco/data/in.nc ~/foo.nc

Mapfiles:
ESMF_RegridWeightGen -s ${DATA}/grids/ne30np4_pentagons.091226.nc -d ${DATA}/grids/129x256_SCRIP.20150724.nc -w ${DATA}/maps/map_ne30np4_to_fv129x256_aave.20150724.nc --method conserve
ESMF_RegridWeightGen -s ${DATA}/grids/ne30np4_pentagons.091226.nc -d ${DATA}/grids/257x512_SCRIP.20150724.nc -w ${DATA}/maps/map_ne30np4_to_fv257x512_bilin.20150724.nc --method bilinear
ESMF_RegridWeightGen -s ${DATA}/grids/ne120np4_pentagons.100310.nc -d ${DATA}/grids/257x512_SCRIP.20150724.nc -w ${DATA}/maps/map_ne120np4_to_fv257x512_aave.20150724.nc --method conserve
ESMF_RegridWeightGen -s ${DATA}/grids/ne120np4_pentagons.100310.nc -d ${DATA}/grids/801x1600_SCRIP.20150724.nc -w ${DATA}/maps/map_ne120np4_to_fv801x1600_bilin.20150724.nc --method bilinear

Once I've finished my travels, heard your feedback, and digested the 
implications, I can post this info to Confluence if you'd like.

Best,
c
************************************************************************
ncks -v GS_bc_a1,SFso4_a2,V,ncl_c2SFSBS,num_c1_sfcsiz2 -H ~/amwg_mam_dff.nc | more 
GS_bc_a1 = 6.77626e-21 
SFso4_a2 = -3.38813e-21 
V = 1.16415e-10 
ncl_c2SFSBS = 8.47033e-22 
num_c1_sfcsiz2 = -1.81899e-12 
************************************************************************
Hi Todd,

I making NCO more useful for regridding and analysis in ACME.
It now works well for the atmosphere and land models.
I write for help to ensure NCO can serve ocean and sea-ice needs too.
My understanding from our airport chat is that the ocean group does
its own "on-line" diagnostics, so testing NCO on the ocean model(s)
is, currently, for my peace of mind, to be sure NCO can be used in a
pinch if needed. 

So, two questions:

1. Would you please point me to an ocean model simulation and the
appropriate mapfile to regrid it to a rectangular lat/lon grid?
Same for the sea ice model. I've looked at this simulation on rhea:

/lustre/atlas1/cli115/world-shared/mbranst/B1850C5e1_ne30/ocn/hist
/lustre/atlas1/cli115/world-shared/mbranst/B1850C5e1_ne30/ice/hist

But I'm unfamiliar with the naming conventions of the POP and cice
models, and how to find the right map-files for those grids....

2. Is there a already in the ocean group a "template simulation"
that you recommend I use for any regridding/climatology benchmarks I
create? 

Thanks!
Charlie
************************************************************************
ncra -O --no_nrm -w 0.33695652173913043478,0.32608695652173913043,0.33695652173913043478 -p ${DATA}/ne30/climo famipc5_ne30_v0.3_00003_03_climo.nc famipc5_ne30_v0.3_00003_04_climo.nc famipc5_ne30_v0.3_00003_05_climo.nc ~/amwg_mam_sum_nco_wgt.nc
ncwa -w area -O ~/amwg_mam_sum_nco_wgt.nc ~/amwg_mam_sum_nco_wgt_avg.nc

ncra -O --no_nrm -w 0.3369565308094025,0.3260869681835175,0.3369565308094025 -p ${DATA}/ne30/climo famipc5_ne30_v0.3_00003_03_198003_198203_climo.nc famipc5_ne30_v0.3_00003_04_198004_198204_climo.nc famipc5_ne30_v0.3_00003_05_198005_198205_climo.nc ~/nco_mam_sum_amwg_wgt.nc
ncwa -w area -O ~/nco_mam_sum_amwg_wgt.nc ~/nco_mam_sum_amwg_wgt_avg.nc

ncwa -w area -O ${DATA}/ne30/climo/famipc5_ne30_v0.3_00003_MAM_198003_198205_climo.nc ~/nco_mam_avg.nc
ncwa -w area -O ${DATA}/ne30/climo/famipc5_ne30_v0.3_00003_MAM_climo.nc ~/amwg_mam_avg.nc

ncdiff -O ~/amwg_mam_sum_nco_wgt_avg.nc ~/nco_mam_avg.nc ~/nco_mam_dff.nc
ncdiff -O ~/nco_mam_sum_amwg_wgt_avg.nc ~/amwg_mam_avg.nc ~/amwg_mam_dff.nc

ncks -H ~/amwg_mam_dff.nc | more
ncks -H ~/nco_mam_dff.nc | more
************************************************************************
Hi Carmen, hope you are well. Yours is the only phone number I have, and I think you might be in Mexico so I am texting. Feel free to call back. Carlos promised to remove the playset from my backyard yesterday but he did not show up. This is about the third time he has delayed removing the playset, which is only here because he asked me to keep it for him. I will leave on vacation, and renters will arrive, in one week. I want that playset removed before they arrive. If the playset has not been removed by 7/25 then I must, unfortunately, terminate your monthly services (because I cannot trust Carlos anymore). I guess I will find someone else to help clean my house. Since I will be on vacation, please skip (do not come) 8/10. If the playset has been removed, then come again on 9/7, as scheduled. Best, Charlie
************************************************************************
conus cooley
9843. nco: rgr daily data allowances and h1 tapes
      /home/wlin/EM/archive/NUV_ACMEv02pr_interim_FC5_conusx4v1_conusx4v1/atm/hist
      /home/wlin/EM/archive/NUV_ACMEv02pr_interim_FC5_conusx4v1_conusx4v1/atm/hist/NUV_ACMEv02pr_interim_FC5_conusx4v1_conusx4v1.cam.h0.2011-01.nc
      NUV_ACMEv02pr_interim_FC5_conusx4v1_conusx4v1.cam.h1.2011-01-01-00000.nc

climo_nco.sh -d 1 -v AODVIS,FSNT -c NUV_ACMEv02pr_interim_FC5_conusx4v1_conusx4v1 -f conus -s 2011 -e 2011 -i /projects/HiRes_EarthSys/wlin/archive/NUV_ACMEv02pr_interim_FC5_conusx4v1_conusx4v1/atm/hist -o ${DATA}/conus/clm > ~/climo_nco.txt 2>&1 &
    
climo_nco.sh -d 1 -c B1850C5e1_ne30 -s 0002 -e 0167 -i /lustre/pfs1/cades-esgf/world-shared/B1850C5e1_ne30/atm/hist -o ${DATA}/ne30/clm > ~/climo_nco.txt 2>&1 &

climo_nco.sh -d 1 -c B1850C5e1_ne30 -s 0002 -e 0167 -i /lustre/atlas1/cli115/world-shared/mbranst/B1850C5e1_ne30/atm/hist -o ${DATA}/ne30/clm > ~/climo_nco.txt 2>&1 &
    
climo_nco.sh -d 1 -m clm2 -c B1850C5e1_ne30 -f clm2 -s 0002 -e 0003 -i /lustre/pfs1/cades-esgf/world-shared/B1850C5e1_ne30/lnd/hist -o ${DATA}/ne30/clm -R '--rgr col_nm=lndgrid' -r ${DATA}/maps/map_ne30np4_to_fv129x256_aave.150418.nc > ~/climo_nco.txt 2>&1 &

climo_nco.sh -d 1 -v Tsfc -m cice -h h -c B1850C5e1_ne30 -f cice -s 0002 -e 0003 -i /lustre/pfs1/cades-esgf/world-shared/B1850C5e1_ne30/ice/hist -o ${DATA}/ne30/clm > ~/climo_nco.txt 2>&1 &
    
climo_nco.sh -d 1 -v TEMP,QSW_HTP -m pop -h h -c B1850C5e1_ne30 -f pop -s 0002 -e 0003 -i /lustre/pfs1/cades-esgf/world-shared/B1850C5e1_ne30/ocn/hist -o ${DATA}/ne30/clm > ~/climo_nco.txt 2>&1 &
    
************************************************************************
Amy? Amy Hennig? Yes! You do read Facebook. I hope you are well. That picture of the Renaissance Faire did not do us justice. We were so ahead of our time! So glad we both found interesting and creative work, and more, in life. To fast forward thirty years, reserve catching-up for later (less than thirty years?), I write you now because I name-dropped you to my girlfriend, who mentioned you to her friend, who asked me to ask you a favor. 

My girlfriend's friend's son is Connor Dean, a 20-something who studies Game Design at Quinnipiac University. He would like to talk with you about working in that field. If you are amenable, he will make contact by phone or email, however you prefer. No worries if not. 

Charlie
************************************************************************
GenerateRLLMesh --lat 180 --lon 360 --file ${DATA}/rgr/msh_180x360.g
GenerateOverlapMesh --a ${DATA}/rgr/msh_180x360.g --b ${DATA}/grids/california_25km.nc --out ${DATA}/rgr/msh_ovr_180x360_to_ca25km.g
GenerateOfflineMap --in_mesh ${DATA}/rgr/msh_180x360.g --out_mesh ${DATA}/grids/california_25km.nc --ov_mesh ${DATA}/rgr/msh_ovr_180x360_to_ca25km.g --out_map ${DATA}/maps/map_180x360_to_ca25km.20150715.nc
ncks -M -m -H -v xc_b,yc_b ${DATA}/maps/map_180x360_to_ca25km.20150715.nc | more

************************************************************************
pwjones@lanl.gov
Hi Phil,

First, SCRIP is awesome and you wrote it, and it stands the test of time.
Well done. NCO has supported SCRIP and SCRIP-style files for a few months
now, but NCO still does not yet support all of SCRIP's features.
In particular, NCO does not support SCRIP 2nd order conservative regridding  
or SCRIP bicubic interpolation---the methods with num_wgt != 1.
ESMF (I think) seems to handle those differently from SCRIP, and never has 
an "inner-loop" over num_wgts weights. I want NCO to support all SCRIP
(and ESMF) files, hence I'm asking you how to implement the num_wgts loop.

Here is the pseudocode (in C-notation) for num_wgts == 1:

for(lnk_idx=0;lnk_idx<lnk_nbr;lnk_idx++){
  // Normalization: fractional area (fracarea)
  dst[ddr_dst[lnk_idx]]+=src[ddr_src[lnk_idx]]*remap_matrix[lnk_idx,0];
  // Normalization: destination area (destarea)
  dst[ddr_dst[lnk_idx]]+=src[ddr_src[lnk_idx]]*remap_matrix[lnk_idx,0]/dst_area[ddr_dst[lnk_idx]];
  // Normalization: none
  dst[ddr_dst[lnk_idx]]+=src[ddr_src[lnk_idx]]*remap_matrix[lnk_idx,0]/(dst_area[ddr_dst[lnk_idx]]*dst_frc[ddr_dst[lnk_idx]);
} // end loop over lnk

When num_wgts != 1, then what is the SCRIP pseudo-code?
My understanding is there must be an inner loop over num_wgts, but I'm
not sure what template to apply over the inner-loop.

I'll happily call you if you'd prefer to talk by phone rather than type
a response.

Thanks in advance,
Charlie
************************************************************************
FYI I implemented the changes necessary for NCO's regridder to understand Tempest 1D<->ND global mapfiles (so ACME cubed-sphere grids work), and also the changes necessary for NCO to understand a Tempest regionally refined (RR) mapfile (so far tested only on a CONUS mapfile). These improvements are all in NCO 4.5.1, just released. The release notes do not emphasize the regional grid capabilities because only a single mapfile, provided by Wuyin Lin, has been tested: map_conusx4v1_to_fv0.1x0.1_US_bilin.nc

I have no idea how representative this is of other RR grids/maps produced by Tempest or Squadgen. I want to test other RR grids/mapfiles that are qualitatively different than CONUS. If you have such a mapfile (and sample model data to regrid), please test it with ncks --map=map.nc in.nc out.nc and send me any feedback. Binaries in ~zender/bin/ncks on Yellowstone, Rhea, and (soon) Cooley. Or make your files available so I may test it. ncks has some useful features like it tries to add appropriate latitude weights and area when the mapfile provides none. However, I'm sure users will think of other features they want, and I'm eager to get the RR and regional features working well. 

cz
************************************************************************
C. ncdismember was a one-stop shop for flattening and CF-checking that
   supported netCDF4 flattening and two (and counting) CF-checkers.
   We implemented a getopt() interface for precision and extensibility.
   Now users can independently specify whether they want flattening,
   and which CF checker to employ, and other minor conveniences.
   Given that most of its use is ultimately for CF-checking, we have
   re-dubbed ncdismember as nccf. The ncdismember name is now
   deprecated and will be removed when you least expect it. Kidding.
   Using no checker, default (Decker) checker, NERC checker:
   nccf ~/nco/data/mdl_1.nc -t /tmp
   nccf ~/nco/data/mdl_1.nc -t /tmp -c decker
   nccf ~/nco/data/mdl_1.nc -t /tmp -c nerc
   http://nco.sf.net/nco.html#ncdismember
   http://nco.sf.net/nco.html#nccf

************************************************************************
Let me give a fuller picture of the "climatology" issues you raise. First, thank you for intercomparing all the climo codes. It's important to get an overview and feedback.

Here is what I mean by "the requirements need to change a bit": I learned that the ANN file is exceptional and should not implement "climatology", and should instead stick with "bounds" because it is a straight-up time-average well-described by the contiguous description "bounds" is intended for. The rest of the climo files are not, they are statistics on repeating subsets of the time domain. Currently the NCO climo code adds "climatology" to ANN (and all other files). The fix is not difficult with ncatted. Requirement 12b should insert the qualifier phrase "if appropriate" after "attribute". 

The last time I checked, adding "climatology" incurred an additional 25% (or 20-30 minutes, not a doubling) to generating ne120 climos. This was before I implemented load balancing so it may be quicker now. And it is possible to hack a special-purpose routine that would reduce that time nearly to zero. Such a hack would, I think, be ugly to the host code and the climo script, and involve trade-offs between complexity and portability (to other models and datasets) of the climo script. For ne30 climos the time penalty is negligible because the whole climatology takes only a few minutes. Rhea takes a long time to open, modify, and close an ne120-sized file. Perhaps this is unavoidable due to size, or maybe because of lustre, not sure. Pending a faster solution for ne120-sized climos, I made adding "climatology" optional (with -x Yes) in climo_nco.sh. 

The "climatology" information is indeed encoded in the filename, and easier to discern there than from the history attribute. If I had to guess, I'd say that, after some not-too-ugly optimizations, "climatology" could be included (and improve CF-compliance) in ne120 climos with by paying a ~10% (~10 minute) penalty (on rhea). I hope this gives a fuller picture so that AG can make a more informed decision about whether "climatology" should be required, optional, deferred, omitted...
************************************************************************
Hi Peter,

You raise a good question. Glad you're comparing the climos.
My understanding is that workflow should be doing this.
But their work does not flow in a public manner so I'm unsure what
conclusions they've reached. Your efforts are worthwhile.

The history attribute of the AMWG seasonal climatologies makes clear
the method AMWG attempts to use to perform the seasonal weighting.
They use ncflint to weight each month.
One odd thing is that the history does not show the command needed to
recombine the weighted months together into a seasonal mean.
This may be an oversight on my part, but I expect the ncflint command
to be followed by an ncra -y ttl command to add three files together.
Clearly something is missing from history because the AMWG MAM file
cannot be created out of thin air. I'll assume that ncra was invoked
and that its history was not recorded for an unknown reason.

climo_nco.sh weights March as 31/(31+30+31)=0.33695652173913043478.
AMWG weights March as 0.3369565308094025. Close but no cigar.
climo_nco.sh uses the weights we agreed on and AMWG does not. 
This small weighting difference will cause discrepancies between
AMWG and NCO climatologies. The discrepancy is in the eighth digit.
There are ~50000*3*3=~500k float operations, which means you expect
to lose another three digits of precision. Answers that agree to four
decimal places are therefore indistinguishable. AODVIS is on the
hairy edge of that, and FSNT is definitely fine. In short, the
AMWG<->NCO differences can be traced to different weights and
therefore do not worry me. Are there other, larger discrepancies
that concern you?

zender@rhea-login2g:~$ ncks -H -v FSNT,AODVIS -C MAM_amwg_avg.nc 
AODVIS = 0.18134 
FSNT = 235.351 

zender@rhea-login2g:~$ ncks -H -v FSNT,AODVIS -C MAM_cdat_avg.nc 
AODVIS = 1.69183937012e+18 
FSNT = 235.707750363 

zender@rhea-login2g:~$ ncks -H -v FSNT,AODVIS -C MAM_nco_avg.nc 
AODVIS = 0.181404 
FSNT = 235.351 

c
************************************************************************
Hi Peter,

1. I changed the filenames to include _climo.
2. Simplified the script to use filename array everywhere.
   Seasonal and annual commands are much cleaner now.
3. Removed 'time' command from examples
4. Changed module command to swap rather than load PE-GNU

It should run in ~1m30s on one dedicated node.
For ne30, only request 1 node. Do not use threading (because so many
threads would interfere with eachother on one node with ~16 cores).
To be clear, just do this:

qsub -I -A CLI115 -V -l nodes=1 -l walltime=03:00:00 -N climo_nco
climo_nco.sh -a sdd -c famipc5_ne30_v0.3_00003 -s 1979 -e 1983 -i /lustre/atlas1/cli115/world-shared/mbranst/famipc5_ne30_v0.3_00003-wget-test -o /lustre/atlas/proj-shared/cli115/zender/ne30/clm > ~/climo_nco.txt 2>&1 &

Run 'top' at the same time and you'll see the parallelism in action.
Just call me if this doesn't work painlessly. I'm see we can
figure-out any changes needed.

c
************************************************************************
Hi Molly,

Overall the flyer looks great (too great?), and the text is fine.
Please

1. Eliminate comma in flyer subtitle
2. Fix busted f-ligatures in course descriptions "loods, ..., ires"
3. Hyphenate "open air"

The main change I wand is to include some scientific content.
Please replace front lower left or right picture with attached.



Thanks!
c
************************************************************************
9702. nco: clm ncra wgt_nm
      ncks -O -C -d time,,1 -v one_dmn_rec_wgt,one_dmn_rec_var_flt -p ${HOME}/nco/data h0001.nc ~/foo.nc
      ncra -O -D 2 -w one_dmn_rec_wgt -v one_dmn_rec_var_flt ~/foo.nc ~/foo2.nc
9703. nco: clm ncra not keeping running sum nor tally. no hysterisis
************************************************************************
coverity.txt

https://scan.coverity.com/download?tab=cxx
cd ${DATA}/tmp
wget https://scan.coverity.com/download/cxx/linux-64
mv ~/Downloads/cov-analysis-linux64-7.6.0.tar.gz ${DATA}/tmp
tar xvzf cov-analysis-linux64-7.6.0.tar.gz
export PATH=${DATA}/tmp/cov-analysis-linux64-7.6.0/bin:${PATH}
cd ~/nco
************************************************************************
netcdf map_ne120np4_to_ne30np4_aave.20150614 {
  dimensions:
    dst_grid_rank = 1 ;
    n_a = 777602 ;
    n_b = 48602 ;
    n_s = 1217786 ;
    nv_a = 5 ;
    nv_b = 5 ;
    src_grid_rank = 1 ;

sz(ne30)=5440
sz(ne120)=
ne120->ne30 segfault:
lnk_idx=4904142
row_dst_adr[4904142]=16806
var_sz_out=5400 (correct for ne30)

netcdf map_ne120np4_to_ne30np4_tps.20150613 {
  dimensions:
    dst_grid_rank = 1 ;
    n_a = 777602 ;
    n_b = 48602 ;
    n_s = 14045402 ;
    nv_a = 1 ;
    nv_b = 1 ;
    src_grid_rank = 1 ;

Hi All,

Thanks for responding, Mark. NCO uses the same remapping algorithm,
and it regrids ESMF, SCRIP, and Tempest RLL files fine. There is a 
bug either in the Tempest 1D mapfiles or in the NCO treatment of
Tempest 1D mapfiles in particular (NCO does ESMF 1D->ND fine).
I think I have tracked down the issue to its root cause.

The dimension size n_b could be used to determine 1D grid sizes, but
n_b alone is insufficient for 2D grids, so NCO always uses
dst_grid_dims to determine output dimension sizes. Tempest produces 
different values of dst_grid_dims and src_grid_dims than ESMF.
When NCO uses Tempest values, the gods become angry.
Values of dst/src_grid_dims from ESMF and Tempest mapfiles are below.

Is there a good explanation for why ESMF and Tempest differ?
I inferred that ApplyOfflineMap utilizes n_b (not dst_grid_dims).
Sure enough, ApplyOfflineMap fails when asked to apply ESMF 1D->1D
mapfiles. Based on SCRIP definitions of dst/src_grid_dims, this seems
like a Tempest bug. 

Thanks for continued feedback and fixes!
c

zender@roulee:/data/zender/maps$ ncks -H -v dst_grid_dims,src_grid_dims --cdl map_ne120np4_to_ne30np4_aave.20150614.nc
netcdf map_ne120np4_to_ne30np4_aave.20150614 {
  dimensions:
    dst_grid_rank = 1 ;
    src_grid_rank = 1 ;

  variables:
    int dst_grid_dims(dst_grid_rank) ;

    int src_grid_dims(src_grid_rank) ;

  data:
    dst_grid_dims = 48602 ;

    src_grid_dims = 777602 ;

} // group /
zender@roulee:/data/zender/maps$ ncks -H -v dst_grid_dims,src_grid_dims --cdl map_ne120np4_to_ne30np4_tps.20150613.nc 
netcdf map_ne120np4_to_ne30np4_tps.20150613 {
  dimensions:
    dst_grid_rank = 1 ;
    src_grid_rank = 1 ;

  variables:
    int dst_grid_dims(dst_grid_rank) ;
      dst_grid_dims:name0 = "num_elem" ;

    int src_grid_dims(src_grid_rank) ;
      src_grid_dims:name0 = "num_elem" ;

  data:
    dst_grid_dims = 5400 ;

    src_grid_dims = 86400 ;

} // group /

************************************************************************
Hi Kate,

I've written a climatology generator that might suit your needs. 
It uses NCO, not UV-CDAT, so I wanted to check with Dean before
mentioning it to you. He's OK with it. This NCO-based generator is
much faster than the AMWG diagnostics-based climo generator.
It's ready now, runs on rhea and cooley, and I'd welcome feedback.

The climo_nco.sh script is in Susannah's repository here:

https://github.com/ACME-Climate/DiagnosticsWorkflow/blob/master/climo_nco.sh

It seems to meet all other Atmosphere Group climo requirements except
for speed. With optimal configuration (12 nodes) on rhea, it completes
five years (e.g., 1979-1983) of ne120 climate in 100-200 minutes. This
includes regridding. There is room for improvement, yet I'd like user
feedback first. 

If you type "climo_nco.sh", it tries to explain itself...

Best,
Charlie
************************************************************************
MSH15 myhre nature rvw:

http://mts-nclim.nature.com/cgi-bin/main.plex?el=A7T4BBV7A5KoL6J6A9ftd7ejdrKDIZzJaxKAVzOOVgZ

PLEASE NOTE: When reviewing the paper, we would be grateful if you
could pay particular attention to the statistics. All error bars
should be defined in the corresponding figure legends. Please include
in your report a specific comment on the appropriateness of the
statistical tests and the accuracy of the description of the error
bars and probability values. 

Dear All,

The revised manuscript by Myhre et al. improves the original and
and is a worthwhile publication for NCC. They addressed my major
critique and two of my three lesser points. The review of, and the
response to, Reviewer 2, both impress me. I'll describe the third
point and then address the statistics question posed by the editor. 

Their response to my third point confuses but does not worry me.
I tried to convey that the assumption that snow/ice RE remains
constant with shorter BC lifetime is conservative/generous because 
less BC per unit emission will deposit in the cryosphere.
They responded that forcing per emission could remain similar---which
I accept, because much of the forcing is in mid-latitudes, closer to
sources---and they substantiated that with Reference 32.
Reference 32 was published in 1997, before BC RE on snow/ice was
carefully studied, and its relevant simulations are those of "ghost
forcings" of constant magnitude to the surface. 
How are these R32 "Ghost forcings" germaine to my intended point that 
lower BC lifetime has to produce _less forcing relative to emissions_ 
on snow/ice (because less BC relative to emissions will
transport/deposit there due to shorter lifetime)?
In any case, as I originally stated, this underscores their main
point, and makes their conclusion more robust than they acknowledge. 

I found no obvious errors/omissions/inappropriate treatment of
statistics. The uncertainty bars are described in the main text,
which refers to the supplementary material, which offers a fuller
explication. This study builds-on and improves the assignation of
uncertainties based on improved understanding from measurements and
models since Bond et al., the IPCC AR5, and Aerocom2.
The new RE estimates come from mostly the same methods as the earlier
studies, informed by new data and refined process understanding.
The authors were primary contributors to those studies and understand
what one might call the uncertainties in the uncertainties.

Figure 2 shows, remarkably clearly, uncertainties and their combined
PDF. The presumption of independent uncertainties and their
combination using Monte Carlo techniques seems reasonable since other
approaches would make different, and likely greater, assumptions. 
Not many papers show the PDFs of uncertainties!
Assigning uncertainties is part art, part science, and both seem
reasonable to me. The authors are familiar with the strengths and
weaknesses of earlier studies and here they leverage the strengths
and revise the weaknesses. In short, their explanation and attribution
of new BC RE based on shorter lifetimes satisfies me.

************************************************************************
Hi Unidata,
The netCDF4 development branch breaks the NCO regression test for
modifying a _FillValue attribute in a netCDF4 file. My understanding
is that fixing NCF-187 meant that we are now free to modify/add
_FillValue attributes in netCDF4 files after the initial define mode.
So I turned off NCO's fancy patented workaround for NCF-187 whenever
the library version is 4.4.blah. And, without my old workaround,
the regression test fails. So either my understanding of what's
permissible is wrong or there is still a problem with NCF-187.

This command works on all NCO and netCDF versions with my workaround:
ncatted -h -O -a _FillValue,val_one_mss,m,f,0.0 ~/nco/data/in_grp.nc ~/foo.nc
And it fails when I turn-off the workaround when using netCDF 4.4.0.
Please let me know if my understanding of NCF-187 is wrong, and
double-check that your fix always works.

Best,
cz
************************************************************************
/bin/mv -f ~/bin/${PVM_ARCH}/* ~/bin
/bin/mv -f ~/obj/${PVM_ARCH}/* ~/obj
/bin/mv -f ~/lib/${PVM_ARCH}/* ~/lib
rmdir ~/bin/${PVM_ARCH}
rmdir ~/obj/${PVM_ARCH}
rmdir ~/lib/${PVM_ARCH}
scp givre.ess.uci.edu:dot/bashrc ~/dot
************************************************************************
climo_nco.sh Timings:
ne30  00h18m15s 20150620, 00h17m30s 20150621, 01h05m00s 20150624, 01h20m00s 20150624, 00h05m35s 20150625, 00h04m28s 20150626, 00h05m05s 20150626
ne120
13h20m00s 20150624, single-threaded login 4yrs
09h15m00s 20150625, single-threaded interactive 4yrs
02h02m00s 20150626, mpi interactive cf 4yrs
01h35m00s 20150627, mpi interactive no_cf 4yrs
01h47m00s 20150627, mpi interactive regridding no_cf 4yrs
01h36m00s 20150627, mpi batch regridding no_cf 4yrs
02h32m00s 20150628, mpi batch no_cf 5yrs
02h22m00s 20150628, mpi batch regridding no_cf 5yrs
02h15m00s 20150628, mpi batch regridding no_cf 5yrs
01h40m00s 20150628, mpi batch no_cf 5yrs
02h01m00s 20150628, mpi batch regridding no_cf 5yrs balanced
01h46m00s 20150629, mpi batch regridding no_cf 5yrs balanced
************************************************************************
Hi Peter and Susannah,

The climo_nco.sh script now processes climatologies more quickly
and flexibly and I would welcome your feedback. It cannot handle three 
different types of missing values ("missing_value", "mask", and
"_FillValue"). NCO only supports one ("_FillValue"), by design.
It seems to meet the other climo requirements except for speed.  

With optimal configuration (12 nodes) on rhea, it completes five years
(e.g., 1979-1983) of ne120 climate in 100-200 minutes. The base climo
can take as little as 100m, then the CF annotation for "climatology
bounds" (which can be turned-off) takes another ~30m (!) then
regridding, also optional, can take another 5-10m.
All of these have ranges, hence the overall range 100-200m.
There is room for improvement, yet I'd like user feedback first.

Peter, it's mostly compatible with the older climo_nco.sh, and thus
with climo_nco.py switches. I modified climo_test.sh to show how to
call climo_nco.sh directly. A number of minor changes improve
flexibility. The main differences are new capabilities to handle CF
attribute generation, regridding, and two types of parallelism 
(backgrounding on a single node and batch/MPI environments spanning
up to 12 nodes). 

If you type "climo_nco.sh", it tries to explain itself...

Best,
Charlie
************************************************************************
...compute ncra shell commands to execute then

mpirun -npernode 1 -n 1 ncra_command_1 &
mpirun -npernode 1 -n 1 ncra_command_2 &
mpirun -npernode 1 -n 1 ncra_command_3 &
mpirun -npernode 1 -n 1 ncra_command_4 &
mpirun -npernode 1 -n 1 ncra_command_5 &
mpirun -npernode 1 -n 1 ncra_command_6 &
mpirun -npernode 1 -n 1 ncra_command_7 &
mpirun -npernode 1 -n 1 ncra_command_8 &
mpirun -npernode 1 -n 1 ncra_command_9 &
mpirun -npernode 1 -n 1 ncra_command_10 &
mpirun -npernode 1 -n 1 ncra_command_11 &
mpirun -npernode 1 -n 1 ncra_command_12 &
wait

...collect results and continue...

************************************************************************
interactive batch jobs on rhea
https://www.olcf.ornl.gov/support/system-user-guides/rhea-user-guide/#61
https://www.olcf.ornl.gov/kb_articles/writing-batch-scripts-for-commodity-clusters/
https://www.olcf.ornl.gov/kb_articles/interactive-batch-jobs-on-commodity-clusters/
************************************************************************
Dear OLCF gurus,

How do I determine the names of the nodes allocated to me in a batch
session on rhea?

I have a twelve shell-command job that requires multi-node
parallelism, because the memory demands (~50 GB RAM per command) are
too high to accomplish on a single node without swapping to disk.
Conceptually the job reduces to running each command in parallel on
different nodes, wait()ing for the last command (node) to finish, then
tallying the results. The target machine here is rhea, and it's easy
to obtain 12 nodes in a batch session via qsub. The next step is to
send a different shell command to each of the twelve nodes and wait()
for them all to finish. How do I determine the names of the nodes?
And will ssh from the control node to the other nodes work?

Thanks!
Charlie

P.S. I looked for and did not find GNU Parallel and Swift
(http://swift-lang.org/main). Does OLCF use/recommend some other
package/method for parallelizing script-based processing?
************************************************************************
Dear Jeff & Purchasing,

I would like to purchase the equipment at

http://store.apple.com/us/cart/saved_cart/100000000603784159

(see attached PDF) for use by my lab in support of BC 2049.

This DOE grant promises delivery of software to analyze terascale
datasets that require tens of GB RAM and hundreds of GB of SSD
(spinning disk is too slow) just to process, and a parallel external
storage filesystem (i.e., RAID) for archival.  My group has no such
system, and we urgently need one in-house to mimic the DOE
supercomputer environments where our software is ultimately used.
Moreover, the DOE-required software for teleconferences is GoToMeeting
and GoToWebinar, and this software only works well on Mac and Windows
systems. My group cannot use Windows because it is unsuitable for our
high performance computing research.  We have already shown that Macs
can blend into our Linux-based group, and so we are confident that
this second Mac, integrated with its external storage, will meet the
needs of this grant.

Please let me know if you require any further information.

Best,
Charlie
************************************************************************
bm:
ne30 ncwa rhea
omp1	omp8
14m46s	14m06s

ne30 ncwa roulee
omp1	omp4
00m06s  00m04s

ne120 ncwa roulee
omp1	omp4
60m05s	14m57s

ne120 ncwa aerosol
omp1	
01m37s	

ne30 ncra roulee 12 months
omp1	omp4	 omp8
00m51s  00m37s   00m34s
omp4 failed with nco_put_vara() error

ne30 ncra rhea 12 months
omp1	omp4	 omp8
        10m34s		(login)

ne120 ncra aerosol
omp1 (41 GB compressed memory)

ne120 ncra rhea
omp1 (50 GB VIRT, 50 GB RES, takes 30m to climb to this) ~45m login
omp1 (50 GB VIRT, 50 GB RES, takes 4m to climb to this) ~fxm interactive

ne30 climo rhea (sustained RAM use ~3 GB VIRT/RES)
omp1 00h18m15s 20150620, 00h17m35s 20150621, 01h05m00s 20150624, 01h20m00s 20150624 (login)
omp1 00h04m46s 20150624 (interactive)

ne120 climo rhea (sustained RAM use ~42 GB VIRT/RES)
omp1 13h20m00s 20150624
************************************************************************
It might be helpful to explicitly state the desired name/convention so other programs (e.g., metadiags) know what to expect. Earlier we used ${caseid}_XXX_climo.nc. This new requirement suggests something like ${caseid}_XXX_yyyymm_yyyymm.nc, e.g., famipblah_ANN_197912_198311.nc, famipblah_MAM_198003_198305.nc, famipblah_01_198001_198301.nc. Are there other suggestions?

************************************************************************
climatology:
ncatted -a climatology,time,o,c,"climatology_bounds" ~/nco/data/in.nc ~/foo.nc
# Monthly mean datasets
time_srt=`ncks -C -H -s %g -v time_bnds -d tbnd,0 -d time,0 ${DATA}/essgcm14/essgcm14.cam2.h0.0001-01.nc`
time_end=`ncks -C -H -s %g -v time_bnds -d tbnd,0 -d time,0 ${DATA}/essgcm14/essgcm14.cam2.h0.0020-01.nc`
ncap2 -O -s "time@climatology=\"climatology_bnds\";climatology_bnds[\$time,\$tbnd]=0;climatology_bnds(0,0)=$time_srt;climatology_bnds(0,1)=$time_end;time@cell_methods=\"time: mean within years time: mean over years\"" ${DATA}/anl/essgcm14_01_climo.nc ~/foo.nc
ncatted -O -a bounds,time,d,,, ~/foo.nc
ncks -O -C -x -v time_bnds ~/foo.nc ~/foo.nc
ncks -m -v time --cdl ~/foo.nc

hdr_pad reserves space for these attributes:
time:cell_methods = "time: mean within years time: mean over years" ;
time:climatology = "climatology_bnds" ;
************************************************************************
Hi All,

I used Tempest to create (script below) ne120np4<->ne30np4
map-files for the purpose of applying them (with ncks) to CAM-SE output
(from ACME). For comparison, I did the same with ESMF_RegridWeightGen.
The Tempest mapfiles have the same n_a, n_b as ESMF, yet the Tempest
n_S is approximately 10x greater than ESMF. I don't know what the
extra information is, or how to apply it.

Tempest:
netcdf map_ne30np4_to_ne120np4_tps.20150618 {
  dimensions:
    dst_grid_rank = 1 ;
    n_a = 48602 ;
    n_b = 777602 ;
    n_s = 14045402 ;
    nv_a = 1 ;
    nv_b = 1 ;
    src_grid_rank = 1 ;

ESMF:
netcdf map_ne30np4_to_ne120np4_aave.20150603 {
  dimensions:
    dst_grid_rank = 1 ;
    n_a = 48602 ;
    n_b = 777602 ;
    n_s = 1217786 ;
    nv_a = 5 ;
    nv_b = 5 ;
    src_grid_rank = 1 ;

I must not understand some key dimension in the Tempest files.
Is there a subloop over the GLL nodes? Something like that must
be what allows Tempest to do better than first-order area
accuracy, etc. The sad, sad truth is that I don't know how
that's encoded/decoded (SCRIP has a num_wgts loop, for example).
Can someone please steer me towards landfall? Some kind of
equation in metacode for a destination gridpoint value would
be peachy.

Thanks!
c

GenerateCSMesh --alt --res 30 --file ${DATA}/rgr/msh_ne30.g
GenerateCSMesh --alt --res 120 --file ${DATA}/rgr/msh_ne120.g
GenerateOverlapMesh --a ${DATA}/rgr/msh_ne120.g --b ${DATA}/rgr/msh_ne30.g --out ${DATA}/rgr/msh_ovr_ne120_to_ne30.g
GenerateOfflineMap --in_mesh ${DATA}/rgr/msh_ne120.g --out_mesh ${DATA}/rgr/msh_ne30.g --ov_mesh ${DATA}/rgr/msh_ovr_ne120_to_ne30.g --in_type cgll --out_type cgll --in_np 4 --out_np 4 --out_map ${DATA}/maps/map_ne120np4_to_ne30np4_tps.20150613.nc

GenerateOverlapMesh --a ${DATA}/rgr/msh_ne30.g --b ${DATA}/rgr/msh_ne120.g --out ${DATA}/rgr/msh_ovr_ne30_to_ne120.g
GenerateOfflineMap --in_mesh ${DATA}/rgr/msh_ne30.g --out_mesh ${DATA}/rgr/msh_ne120.g --ov_mesh ${DATA}/rgr/msh_ovr_ne30_to_ne120.g --in_type cgll --out_type cgll --in_np 4 --out_np 4 --out_map ${DATA}/maps/map_ne30np4_to_ne120np4_tps.20150618.nc


******************* ****************************************************
Deat Tom,

This panel sounds very interesting. Thank you for the invitation.
I am traveling 7/27-8/18, and will likely be unable to participate
in any teleconferences during that time. If my scheduled downtime
is OK with your review plans, then I would be pleased to participate 
in the 10/21-10/23 panel. Please keep me posted.

Best,
Charlie
************************************************************************
Sounds like there is work to be done to "fill the gap". In the meantime, if it would be useful, I could still evaulate NCO's regridder on output consistent with Tempest's current CS maps. To do so, I would need a complete Tempest recipe to generate the mapfile, and a CS dataset (from any model, CAM or not) consistent with the source grid in the mapfile. A CS->RLL mapfile like Miranda's would be ideal. Otherwise please let me know when I can use Tempest to generate CS mapfiles that are consistent with the ACME CAM-SE data I already have.
************************************************************************
ncks -D 5 -O --map=${DATA}/maps/map_ne30np4_to_fv129x256_aave.150418.nc ${DATA}/ne30/rgr/ne30_1D.nc ${DATA}/ne30/rgr/rgr_1D_2D.nc > ~/foo 2>&1
ncks -D 5 -O --map=${DATA}/maps/map_fv129x256_to_ne30np4_aave.20150602.nc ${DATA}/ne30/rgr/ne30_2D.nc ${DATA}/ne30/rgr/rgr_2D_1D.nc > ~/foo 2>&1
ncks -D 5 -O --map=${DATA}/maps/map_fv129x256_to_fv257x512_aave.20150529.nc ${DATA}/ne30/rgr/ne30_2D.nc ${DATA}/ne30/rgr/rgr_2D_2D.nc > ~/foo 2>&1
ncks -D 5 -O --map=${DATA}/maps/map_ne30np4_to_ne120np4_aave.20150603.nc ${DATA}/ne30/rgr/ne30_1D.nc ${DATA}/ne30/rgr/rgr_1D_1D.nc > ~/foo 2>&1

************************************************************************
GNU clang:
export LINUX_CC='clang'
export LINUX_CXX='clang -std=c++11'

GNU g++:
export LINUX_CC='g++ -std=c++11'
export LINUX_CXX='g++ -std=c++11'

GNU gcc:
export LINUX_CC='gcc -std=c99 -pedantic -D_DEFAULT_SOURCE -D_BSD_SOURCE -D_POSIX_SOURCE'
export LINUX_CXX='g++ -std=c++11'
export LINUX_FC='gfortran'
module swap PE-intel PE-gnu
module add gsl hdf5
export LD_LIBRARY_PATH='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_gcc4.8.2--with-dap+hdf4/lib:/sw/redhat6/szip/2.1/rhel6.6_gnu4.8.2/lib':${LD_LIBRARY_PATH}
export NETCDF_ROOT='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_gcc4.8.2--with-dap+hdf4' 
export PATH='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_gcc4.8.2--with-dap+hdf4/bin':${PATH}
cd ~/nco/bld;make ANTLR_ROOT=${HOME} NETCDF_ROOT='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_gcc4.8.2--with-dap+hdf4' SZ=Y SZ_LIB='/sw/redhat6/szip/2.1/rhel6.6_gnu4.8.2/lib' UDUNITS_INC='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/include' UDUNITS_LIB='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/lib' OPTS=D OMP=Y allinone;cd -

Intel:
export LINUX_CC='icc -std=c99 -D_DEFAULT_SOURCE'
export LINUX_CXX='icpc'
export LINUX_FC='ifort'
module add intel gsl
export LD_LIBRARY_PATH='/ccs/compilers/intel/rh6-x86_64/14.0.4/composer_xe_2013_sp1.4.211/compiler/lib/intel64:/sw/redhat6/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4:/sw/redhat6/szip/2.1/rhel6.6_gnu4.8.2/lib':${LD_LIBRARY_PATH}
export NETCDF_ROOT='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4' 
export PATH='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4/bin':${PATH}
cd ~/nco/bld;make ANTLR_ROOT=${HOME} NETCDF_ROOT='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4' SZ=Y SZ_LIB='/sw/redhat6/szip/2.1/rhel6.6_gnu4.8.2/lib' UDUNITS_INC='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/include' UDUNITS_LIB='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/lib' OPTS=D OMP=Y allinone;cd -

cd ~/nco/bld;make ANTLR_ROOT=${HOME} LDFLAGS='-L/ccs/compilers/intel/rh6-x86_64/14.0.4/composer_xe_2013_sp1.4.211/compiler/lib/intel64 -lirc -limf' NETCDF_ROOT='/sw/redhat6/netcdf/4.3.3.1/rhel6.6_intel-14.0.4--with-dap+hdf4' SZ=Y SZ_LIB='/sw/redhat6/szip/2.1/rhel6.6_gnu4.8.2/lib' UDUNITS_INC='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/include' UDUNITS_LIB='/sw/redhat6/udunits/2.1.24/rhel6.4_intel13.1.3/lib' OPTS=D OMP=Y allinone;cd -w

-L /ccs/compilers/intel/rh6-x86_64/14.0.4/composer_xe_2013_sp1.4.211/compiler/lib/intel64 -lirc -limf

************************************************************************
Hola Paul et al, 

NCO version 4.5.0 (latest stable release) works well with all SCRIP
and ESMF-generated map-files that I have tested. Version 4.5.1-alpha1
(just now released) can apply (some) Tempest-generated map-files.
Regridding rectangular lat-lon (RLL) meshes RLL<->RLL seems to work.
I don't understand the parameters to generate correct cubed-sphere
(CS) meshes, so I can't yet test CS<->CS and CS<->RLL.

I have ne120np4 and ne30np4 (pentagons) data from ACME, and am trying
to regrid ne120np4 to ne30np4. Can Tempest do this? If so, please help
me refine this Tempest procedure, which completes but issues many
non-conservative warnings, and then has unexpected dimension sizes
on both source and destination grids:

GenerateCSMesh --res 30 --file ${DATA}/rgr/msh_ne30.g
GenerateCSMesh --res 120 --file ${DATA}/rgr/msh_ne120.g
GenerateOverlapMesh --a ${DATA}/rgr/msh_ne120.g --b ${DATA}/rgr/msh_ne30.g --out ${DATA}/rgr/msh_ovr_ne120_to_ne30.g
GenerateOfflineMap --in_mesh ${DATA}/rgr/msh_ne120.g --out_mesh ${DATA}/rgr/msh_ne30.g --ov_mesh ${DATA}/rgr/msh_ovr_ne120_to_ne30.g --in_type cgll --out_type cgll --in_np 4 --out_np 4 --out_map ${DATA}/maps/map_ne120np4_to_ne30np4.20150613.nc

If Tempest cannot do this then please suggest a different unstructured
grid test that I can perform with publicly available CAM-SE data.

This is also a request for any feedback on the NCO implementation,
where ncks plays the role of Tempest ApplyOfflineMap. Source + docs at
https://github.com/czender/nco/releases
http://nco.sf.net/nco.html#regrid
Especially interested in feature requests to better handle 0D, 1D, and 
grid-dependent variables, and in supporting awesome Tempest meshes. 

Thanks!
Charlie
************************************************************************
The parent page contains extensive discussions of the treatment of missing values. Two contenders emerged as the most appropriate algorithm. Charlie Zender (and NCO) refer to these as the "conservative" (aka simple) and the "renormalized" (aka extrapolative) algorithm, respectively. NCL and NCO implement the conservatie algorithm by default. NCO users can select the renormalized algorithm with --rnr=wgt_thr, where wgt_thr is the threshhold fraction of the destination gridcell that must have valid source data in order for the gridcell to be assigned a valid (not a missing) datum. When the threshold is met, the destination value is the mean of the valid inputs, otherwise the destination value is considered missing.

These algorithms generally produce similar answers in the global mean, yet results of individual gridcells can differ substantially. The difference factor is the reciprocal of the fraction of valid data. Global-mean AODVIS after conservative regridding and using first the conservative then the renormalized missing value algorithms is:
AODVIS = 0.151705 (conservative) AODVIS = 0.151776 (renormalized)

ncwa -O           -w area -v FSNT,AODVIS ${DATA}/ne30/rgr/famipc5_ne30_v0.3_00003.cam.h0.1979-01.nco.nc avg_area_nco.nc
ncks -H -C -u -v FSNT,AODVIS
ncwa -O --rnr=0.0 -w area -v FSNT,AODVIS ${DATA}/ne30/rgr/famipc5_ne30_v0.3_00003.cam.h0.1979-01.nco.nc avg_area_nco.nc
************************************************************************
Bali is a cultural and spiritual crossroads, a tectonic epicenter
of environmental change, and an open air laboratory where modern
practices, traditional culture, and natural sustainability intersect. 
Join UCI Earth System Science professor Charlie Zender in Bali
for five weeks in Summer 2016 to understand the processes that created
this tropical paradise, and the pressures pushing it in new
directions. The first course offered covers the origin and evolution
of the Earth, its atmosphere, and oceans. The second focuses on
natural catastrophes such as earthquekes, volcanic eruptions, and
floods. A full itinerary of field trips (an active volcano, tropical
reef, and coastal ecosystem) will bring course material to life, while
Balinese speakers share their culture and environmental perspectives.
You will return from this transformational educational experience will
a deepened understanding of the environmental challenges facing
society in the US and globally.

************************************************************************
Dear All,

All first-year graduate students in ESS 298 (Practicum) turned-in their
written proposal this morning, and now I need your input on grades.
The course grade has three components: written (50%), oral
presentation (this Friday, 30%), and class preparednes/participation
(20%). The advisor (i.e., you) and I both grade the written proposal
and the student receives the mean grade.

Students were given wide latitude to re-craft or craft a practical
proposal, e.g., one whose core would be suitable for a NASA ESSF or
NSF GRFP. To meet the required length (10 pages) many students
extended their existing ESSF or GRFP to become more of a thesis
proposal. Regardless of scope, though, all the proposals should have
an identifiable and strongly motivated research question. 

Please send me your grade as a traditional scale percentage, out of
100, by 9 AM, Monday, 6/15. You are free to grade on scientific
content, merit, and style whereas I will grade mostly on the proposal
effectiveness as read by a non-expert. 

Thanks again for working on these proposals last quarter,
Charlie
************************************************************************
Hi Laura,
This Thursday night 6/11 the girls and Shari and I are taking a
sunset duffy ride in Newport Harbor. We hope you and Marina can come.
Another family with 12yo boy and 15yo girl are coming too, and so
is your neighbor Rachel. The cruise is from 6:15-8:15 in Newport Harbor.
Yes, it's a school night :) We'll leave here about 5:30.
Food will be simple. I'm planning to bring some platters (fruit,
veggie, cheese and crackers) from Albertson's and a homemade dessert.
c
************************************************************************
> I like the thematic motto. It would be really helpful if you could
> provide a paragraph that expands on this and describes the overall
> experience (even if it is vague at this point). Just a bit of
> excitement that would entice students. Hope you don't mind doing
> that. We can find appropriate images and design the flier. 

Bali 2016: Learn how we change Earth, and how Earth fights back

Bali is a cultural and spiritual crossroads, a tectonic epicenter
of environmental change, and an open air laboratory where modern
practices, traditional arts, and natural sustainability butt heads. 
Join UCI Earth System Science professor Charlie Zender in Bali
for five weeks in Summer 2016 to understand the processes that created
this tropical paradise, and the pressures pushing it in new
directions. The first course offered covers the origin and evolution
of the Earth, its atmosphere, and oceans. The second focuses on
natural catastrophes such as earthquekes, volcanic eruptions, and
floods. A full itinerary of field trips (an active volcano, tropical
reef, and coastal ecosystem) will bring course material to life, while
Balinese speakers share their culture and environmental perspectives.
You will return from this transformational educational experience will
a deepened understanding of the environmental challenges facing
society in the US and globally.

************************************************************************
ESMF is a Fortran library with a new C-interface, so NCO once again
(optionally) links to Fortran. 
It was sixteen years ago that NCO (version 1.1.18) changed from
default Fortran arithmetic to C. 

The input grid dataset provider, meaning the science team that
assembled the observational climatology against which ACME may be
compared, has the prerogative to decide that retrieved data are OK
to be interpreted as uniform over the gridcells they occupy.

In my opinion it is dangerous for people who regrid that data to
presume that the original data can be interpreted as extending beyond
the original gridcell. This is what the renormalize-by-valid-weights
method does. It is the third reason I implemented the simpler
sum-the-weights-and-move-on as the default missing value algorithm.
(the first two reasons are it is simpler and it is conservative).

I'll implement the renormalize method as a (non-default) runtime
switch so we can easily test the difference it makes on our favorite
datasets. Then we can make a more informed decision on the default
behavior, and whether that should differ by field (in which case
we could use a key-value switch or an attribute as Peter suggests).
************************************************************************

Let me clarify about "dogfood". I mis-stated this capability:

"ne30->fv129x256->T42->ne120->ne30 gives you the original input
(modulo rounding differences which are small for conservative
mapping)." 

Regridding to other grids and then back will not yield the same
answers (of course!). It produces answers that look much like the
coarsest intermediate grid. Only identity regridding (or regridding
to an integral subset of the original and back) will produce the
same original data (modulo roundoff).

What I mean by dogfood is that a file can be successively regridded
and the appropriate data and metadata (like gw and lat_vertices) will
be appended/replaced/deleted/recreated. Inappropriate data and
metadata for the current grid does not accumulate and break the
regridding.

c

Hey Dudes,

FYI, NCO regridder (4.5.0-beta7) supports 1D->1D, 1D->2D, 2D->1D, and
2D->2D regridding for unstructured 1D and rectangular 2D grids.
Tested on T42, fv129x256, fv257x512, ne30np4, and ne120np4 grids. 
It seems to meet all current ACME requirements. And the metadata
annotation looks much better. 

I'll put new maps you can play with in my maps directory on rhea:
/lustre/atlas/proj-shared/cli115/zender/maps

One cool feature is that NCO eats its own regridded dogfood, e.g.,
ne30->fv129x256->T42->ne120->ne30 gives you the original input (modulo
rounding differences which are small for conservative mapping).
AND the output files contain better metadata than the input, so that
remapping a file to its original grid (identity mapping) does not
change answers at all yet improves the metadata. For instance, one
can hyperslab based on the lat/lon on the unstructured grid because 
the metadata turns lat and lon into auxiliary coordinate variables.

I give an example below. If you have a chance to play with this
and give me feedback, I'll make adjustments accordingly before
releasing this as NCO 4.5.0.

Best,
Charlie

% ncks --cdl -d ncol,0 -v FSNT famipc5_ne30_v0.3_00003.cam.h0.1979-01.nc
netcdf famipc5_ne30_v0.3_00003.cam.h0.1979-01 {
  dimensions:
    nbnd = 2 ;
    ncol = 1 ;
    time = UNLIMITED ; // (1 currently)

  variables:
    float FSNT(time,ncol) ;
      FSNT:Sampling_Sequence = "rad_lwsw" ;
      FSNT:units = "W/m2" ;
      FSNT:long_name = "Net solar flux at top of model" ;
      FSNT:cell_methods = "time: mean" ;

    double time(time) ;
      time:long_name = "time" ;
      time:units = "days since 1979-01-01 00:00:00" ;
      time:calendar = "noleap" ;
      time:bounds = "time_bnds" ;

    double time_bnds(time,nbnd) ;
      time_bnds:long_name = "time interval endpoints" ;

  data:
    FSNT = 358.3005 ;

    time = 31 ;

    time_bnds = 0, 31 ;

} // group /

Same model output after identity regridding:
% ncks -D 5 -L 1 -4 -t 8 -O --map=${DATA}/maps/map_ne30np4_to_ne30np4_aave.20150603.nc ${DATA}/ne30/rgr/famipc5_ne30_v0.3_00003.cam.h0.1979-01.nc ${DATA}/ne30/rgr/dogfood.nc 
% ncks --cdl -d ncol,0 -v FSNT dogfood.nc
netcdf dogfood {
  dimensions:
    nbnd = 2 ;
    ncol = 1 ;
    nv = 5 ;
    time = 1 ;

  variables:
    float FSNT(time,ncol) ;
      FSNT:Sampling_Sequence = "rad_lwsw" ;
      FSNT:units = "W/m2" ;
      FSNT:long_name = "Net solar flux at top of model" ;
      FSNT:cell_methods = "time: mean" ;
      FSNT:coordinates = "lat lon" ;

    double lat(ncol) ;
      lat:long_name = "latitude" ;
      lat:standard_name = "latitude" ;
      lat:units = "degrees_north" ;
      lat:axis = "Y" ;
      lat:bounds = "lat_vertices" ;
      lat:coordinates = "lat lon" ;

    double lat_vertices(ncol,nv) ;
      lat_vertices:long_name = "gridcell latitude vertices" ;

    double lon(ncol) ;
      lon:long_name = "longitude" ;
      lon:standard_name = "longitude" ;
      lon:units = "degrees_east" ;
      lon:axis = "X" ;
      lon:bounds = "lon_vertices" ;
      lon:coordinates = "lat lon" ;

    double lon_vertices(ncol,nv) ;
      lon_vertices:long_name = "gridcell longitude vertices" ;

    double time(time) ;
      time:long_name = "time" ;
      time:units = "days since 1979-01-01 00:00:00" ;
      time:calendar = "noleap" ;
      time:bounds = "time_bnds" ;

    double time_bnds(time,nbnd) ;
      time_bnds:long_name = "time interval endpoints" ;

  data:
    FSNT = 358.3005 ;

    lat = -35.2643896827547 ;

    lat_vertices = -35.5977213707941, -35.5977213707941, -35.0972113816906, -35.097211381693, -35.097211381693 ;

    lon = 315 ;

    lon_vertices = 315, 315, 315.352825437493, 314.647174562502, 314.647174562502 ;

    time = 31 ;

    time_bnds = 0, 31 ;

} // group /

Rectangular coordinate hyperslabs work on the native, unstructured grid:

% ncks -H -C -X 314.6,315.3,-35.6,-35.1 -v FSNT dogfood.nc
time[0]=31 ncol[0] FSNT[0]=358.301 
time[0]=31 ncol[4] FSNT[4]=356.739 

************************************************************************
Thank you for your thoughts on this David, Karl, and Jim.
David, I concur with you.
Karl, rest assured we are talking about adding cell_methods to _both_
the variable and its coordinate, not the coordinate instead of the
variable.  
Jim, you seem to view coordinates as abstract mathematical axes while
I prefer their geophysical meaning. With respect to this discussion
I see time similarly to other coordinates like lat, lon, lev.
For example, when NCO averages a spatiotemporal region I want users to
know that the scalar (or size 1 array) values of time, lat, and lon
values in the output represent averages over the original input region
and cell_methods seems to be the natural way to do this.
Since CF seems to allow cell_methods on coordinates, NCO will continue
its current behavior. I hope the NERC CF checker starts to allow
this behavior (the Decker/Schultz cfchecker always has). 

Best,
Charlie
************************************************************************
Hi M&M,

These course descriptions are straight from UCI's course catalog:

EARTHSS 1. Introduction to Earth System Science. 4 Units.
Covers the origin and evolution of the Earth, its atmosphere, and
oceans, from the perspective of biogeochemical cycles, energy use, and 
human impacts on the Earth system. (GEs II, Va)

EARTHSS 17. Hurricanes, Tsunamis, and other Catastrophes. 4 Units.
Introduction to the basic science and state of predictability of
various natural catastrophic events including earthquakes, volcanic
eruptions, tsunamis, landslides, floods, hurricanes, fires, and
asteroid impacts and their interactions and implications with human
society in the U.S. and globally. (GEs II and Va or VIII)

As discussed at our meeting, I will replace some of the standard
material by teachings more relevant to the locations in Bali.
I'll increase emphasis on the tropics, islands, and sea level rise.
We'll invite some local experts, and maybe non-experts like fishermen, 
to describe Balinese perspectives on environmental change.

A thematic motto could be...
Bali 2016: Learn how we change Earth, and how Earth fights back

During our discussions I developed the impression that Molly and her
staff would take the lead in generating student interest for Bali,
that this would occur before, during, and after this summer, and that
I would participate in outreach events as appropriate. 
For example, to the council of Associate Deans, and to incoming
Freshman audiences. More recently it seems I have been assigned the
responsibility of generating interest and "a solid interest list"
either explicitly ("it all rests in Charlie's hands at the moment")
or implicitly, because Summer Session staff have not contacted me
since Molly's email of 4/30 which said "I will get back to you on
specifics of marketing soon."

We can work together better than this. Maybe the written course
descriptions and my intended alterations of them will provide 
impetus to summer session folks. I will follow-through with the
cousellors once I have the flyers they requested to distribute.
I'm tailoring ESS 1 and ESS 17, from gathering material from the
recent volcanic eruption in Chile, to contacting colleagues aournd the 
Pacific for ideas. I remain at the disposal of anyone helping to
bring this project to fruition.

Best,
Charlie
************************************************************************
ESS 298 particpation grades:
Hongchen-- absent
Qian-- absent
************************************************************************
Aloha CFers,

Is it correct to add a cell_methods = "time: mean" attribute to the
time coordinate when the coordinate is averaged over time?
NCO's ncra does this. However, it breaks the NERC CF checker.
Its clear from the CF docs that ncra time-averaging a variable like 
wind(time) from an array to a degenerate (size 1) array should
add a cell_methods = "time: mean" attribute to wind.
Yet should it add cell_methods to the time coordinate itself?

My take is that it should, so that the time_bounds variable, if any,
shows the original extent of the temporal range, and the time
coordinate value contains the mean of the original range.
Yet an NCO user is making a good case that cell_methods are only
for non-coordinate variables. His point is that many variables with 
different cell_methods can all contain the same coordinate, so that
the coordinate should not have cell_methods. My response is that 
cell_methods refers to the bounds variable of the coordinate, not
the coordinate values themselves.

Is the question clear? If not, I can supply CDL...

Mahalo,
Charlie
************************************************************************
Hi Phil,

I had not previously consiered your point about (not) adding
cell_methods to coordinate variables. It would be straightforward
for me to change ncra & ncwa behavior, and if you're right I will. 
However, let me first ask the CF group what the behavior should be. 

The problem you identify does break the NERC checker, but it may be 
a checker issue. I hope cell_methods can decorate coordinates,
otherwise that lat, lon, time variables output from a regional
average will be non-representative of the average.
My opinion is that coordinate values should be averaged during
dimension reduction, and bounds values should be "hollowed out".
Then the original spatio-temporal bounds are preserved, and the
new coordinates represent the midpoints of the old region.

BTW, you can use something like this to add bounds:

ncap2 -O -s 'TIME@bounds="time_bnds";defdim("bnds",2);time_bnds[$TIME,$bnds]=0;time_bnds(:,0)=TIME-0.5;time_bnds(:,1)=TIME+0.5;' in.nc out.nc

Two years ago I installed a CF compliance checker by Martin Schultz on
my local machine, and wrote 'ncdismember' to call it:
http://nco.sf.net/nco.html#ncdismember
This checker does not complain about TIME/cell_methods.
The NERC checker is/was hard too hard to install locally.
Many back and forths, and they could not fix my problems.
ncdismember's main features are being local/fast and working
well with netCDF4 files, including groups.

I like your script! May I steal the web upload part?  I'd use it to 
make ncdismember call NERC when it cannot find a local checker. 

Charlie
************************************************************************
This sentence is just guesswork so I removed it. I presume but don't know that UV-CDAT diagnostics are correct (because many eyes have looked at them, right?), and I don't know what the processing chain is. I do know that ACME area is currently incorrect and should not be used. Resolve if OK.

Regridders all agree and are correct on all fields (except area) not containing missing values

************************************************************************
Current behavior:
concentration(time,lev,ncol) cell_methods = time: mean
concentration(time,lev,ncol) units: stuff meter-3
flux(time,lev,ncol) cell_methods = time: mean
flux(time,lev,ncol) units = stuff meter-2 second-1
moist_mass_mixing_ratio(time,lev,ncol) cell_methods = time: mean
moist_mass_mixing_ratio(time,lev,ncol) units: kilogram kilogram-1
temperature(time,lev,ncol) cell_methods = time: mean
temperature(time,lev,ncol) units = kelvin

Proposed behavior:
concentration(time,lev,ncol) cell_methods = time, lev, ncol: mean
concentration(time,lev,ncol) cell_measures = measure: area measure: volume
concentration(time,lev,ncol) units: stuff meter-3
flux(time,lev,ncol) cell_methods = time, lev, ncol: mean
flux(time,lev,ncol) cell_measures = measure: area
flux(time,lev,ncol) units = stuff meter-2 second-1
moist_mass_mixing_ratio(time,lev,ncol) cell_methods = time, lev, ncol: mean
moist_mass_mixing_ratio(time,lev,ncol) cell_measures = measure: mass
moist_mass_mixing_ratio(time,lev,ncol) units: kilogram kilogram-1
temperature(time,lev,ncol) cell_methods = time, lev, ncol: mean
temperature(time,lev,ncol) cell_measures = measure: area measure: mass measure: density
temperature(time,lev,ncol) units = kelvin

************************************************************************
Accuracy, performance, and issues for ne30 now documented. regrid_test.sh updated to use Peter's suggested files. No one else has told me they have run it, though it should be close to working for other users. Rhea has very uneven timings. Timings on todays runs are 50-150% worse than yesterdays, so I've changed the table to show ranges. Feedback welcome. Will proceed with ne120 next.

************************************************************************
There are two anomalies in the "fv" grids currently on this page. The initial and final center latitudes (grid_center_lat or yc[0] and yc[nlat-1]) are -90 and +90, respectively. These cells have non-zero area and interior edges off the poles yet their center latitude is defined as also being their outer interface latitude. Why aren't the endpoint center latitudes defined at the gridcell centroid, i.e., the average of the interface latitudes (which are correctly defined)? A similar issue occurs with the longitudes, which do not seem to fully cover the sphere. Would sleep better at night if someone revealed a comforting rationale for this strangeness.

I talked to @Phil Rasch about this long ago, and there may be a reason why a dynamical core groups all the pole points together but it does not make sense to me that the grid representation of cells with non-zero area has midpoint latitudes that fall on the edges of the cells. As I understand it, ESMF_regridder propagates this anomaly (which starts in the grid_center_lat and grid_center_lon source grid variables) into the remap files in the yc_a/b variables. Using the current polar yc_a/b values for anything other than axis labels may lead to wrong answers. Would prefer self-consistent xc/yc variables. Options include:
A. Do nothing
B. Regenerate SCRIP grid files and see if ESMF_regridder propagates correct values to weight files
C. Fix problem in the downstream regridder and thus produce regridded files that contain define correct lat/lon variables.
D. ?
************************************************************************
Hi,

I'm having trouble cloning this git repository on rhea:

zender@rhea-login1g:~$ git clone https://github.com/ACME-Climate/DiagnosticsWorkflow.git 
Initialized empty Git repository in /autofs/nccs-svm1_home1/zender/DiagnosticsWorkflow/.git/
error: The requested URL returned error: 403 Forbidden while accessing https://github.com/ACME-Climate/DiagnosticsWorkflow.git/info/refs
fatal: HTTP request failed

This same cloning command works fine on my home machine, not on rhea.

Any help appreciated. Thanks!

Charlie
************************************************************************
Hi Peter,

FYI, the NCO regridder is now available:

ncks -O --map=map.nc in.nc out.nc
ncks -O --map=${MY_PROJWORK}/rgr/map_ne120np4_to_181x360_aave.nc ${MY_PROJWORK}/ne120/raw/b1850c5.cam.h0.0001-01.nc ${MY_PROJWORK}/ne120/rgr/b1850c5.cam.h0.0001-01.nco.nc

It seems to satisfy all ACME requirements, and more.
And it seems pretty fast relative to NCL and UVCDAT: factors of 3-10
faster for smaller (ne30) files, and 1.0 to 1.2 for larger (ne120).
Of course I'm curious whether others find the same.
It will get better as I hook it up to more standard NCO features (like
chunking), yet the performance already pleases me.

I may take a crack at a climatology script soon.

Best,
cz
************************************************************************
Mom & Ed's stuff:
Ed's banjo
Hat rack
Elliptical?
************************************************************************
Dear DIWG,

I have (finally) created a GitHub repository for DIWG work:

http://github.com/diwg

Obtain the diwg repository with:

git clone http://github.com/diwg/diwg.git

Anyone may check-out this repository. In order to modify please
send me your (free) GitHub account name and I will add you as
an owner of the DIWG organization, and then you may commit changes. 

So far diwg repository contains two draft files global_rectangular.cdl
and swath_rectangular.cdl. Their contents contain usage instructions.
They are basically copies of last year's summary by Peter and MaryJo,
modified for CF-compliance and to work with ncgen for completeness.
To help our effort at demonstrating best practices for gridded
datasets, please modify the global_rectangular.cdl file.
Test that ncgen likes your moifications, then commit so we can see.

Issues, pull requests etc. can be handled via GitHub rather than
the DIWG group email. I encourage coders to dress-up the repository
from its current naked state, add new files, make it useful for you. 

Peter will soon send a Doodle poll for a telecon in June.
Please try to familiarize yourself with these templates by then.

Thanks!
Charlie
************************************************************************
This release includes small feature improvements in chunking, deflation, and attribute handling during dimension reduction. Also improvements in clarity and consistency of messages. While chunking is an esoteric feature for many, its importance is increasing as the move to netCDF4 (classic) continues. The chunking defaults and flexibility have reached a new level of robustness in this release. Benchmarks suggest this chunking can accelerate wallclock performance of netCDF4 workflows by 50% relative to 4.4.8.

The main new awesome feature is that ncra understands weights. Finally. Applying uneven weights to different input files was too clunky. This release is dedicated to the intrepid souls who use NCO to generate climatologies that require uneven weights. This release should make that task much easier.
************************************************************************
Hi Peter,

> Do you have a timeline for releasing the new -w option for ncra?

Yes, I just released it, NCO 4.4.9, an hour ago.

> If there's a new release soon I'll change my simple script to use -w, declare
> my script fully operational, and say "I'm going to use this until Workflow
> creates something better".
> 
> And please do let me know when you have a working regrid capability. I'm
> trying to get Kate Evans to write a testing suite for regrid codes and we
> could easily run your code through the tests as part of my "Workflow must
> be competitive" initiative. Additionally, since Dean and Dave refuse to
> build UV-CDAT anywhere except ORNL having a regridder which can be easily
> installed where we actually do work would be most helpful.

Will do. That's next on my agenda.

> I'm enthusiastic about Dean's response. I think he's basically given up on
> resisting me... Any help you're willing to give would be appreciated. I"m
> still unsure whether an NCO option is politically or computationally
> optimal. I've been priming Charles to write a climo code in python/C with
> multi-node parallelism and parallel IO. I'm not convinced we need that
> much parallelism to compute 1/4 degree climos in a reasonable amount of
> time, but perhaps building it in is useful for future applications. What
> are your thoughts?

It remains to be seen what the best approach is for climo files.
I am interested seeing the results from different approaches including
Python and parallelism. This is because my natural bullheadedness
compels me to try to make NCO competitive with other methods.
I have too little experience with ne120-size data climatologies to be
convinced of anything at this point. Hope to learn enough in the next
month to understand what's necessary, and what's sufficient, to do a
climo in half and hour.

> I have another (related) question for you though - Kate and Mark suggested
> to me yesterday that if we switch our files from 'all-variables-one-time'
> to 'all-times-one-variable' we could do all our climo files and regridding
> in parallel, which would be much faster. I think you were trying to tell
> me this at the all-hands as well. If I recall correctly, your argument was
> that opening a file, adding all its contents to an accumulation buffer,
> then closing it and moving on to the next file required more RAM than was
> available, requiring use of (slow) swap. You thought that modern computers
> could open all files at once and loop over each variable more quickly. My
> understanding is that Rhea has huge latency in its ability to open files
> (perhaps because of the Lustre filesystem?) so closing/reopening files for
> each variable would be a killer (but probably avoidable?). Switching to
> all-times-one-variable would also be advantageous from the perspective of
> publishing/downloading the data (since our files would be identical to
> those used in CMIP, which ESGF was built for). One downside is that if we
> wanted to compute climos for 5 yrs, run another 5 yrs, then create 10 yr
> climos we would need to append the last 5 yrs for each variable's file...
> Do you have any thoughts on whether this would be better/worse? Any
> predictions as to whether switch to var-per-file -> compute climos ->
> regrid would be faster than create climos -> regrid using
> traditionally-organized history files?

My limited testing confirms that data access patterns are crucial.
I suspect that one variable-per-file would be faster.
There reasons for keeping all years in one file are debatable. 
So keep the raw files, e.g., five years long each.
But that's just a guess until someone tests it.

Best,
c
************************************************************************
Hi Peter,

> IÂ¹ve noticed you seem to have a good sense for that invisible line between
> being interested and helpful and stepping on the UV-CDAT peopleÂ¹s toesÅ  I
> imagine your back-channel info is helping you there.

There is no "Deep Throat" just a trickle of mutual commisseration
between me and other scientists who "feel my pain" :)
My pain-avoidance instincts are strong, hence my reticence to rock the
boat too much.

Regarding that, I just asked Dean how/if I could help with the
climatology generation mess. He says:

"The generating of climo files seems to be a free for all. I counted five,
possibly six, separate efforts. This is huge miss management of resources.
I have asked Peter to assemble all the players (including you) who are
interested in the generation of climatology files and the method by which
to produce them. 

	If you could be involved with this team effort, that would be great. You
will be on the ground floor on how best to achieve our goals now and for
the future. I am also hoping that you will be involved with vetting
process and you and the team keep me informed on the progress.

	For now, I would like for you to continue with your primary tasks of
allowing UV-CDAT to call NCO through Python."

My interpretation is that I'm free to help with climatologies but it
should not be my primary effort. If the "team" decides they want some
less kludgy (no offense) NCO-based  scripts, I can certainly help.

> I would love to use your weighted monthly averager when you get it
> working.

It works if you want to try though you'll have to build NCO from
source or use my rhea executables (~zender/LINUXAMD64/bin) until I
make a new release. Command line weights work now with, e.g.,

ncra -w 31,31,28 dec.nc jan.nc feb.nc out.nc

i.e., one weight per _file_. 

> My basic short-term strategy is to say Â³I built this code with
> NCO in an hour. It does the things I need and UV-CDAT doesnÂ¹t, so IÂ¹ll use
> it until UV-CDAT does.Â² That way when people complain that IÂ¹m not
> supposed to be writing code that competes with UV-CDAT I can say, Â³I
> didnÂ¹t really, I wrote this up in an hour on a Saturday morningÂ².  The
> Achilles heel of my current strategy is that I canÂ¹t weight months
> correctly. I could easily fix this following the AMWG strategy, but not in
> an hour.

Sounds good.

> IÂ¹m also interested in your regridder because the UV-CDAT version - while
> good - only works where UV-CDAT is installed and Dave insists that we
> don't support UV-CDAT anywhere except rhea.ornl.gov. Mark Taylor (who I
> agree with) is pretty set on creating the grid weights using ESMF or
> Tempest regridder, then having a tool (in UV-CDAT or NCO or NCL) which
> just does the matrix multiplication to convert to the new grid and ensures
> the metadata gets transferred correctly. Is this how your regridder will
> work (or could it work that way)?

Short version is Yes. Mark and I agreed that this would be my highest
priority. NCO regridding is the main deliverable from the work plan
that the council approved when they brought me on. Unfortunately,
Dave and Dean seem dead-set against abiding by that plan.
Nevertheless, I hope to have the basic version that you describe
working in a few weeks. 

c
************************************************************************
Hi Peter,

Looks good. Seems clear. Suggest two changes:

1. Omit Structural requirement 9a. It's redundant if the grid is in
the h0 file. Connectivity information that is not in the h0 file
raises many questions which can perhaps be left out

************************************************************************
Hi Dean,

How can I help with the climatology generation?

I'm familiar with many issues that climatology generation presents.
If you let me provide an interim option based on NCO, it would be less
clunky and, I think, significantly faster than AMWG's old NCO methods
and Peter's attempt, though that remains to be seen.
Charles' and Jeff's UV-CDAT efforts may ultimately outrun NCO and
that's fine. Yet my understanding of UV-CDAT is too poor to help code
their effort, and I do not wish to step on anyone's toes.

While an interim NCO method may not match the final UV-CDAT version,
it could provide a fallback option and lessen pressure on Workflow.
You would prefer a UV-CDAT solution, and I respect that and am
working to allow UV-CDAT to call NCO through Python, but if you wish I
could temporarily shift my ACME efforts to climatology generation.
Or not. Let me know. Would be happy to discuss by phone if you prefer.

Best,
Charlie
************************************************************************
Hi Val/OLCF,

I'm even more interested than before in updating NCO on rhea/titan.
I have built and tested version 4.4.9-beta on rhea.
You can use those if you wish.
However, they were built "by hand" (not by autoconf/configure)
so it would be hard for sysadmins to duplicate this build.
The binaries are ready if OLCF wants them.

To simplify future updates, I'm changing the configure scripts
to play well with the OLCF modules environment.
And I'm running into some problems and could use some help.
If I can get this fixed then I will add these changes to
NCO 4.4.9 final before it is released.

Here is my initial stab at building from source from ~/nco:
module add intel gsl netcdf
export LDFLAGS="${NETCDF_CLIB} ${GSL_LIB} ${SZIP_POST_LINK_OPTS}"
export CPPFLAGS="-I${NETCDF_DIR}/include ${GSL_INCLUDE_OPTS} ${SZIP_INCLUDE_OPTS}"
CC='icc' CXX='icpc' ./configure --disable-shared --prefix=${HOME} --bindir=${MY_BIN_DIR} --datadir=${HOME}/nco/data --libdir=${MY_LIB_DIR} --mandir=${HOME}/nco/man

The build fails because the linking flags are incomplete:

configure:4217: icc  -I/sw/rhea/netcdf/4.1.3/rhel6.4_intel13.1.3/include -I/sw/rhea/gsl/1.16/rhel6.6_gnu4.4.7/include -I/sw/rhea/szip/2.1/rhel6.6_intel14.0.4/include -I/sw/rhea/netcdf/4.1.3/rhel6.4_intel13.1.3/include -L/sw/rhea/netcdf/4.1.3/rhel6.4_intel13.1.3/lib -lnetcdf -L$HDF5_DIR/lib -lhdf5_hl -lhdf5 -L$SZIP_DIR/lib -lsz -lz -lm -lcurl -I/sw/rhea/gsl/1.16/rhel6.6_gnu4.4.7/include -L/sw/rhea/gsl/1.16/rhel6.6_gnu4.4.7/lib -lgsl -lgslcblas -L/sw/rhea/szip/2.1/rhel6.6_intel14.0.4/lib -lsz conftest.c  >&5
icc: warning #10315: specifying -lm before files may supercede the Intel(R) math library and affect performance
/sw/rhea/hdf5/1.8.11/rhel6.6_intel14.0.4/lib/libhdf5.so: undefined reference to `_intel_fast_memmove'

I'd like some help to fix this.
For some reason ${NETCDF_CLIB} is inadequate to link to libsz.a.
I must separately add ${SZIP_POST_LINK_OPTS} for LDFLAGS to find
libsz.a.

Thanks!
c
************************************************************************
https://acme-climate.atlassian.net/wiki/display/WORKFLOW/Y1Q4+Workflow+Tasks+Plan
https://acme-climate.atlassian.net/wiki/display/WORKFLOW/Y2Q1+Workflow+Tasks+Plan
************************************************************************
Dear Jeff, Charles, and Susannah,

At the ACME PI meeting Dean Williams and I agreed on the draft plan
below to integrate NCO into UVCDAT. This is a new Workflow task. 
This work would allow users to access NCO capabilities using a Python
API (aka PyNCO, basically NCO bindings in Python) from within UVCDAT.
I'm soliciting your input on the plan and especially the timeline:

************************************************************************
New Confluence Task:
4.2.12 UVCDAT exposes NCO via Python bindings

Jira Tasks to accomplish this 
1. Update NCO on rhea (PNNL too?) (CZ) Q4
2. Update/revise PyNCO bindings/API for UVCDAT (CZ) Q5
3. CDAT feedback on PyNCO bindings (w. Jeff P. and/or Charles D.?) Q5
4. Merge PyNCO bindings into UVCDAT (JP and/or CD) Q6
5. Convert obs. climo. file procedure (e.g., RACMO2_SMB) from NCO to UVCDAT-PyNCO
   (w. Susannah Burrows under T3.3.6) Q6

Deliverable:
Example UVCDAT-PyNCO script producing observational climatology
dataset used in diagnostics Q6 
************************************************************************

The plan mentions you all. Charles and Jeff for any feedback
suggestions on the bindings API, and for merging them into UVCDAT.
Susannah for collaborating on turning an example script (I chose
RACMO2_SMB) from NCO into a PyNCO script. The tentative dates are
listed as quarters where Q1 = 20140701-20140930 etc. So Q6 means
finish by 20151231.

Any comments, questions, or suggestions?
Are you willing and able to help as suggested in the draft plan?
Does the timeline suit you?

Thanks in advance!
Charlie
************************************************************************
Dear Kevin,
I have not received anything from David Qanrud.
I am not in a position to write a lengthy contribution for a
proceedings that none of my US dust colleagues will ever read.
Best,
Charlie
************************************************************************
Hi Ted,
Thank you, very much, for the invite. I want to attend an ESIP someday. 
However, I can't travel any more this July because I'm already
traveling for three weeks starting in late July. 
Best,
Charlie
************************************************************************
Hi Dean,

Here is a draft task outline for incorporating NCO in UVCDAT.
Please feel free to implement in Confluence and Jira or give feedback.
Also add me to existing Jira task wg102 if I'm not there already.

New Confluence Task:
4.2.12 UVCDAT exposes NCO via Python bindings

Jira Tasks to accomplish this 
1. Update NCO on rhea (PNNL too?) (CZ) Q4
2. Update/revise PyNCO bindings/API for UVCDAT (CZ) Q5
3. CDAT feedback on PyNCO bindings (w. Jeff P. and/or Charles D.?) Q5
4. Merge PyNCO bindings into UVCDAT (JP and/or CD) Q6
5. Convert obs. climo. file procedure (e.g., RACMO2_SMB) from NCO to UVCDAT-PyNCO
   (w. Susannah Burrows under T3.3.6) Q6

Deliverable:
Example UVCDAT-PyNCO script producing observational climatology
dataset used in diagnostics Q6 

Thanks!
Charlie
************************************************************************
burrows.txt
Hi Susannah,

Some of your ACME scripts heavily use NCO, and I want to be sure
you're aware of some features that might help them run faster.

Did you know that ncatted can change multiple attributes at once?
If so, then it appears the scripts are written one ncatted operation
at a time for clarify, and that's especially important for templates.
If not, and execution time of these templates matters, try combining
together multiple ncatted operations so that the time penalty of
expanding the metadata header is incurred as few times as possible.
We can discuss how that might look if you have spare time at Tysons.

Best,
Charlie
************************************************************************
practicum.txt
Hi All,

I realize now I did not fully answer the questions in class last
Tuesday about whether we would talk about other parts of your proposal 
aside from the summary, and whether we there would be specified
mini-deadlines to make sure the proposal bodies were on track.
As I said, we will only talk in detail about the summaries, and we'll
devote our remaining time to practicing slides and delivery.
The proposal body and its organization are up to you (and your
advisor), as is the timing of when you write it, so long as you make
the deadline.

You may wonder why I do not specify mini-deadlines, review proposal
drafts, etc. The Practicum is designed to help transition you from
students to self-motivated researchers. In most or all of your
academic (especially undergraduate) courses so far, instructors tried
to set milestones, assign problem sets, and discuss reading so that
you reach the course goals. That is a traditional course structure.

A Practicum is, by definition, an exercise in the practice of the art.
This practicum credits for crafting your own research proposal. 
As in "real" proposals, the deliverables are a proposal and 

************************************************************************
bali.txt
Hi Marcia,

> Hi Charlie â€” this is GREAT news!!  We are delighted to start working
> with you on next yearâ€™s courses for you.  Our next step with Molly is
> moving a contract along.  With you, we would like to see a syllabus or
> any other information you have on the two courses as it will help us
> begin to draft up a preliminary schedule.  Also would help us with
> wording we will use on our website, as students go to our site to sign
> up and register and pay.   
> 
> I know that it would also help to know how many class sessions you
> will want to do â€” you mentioned 4 classroom half days per week â€” two
> mornings for one class and two mornings for the other class.  Then
> Fridays-SUndays for at least two of the weekends we would do a trip to
> another part of Bali that would tie in with what you are studying â€”
> for example, a weekend in Pemuteran or Lovina, learning about coral
> reef restoration project and a turtle hatchery.  A full day seminar at
> the main university â€” Udayana University, which is an hour away from
> Ubud.  A weekend in Amed which is a tiny fishing village.  And on and
> on. 
> 
> We are so delighted that this is going to happen Charlie â€” be in
> touch!  Also, let me know if you might be able to b e in Bali this
> summer.  Would love to show you what we do in action! 

Charlie
************************************************************************
holly.txt

Hi Holly,

Havn't seem you in so long. I hope I didn't forget a scheduled appt.
Next expect to see you 5/13 @ 11:45. In the meantime, here are some
excerpts from Olivia's blog since the last one I sent 3/18.

Best,
Charlie
************************************************************************
axs.txt

Hi Chris and Christine and Steve,

Thanks for the update Chris, and congratulations on your new job!
It sounds like a promotion or at least more responsibility.
In any case, well-done and I look forward to seeing your creative
solutions applied in wider venues.

A phone conference would be good. Progress is steady. Regridding
global rectangular input variables to arbitrary output variables
works. One specifies an input data file and a destination grid:

ncks -O --rgr=Y --rgr_grd_dst=${DATA}/scrip/grids/remap_grid_T42.nc \
--rgr_out=${DATA}/rgr/rgr_out.nc ${DATA}/rgr/essgcm14_clm.nc ~/foo.nc 

Right now it only works on one 2-D variable. We are re-plumbing so it 
works as expected on all variables. Once that works it may be useful
for Level3 regridding. Then we will tackle regional and SLD grids.
The tests Feng has generated will help evaluate the correctness.

I am traveling next week so Friday is best day for me to telecon.
May I suggest 12 PT/3 ET on 5/8? Or suggest an alternative.
Also please provide a call-in number. 

Thanks!
Charlie
************************************************************************
Good discussion on necessities for unambiguous climo files.
Note that for cell_methods, CF requires using "mean" not "average".

The value of the "bounds" attribute of the time coordinate in the ACME 
(and CESM) model is currently "time_bnds", not "time_bounds".
The analogous value of the "climatology" attribute of the time
coordinate for ACME climo datasets would be "climatology_bnds" not 
"climatology_bounds". Both "climatology_bnds" and "climatology_bounds"
are CF-compliant. However, perhaps the variable names pointed to by
both the "bounds" and "climatology" attributes should be consistent,
i.e., names both ending in "_bnds" or both ending in "_bounds".
Well-behaved tools will automatically handle either or both, but
consistency usually improves downstream usability and so could be a
consideration... 
************************************************************************
Question about climatology convention
Dear CF'ers,
The draft 1.7 conventions example Example 7.8. Climatological seasons
has the following for the time coordinate:

time="1960-4-16", "1960-7-16", "1960-10-16", "1961-1-16" ;

All else being equal, are the values

time="1975-4-16", "1975-7-16", "1975-10-16", "1976-1-16" ;

also be acceptable for this same example?

The underlying question is whether the user is free to choose the
year that labels the time coordinate, or if for some reason the
beginning year (1960) must be used in this example. An alternative
choice that seems reasonable to me is the use of the midpoint year
(1975). I'm unsure whether 1960 was chosen arbitrarily or because one
is expected to apply the minimum operation discussed in this example
(seasonal minimum temperature) to the values of the time coordinate
as well. 

Thanks,
Charlie
************************************************************************

The preceding discussion identifies an important (potentially
answer-changing) ambiguity that CF does fully address, and a 
non-answer-changing labeling ambiguity that CF does not address. 
The CF climatology attribute, together with CF cell_methods,
unambiguously define the arithmetic necessary to reproduce
climatological statistics. At least I have not seen a standard
climatological statistic for which these attributes do not suffice. 

As Phil suspects, CF does not specify how to label the time axis 
coordinate for climatologies. Examples are given with the time
coordinate labeling either the beginning or middle of the
climatological year. Either one is fine. CF says

"The time coordinates should be values that are representative of the
climatological time intervals, such that an application which does not
recognise climatological time will nonetheless be able to make a
reasonable interpretation"

Consecutive first years, or middle years, both meet this guidance. 
The actual time coordinate labeling makes no difference to the
climatological statistic because only the bounds of the climatological
time variable (i.e., the variable named "climatology_bounds" pointed
to by the value of the "climatology" attribute) matter.
This is analogous to the importance of pressure interfaces in
computing mass, where midpoint pressures are not used at all.
NB: This is the "CF climatology convention" not the "CF
climatology_bounds convention". The convention allows one
to pick the name of the variable defining the bounds.
That said, "climatology_bounds" is a clear choice to make.

ACME is free to choose how to label the climatological time
coordinate. The time-coordinate itself does not unambiguously tell the
user anything. However, the time coordinate is often used in plots and
labels so it should be carefully chosen to be clear. Using midpoint
labels (within years and within months) is clearest to me.
This is what example 7.9 chooses. The corresponding values for example
7.8 would be time="1975-4-16", "1975-7-16", "1975-10-16", "1976-1-16". 
The last value must be 1976 not 1975 to preserve monotonicity.
The author of example 7.8 says he chose to use the first year (1960)
rather than the midpoint year (1975) to avoid having to make another
potentially ambiguous decision about what midpoint year to use when
the climatology spans an even (rather than an odd) number of years.
However no mathematically rounding decision is necessary for the
midpoint year because these are merely labels and do not affect how
the math is performed. That information is (supposedly) unambiguously
conveyed by the "climatology" and "cell_methods" metadata+data.
The labels' job is to be representative such that users and tools
can make a reasonable interpretation of the data.

I suggest that ACME _consistently_ choose either the midpoint or the
first year for time-coordinate labels in climatologies. Personally,
I prefer the midpoint.
************************************************************************
Dear Club Carlson/US Bank,

Club Carlson recently removed their much-touted benefit of a
bonus night on award stays. This benefit was clearly advertised
as "BONUS AWARD NIGHT EXCLUSIVELY FOR CARD MEMBERS & ENJOY THE BENEFIT
AFTER PAYING $75 ANNUAL MEMBERSHIP FEE". Based on this promise,
I paid the membership fee ($75) on Jan 5, 2015 and met all spending
requirements. Club Carlson recently rescinded the promised bonus award
night. In addition to the membership fee, my credit rating was
diminished due to the hard inquiry imposed by US Bank.

I ask that Club Carlson show good faith and remedy the loss to a) my
wallet and b) my credit rating by a) refunding the $75 membership fee
as a statement credit and b) crediting my account with 50000 points.

Sincerely,
Charles Zender
************************************************************************
Dear ESMF folks,

I am using the ESMF C-interface to put regridding into NCO.
I have to patch ESMF Version 6.3.0rp1 before NCO can link to it.
The patch is to comment-out these lines in ESMC_CoordSys.h:

//const double ESMC_CoordSys_Deg2Rad= 0.01745329251994329547437;
//const double ESMC_CoordSys_Rad2Deg=57.29577951308232286464772;

and instead to define them once in the global namespace of main().

If they are not commented out then all files that include
ESMC_CoordSys.h get multiple definitions of these constants,
and this causes the compiler to fail. I'm unsure why the
CPP tokens in ESMC_CoordSys.h do not protect these constants
from being multiply defined, but they don't. Using 

#include <ESMC.h>

in NCO is different than using it in a standalone C-program
because NCO does this in a library, so that multiple source
files will see the header, and these constants then become
declared multiple times at global scope (unless my hack
above is used).

Also, please point me to a full C-language example of using ESMF  
regridding, if one exists. I think I've seen one somewhere,
but can't find it now. This may help clarify correct usage
for me.

Sincerely,
Charlie

Subject: varname cannot be NULL in ESMC_GridCreateFromFile()
Dear ESMF folks,

I am using the ESMC interface to put ESMF regridding into NCO.
The varname argument cannot be NULL in ESMC_GridCreateFromFile()
because if so the method ESMCI::Grid::createfromfile() in ESMCI_Grid.C
will fail (ESMF Version 6.3.0rp1) at line 365:
    vnpresent = strlen(varname) > 0;
If this line were instead
   if(varname) vnpresent = strlen(varname) > 0; else vnpresent=false;
Then C-users not get a core dump when varname == NULL.
I am not sure whether this is intentional.
However, the behavior is unexpected because the rest of ESMC seems to
accept NULL for arguments to ignore or to set to default.

Best,
Charlie
************************************************************************
Dear Hilton/Citibank,

I received my Citi Hilton HHonors Reserve card on January 12, and
stayed at the Hilton Garden Inn Washington DC/Greenbelt from
3/23-3/28, at a cost (on the card) of about $800.
In April Hilton offered a $100 statement credit for obtaining the 
card and spening $100 or more at a Hilton within the first three
months, as I was currently doing.
Had I applied for the card two month later, I would have fully
qualified for this $100 offer.
Given that I have spent more than $3000 on this card since January, 
I ask that you please consider awarding me a $100 statement credit.

Sincerely,
Charlie
************************************************************************
Hi Morgan,

Would you please combine these two PDFs into one PDF and return to me?
I currently lack that technology.

Thanks!
c
************************************************************************
acme.txt
Hi Susannah,

Thanks for contacting me about this.
Defining best practices for time coordinates is hard, and I think
you're right that getting all the metadata perfect may not matter.
Only the few tools that require perfect metadata will care.

On 04/17/2015 02:54 PM, Burrows, Susannah wrote:> Dear Charlie,
> 
> Iâ€™m a scientist working with Phil Rasch at PNNL on the ACME project. 
> One of the tasks I have taken on in the project recently is to help 
> with drafting some standards and conventions for the observational 
> datasets to be used in ACME, and Phil suggested that you might be able 
> to give us some advice on this task.
> 
> This is still a work in progress, but the current draft of the 
> conventions document can be found within ACMEâ€™s Confluence: 
> https://acme-climate.atlassian.net/wiki/display/WORKFLOW/Requirements+for+Observational+Datasets+to+be+used+for+ACME+science 

> I have also started to draft a template script with the goal of reducing 
> the amount of thought and effort required when updating observational 
> datasets to meet ACME requirements.  The draft script, which relies 
> heavily on NCO tools, can be found here:
> https://gist.github.com/susburrows/fb0fe95ad2a3348d8ce1

> The aspect of this that is turning out to be the most annoying / tedious 
> at the moment is defining reasonable defaults for the time axis and its 
> attributes, particularly for files containing climatological averages. 
>   I would like to pick a set of standard values for the time axis, e.g., 
> the center of the month for monthly means, or the center of the season / 
> year for seasonal / annual means.  However, the values will depend on 
> the choice of calendar, and I am not sure which one to choose, or 
> whether it really even matters at all.

> So, I was wondering if you have any advice you could share about which 
> calendar would be the best choice.  Any other advice related to this 
> topic would also be welcome.

> It may not be a big issue in the end, but I figure we might as well make 
> the effort to get the conventions â€œrightâ€ from the start.

> Thank you and best regards,
> Susannah Burrows
************************************************************************
Dear Justine,

Zender studies how clouds and pollution (especially soot and dust)
affect snowy surfaces like the Greenland Ice Sheet (GIS). His group
infers how much clouds and pollution heat the GIS by comparing
computer simulations with local and satellite measurements of surface
reflectance. This helps us understand and predict how much sea-level
rise to expect from the GIS and, eventually, Antarctica. 
************************************************************************
Dear Dr. Edgerton,

Thank you for the opportunity to review this manuscript.
Unfortunately I must decline because I have pre-existing teaching,
travel, and research commitments that prevent me from completing the
review in a timely manner. As alternate reviewers I suggest

Dr. Marco Rodriguez <rodriguez@cira.colostate.edu>
Dr. John Ogren <John.A.Ogren@noaa.gov>

With Best Wishes,
Charlie
************************************************************************
Hello Ms. Gage,

We met Fri. morning 3/27 at the Hilton Garden Inn in Greenbelt.
I had slept terribly the night before because the fire alarm above
my bed (in room 337) rang intermittently from ~11:00 PM to ~1:30 AM.  
(BTW, according to the front desk, it rang in my room and in a few
others longer than in the rest of the hotel.)
You kindly asked what would placate me for such a terrible night,
and I answered, blearily due to lack of sleep, that I'd like enough
HHonors points to get at least a free night at any Hilton I wanted.
You said 20k points would be enough for anywhere and I agreed and you
left after saying those points would post in 3-4 business days.

Upon fully waking, I realized that 20k is not enough to stay at many
(most?) Hiltons.  On checkout the staff told me that they were
authorized to award up to 100k points to compensate us for our
negative experience.  The check-out clerk agreed to request the full
100k points for me.  No points have posted as of today, Mon. 4/13.
Please verify that 100k points will be awarded. Thank you for your
attention and professionalism in attending to this matter.
I look forward to remaining a satisfied Hilton customer.

My HHonors Club # is 944515709. 

Sincerely,
Charlie
************************************************************************
Hi Dave,

This response was delayed because I have been traveling.

Thanks for the report.
The move to SF Allura last year broke the old link.
The docs now contain the new link, so I'm closing this.

c
************************************************************************
Hi Guillaume,

This response wasy delayed because I was traveling.
What you seem to wish to do is make the record coordinate ("model")
a variable of of netCDF4 type NC_STRING so that it can hold a ragged
array of model names. This is sophisticated, though easy with ncap2:

% ncecat -u model -v one -p ~/nco/data in.nc in.nc ~/foo.nc
% ncap2 -4 -O -s 'model[$model]={"cesm"s,"ecmwf"s}' ~/foo.nc ~/foo2.nc

The key here is that ncap2 uses the 's' character after the
double-quoted string value to indicate an NC_STRING type.
Once you know that, everything else is available in the manual.
I will add this to the Users Guide. Also the -4 ensures the
output filetype is netCDF4 in case the input filetype is not.

cz

% ncks -v one ~/foo2.nc
************************************************************************
Soon after I sent you the updated card information I received a new
credit card that I'd like to start charging stuff to in order to meet
the minimum spend requirement and earn fabulous rewards :)
So, if the ticketing folks have not yet processed payment,
please pay with this instead:

AmEx 372325258211003 exp. 04/20 cvn 880 cid 0656
Charles S Zender

Thanks!
c
************************************************************************
Subject: Attend Tysons meeting?
Hi Dave,

I'm unsure whether to attend the Tysons meeting, and I would value
your opinion. According to
https://acme-climate.atlassian.net/wiki/display/CNCL/2015-05-05+ACME+Project+All-Hands+Meeting
"most of the ACME team members should plan to attend".
Does this apply to ACME University partners like me?

I just started my ACME project, with no major progress to report 
(received funding in mid-February, got OLCF access a few weeks ago).
I'll have progress to show by December. I want to spend my time
and the ACME money wisely. I do see value in meeting face-to-face.
I know many ACME scientists yet few of the workflow folks.
Maybe it makes more sense for me to come to the UVCDAT face-to-face
meeting in December, combined with AGU.

I asked this of Dean Williams and he's OK with my coming to the UVCDAT 
meeting in December instead of the Tysons meeting.
But I'm unsure what meeting commitments ACME expects from me, and
there may be considerations to my attending that only you as Project
Manager are aware of. Please let me know if you feel strongly one way
or the other about my attending Tysons.

Thanks,
Charlie
************************************************************************
Hi Brian,

Thanks for your help. I tried your suggestion.
It leads to a different $PATH yet acme_regrid is still not found.
Where does acme_regrid live?

Thanks,
Charlie

zender@rhea-login2g:~$ module load gcc cmake
zender@rhea-login2g:~$ module unload python ompi paraview
zender@rhea-login2g:~$ source /lustre/atlas/world-shared/csc121/uvcdat/2015-02-05-mkl-test/bin/setup_runtime.sh
zender@rhea-login2g:~$ acme_regrid -h
-bash: acme_regrid: command not found
zender@rhea-login2g:~$ echo $PATH
/lustre/atlas/world-shared/csc121/uvcdat/2015-02-05-mkl-test/bin:/lustre/atlas/world-shared/csc121/uvcdat/2015-02-05-mkl-test/Externals/bin:/sw/redhat6/cmake/2.8.11.2/rhel6.4_gnu4.4.7/bin:/ccs/compilers/gcc/rhel6-x86_64/4.8.2/bin:./:/ccs/home/zender/sh:/ccs/home/zender/bin/LINUXAMD64:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/X11R6/bin:/usr/bin:/bin:/bin:/ccs/home/zender/bin
************************************************************************
Hi UVCDAT folks,

I am new to UVCDAT and I'm trying to become familiar with UVCDAT and
ACME regridding techniques on rhea/titan as part of ACME.
The instructions here

https://acme-climate.atlassian.net/wiki/display/WORKFLOW/ACME+regridder+instructions

are clear yet they do not work for me:

zender@rhea-login4g:~$ module unload python ompi paraview PE-intel PE-gnu
zender@rhea-login4g:~$ module load cmake gcc
zender@rhea-login4g:~$ source /lustre/atlas/world-shared/csc121/uvcdat/2.0.0/bin/setup_runtime.sh
UVCDAT setup already sourced for this install location.
zender@rhea-login4g:~$ acme_regrid -h
-bash: acme_regrid: command not found
zender@rhea-login4g:~$ 

Any suggestions? I have not yet successfully run UVCDAT either.
I tried module load uvcdat/devel with no success.
Help with either or both acme_regrid or uvcdat would be appreciated...

Thanks,
Charlie

p.s. my $PATH after your instructions is

zender@rhea-login4g:~$ echo $PATH
/ccs/compilers/gcc/rhel6-x86_64/4.8.2/bin:/sw/redhat6/cmake/2.8.11.2/rhel6.4_gnu4.4.7/bin:./:/ccs/home/zender/sh:/ccs/home/zender/bin/LINUXAMD64:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/X11R6/bin:/usr/bin:/bin:/bin
************************************************************************
Hi Dean,

I don't know whether to attend the Tysons meeting.
Is it intended for non-DOE folk like me?
I want to wisely use the ACME money.

Here is my situation:
I could attend assuming my financial analyst lets me use grant money
for travel instead of salary (probably she will).
I teach T/Th so would miss my class one week (not that big a deal).
I've just started my ACME project, with no major progress to report 
(received funding in mid-February, got OLCF access a few weeks ago).  
I'll have something to show by December.

On the other hand, I do see the value of meeting face-to-face.
I know many of the ACME scientists yet few of the workflow folks.
Dave Bader seems to want everyone there, yet I'm unsure he means
people beyond DOE.

What are your thoughts?

Charlie
************************************************************************
Hi Lucky,

I enjoy your blog and tips and yet, as a climate scientist, I wince at
your prolific production of climate-altering greenhouse gases.
Not to single you out, all of us who play this game are guilty.
Have you and/or other mileage gurus analyzed your carbon budgets and
considered carbon offsets as an ethical cost of doing business?
Any guidelines for environmentally ethical mileage practices?

Charlie
************************************************************************
Hi Maarten,

My suggestion is that CF2 recommend that support variables be
identified by implicit scoping where possible, and by relative
or aboslute paths where necessary (i.e., in groups outside the
scope of the host variable). That is for dataset creation.
For dataset reading, a support variable would then be searched
for according to how it is specified. No slashes means implicit
scoping. A non-leading slash means relative path. A leading slash
means absolute path. No ambiguity there, and the support variables
may be anywhere in the file. Unlike you, I expect the dominant use
case across all users will be that support variables will be in
scope of the variables, but that remains to be seen. Your point
is well-taken that there are conceptual reasons to store some
support variables beneath the host variable, though I think this
is unlikely for "coordinates", and "ancillary" is anyone's guess.

Now let me address your questions in reverse order:
Variable with the same name in different groups are not a problem.
Scoping or relative/absolute paths find the specified variable.
Fragility is an issue because, based on my experience doing this
with NCO, it is much easier to write a tool that moves or subsets
groups from a file into a new arrangement without altering the paths
in the "coordinates"/"ancillary" attributes than to write a tool that
subsets and does alter those contents (because the netCDF API does
nothing to keep the metadata consistent). Therefore people will
create such tools. They will do the job well enough for many purposes,
and lead to more broken links when support variables are identifed
with absolute paths.

Best,
c
************************************************************************
Dr. Zender studies the passage of energy and trace species through
Earth's climate system.
He received his BA in Physics from Harvard University, and PhD
from the University of Colorado at Boulder, where he was a
graduate student, postdoc, and visiting scientist at the
National Center for Atmospheric Research.
He joined the Dept. of Earth System Science at UCI in 1999, and
the Dept. of Computer Science in 2012. 
Current and past research includes mineral dust and carbonaceous
aerosols, snow lifecycle and albedo, aerosol impacts on ocean
biogeochemistry, wind-driven surface energy/mass exchange,
climate-disease links, and large scale data analysis. 
************************************************************************
nco smit.txt
Hi! I have a very simple ncap2 script:

TRMM_3B43_007_precipitation@units="mm/month";
TRMM_3B43_007_precipitation(0,:,:)=float(TRMM_3B43_007_precipitation(0,:,:)(24.0)31);

When I run this script against my file, the TRMM variable values get updated, but the units attribute doesnâ€™t change. However, when I reverse the order of the ncap2 commands, the units and data are both updated. I canâ€™t figure out why order matters here.

My data file is:

netcdf timeAvgMap.TRMM_3B42_daily_precipitation_V7.20030101-20030101.80W_29N_77W_31N {
dimensions:
time = UNLIMITED ; // (1 currently)
lat = 8 ;
lon = 8 ;
variables:
float TRMM_3B43_007_precipitation(time, lat, lon) ;
TRMM_3B43_007_precipitation:_FillValue = -9999.9f ;
TRMM_3B43_007_precipitation:comments = "Unknown1 variable comment" ;
TRMM_3B43_007_precipitation:grid_name = "grid-1" ;
TRMM_3B43_007_precipitation:grid_type = "linear" ;
TRMM_3B43_007_precipitation:level_description = "Earth surface" ;
TRMM_3B43_007_precipitation:long_name = "Precipitation Rate" ;
TRMM_3B43_007_precipitation:product_short_name = "TRMM_3B43" ;
TRMM_3B43_007_precipitation:product_version = "7" ;
TRMM_3B43_007_precipitation:quantity_type = "Precipitation" ;
TRMM_3B43_007_precipitation:standard_name = "pcp" ;
TRMM_3B43_007_precipitation:time_statistic = "instantaneous" ;
TRMM_3B43_007_precipitation:units = "mm/hr" ;
TRMM_3B43_007_precipitation:coordinates = "time lat lon" ;
int datamonth(time) ;
datamonth:long_name = "Standardized Date Label" ;
double lat(lat) ;
lat:standard_name = "latitude" ;
lat:units = "degrees_north" ;
double lon(lon) ;
lon:units = "degrees_east" ;
lon:long_name = "Longitude" ;
lon:standard_name = "longitude" ;
double time(time) ;
time:standard_name = "time" ;
time:units = "seconds since 1970-01-01 00:00:00" ;

// global attributes:
:nco_openmp_thread_number = 1 ;
:Conventions = "CF-1.4" ;
:start_time = "2003-01-01T00:00:00Z" ;
:end_time = "2003-01-31T23:59:59Z" ;
:temporal_resolution = "monthly" ;
:history = "Fri Mar 20 20:24:49 2015: ncks -d lat,30.,32. -d lon,-77.,-75. scrubbed.TRMM_3B43_007_precipitation.20030101000000.nc subsetted.TRMM_3B43_007_precipitation.20030101000000.nc" ;
:NCO = "4.4.4" ;
data:

TRMM_3B43_007_precipitation =
0.1935484, 0.1493548, 0.2169758, 0.1646371, 0.1130645, 0.1256049,
0.1791935, 0.239758,
0.2160081, 0.2454435, 0.2130645, 0.2706855, 0.2152822, 0.1144758, 0.174637,
0.1496774,
0.2345967, 0.2394758, 0.25625, 0.1916129, 0.2262096, 0.2272177, 0.2196371,
0.2046371,
0.1861694, 0.2237903, 0.2035484, 0.2125, 0.1922983, 0.2316128, 0.2521774,
0.2764516,
0.2030242, 0.2309274, 0.1951613, 0.1847984, 0.1695968, 0.1889113,
0.2616532, 0.2675403,
0.1658064, 0.217258, 0.2341532, 0.1769758, 0.2118145, 0.2470565, 0.3009274,
0.2822178,
0.1446371, 0.162258, 0.1447177, 0.1417339, 0.1510081, 0.2327016, 0.2850403,
0.2698387,
0.1590323, 0.1825806, 0.148629, 0.1675, 0.1703226, 0.2367339, 0.2171774,
0.2148387 ;

datamonth = 200301 ;

lat = 30.125, 30.375, 30.625, 30.875, 31.125, 31.375, 31.625, 31.875 ;

lon = -76.875, -76.625, -76.375, -76.125, -75.875, -75.625, -75.375, -75.125 ;

time = 1041379200 ;
}

Iâ€™m running ncap2 as follows:

ncap2 -O -S script.ncap2 timeAvgMap.TRMM_3B42_daily_precipitation_V7.20030101-20030101.80W_29N_77W_31N.nc out.nc

My ncap2 version is 4.4.4.

Thanks!
Christine
************************************************************************
diwg cf sneep.txt
Hi Maarten,
Adding distinct standard_name for interface variables is simple, yet
does not seem a fully satisfactory way to convey the off-by-one issue.  
Then there would be two names for pressure, one for the pressure 
whose vertical coordinate is at layer midpoints and one for the
pressure whose vertical coordinate is at layer interfaces. 
A dataset containing both would then have variables with
standard_names like air_pressure and air_pressure_at_layer_interfaces.
While that is clearer (to me) than using air_pressure for the
off-by-one case, it still leaves something to be desired.
Mainly, it does not clearly convey the distinction between the two
pressures. 

Maybe there is no crystal-clear way to convey off-by-one because it is
a subtle property. I hope others who like your off-by-one proposal
will chime-in with suggestions on whether and how to convey that the
off-by-one convention applies to a particular variable. If nobody
suggests something clearer than a new standard_name then so be it. 
Charlie
************************************************************************
diwg cf sneep.txt
Hello Maarten et al.,

Thanks to Aleksandr Jelenek for pointing me to this discussion.
Maarten asked me this question a year ago and my response could be
characterized as the opposite of what John recommends :)
Here are the three plausible options mentioned so far:

1. :coordinates = "lat lon time"; // Scoping rules determine nearest ancestor
2. :coordinates = "/absolute_path/lat /absolute_path/lon /absolute_path/time";
3. :coordinates = " relative_path/lat  relative_path/lon  relative_path/time";

Option 1 uses scoping rules to disambiguate which variables the
coordinates attributes refer to. Scoping obviates the need for
relative or absolute paths. Datasets created this way can be 
most easily dismembered and reassembled without invalidating the
metadata, or needing to recompute paths based on the locations
of groups in the new file. 

Option 2, full paths, will lead to orphaned coordinates once 
the original group is extracted into a new file. So will Option 3,
except in the degenerate case where the lat/lon/time variables are
in the same group as the referring variable. 

The best argument for recommending Option 2 is its specificity---
it's unambiguous. Option 3 is less specific, yet subject to the
same downsides as Option 2 after downstream processing. 
Hardcoded paths will lose their self-consistency as users dream
of new ways to recombine and aggregate measurements and models.

The most elegant and resilient solution seems to be Option 1.
Scoping variables with the same rules as dimensions maintains
consistency in the "coordinates" attributes at all processing stages.    

I see some virtue of supporting Option 2 and Option 3, namely,
CF2 ought to support full names instead of short names in all,
or at least most, instances, to exploit the CDM. Thus I advocate
Option 1 as the _recommended_ solution to Maarten's issue, while
retaining Options 2 and 3 as "legal", yet not recommended.

Finally, Maarten noted that the existing API could be enhanced to
more easily locate the nearest "in-scope" variable. Should Option 1
gain traction, it would be important to follow-up on this to ensure
it is easier for tool developers to implement. We had a tough time
implementing support for Option 1 when we implemented support for
extracting ancillary and coordinate variables from hierarchical
datasets in NCO. Implementing options 2 and 3 was straightforward. 
With a few additions to the netCDF library API, Option 1 would be
as easy to implement, and a more desirable and robust design as well. 

Best,
Charlie
************************************************************************
Thursday notes:
Citations 3
Search (Ed, Chris) 4
Virtual Collections (Chris Anna) 5
ICARTT (Emily ASDC) 5
OpenSource (Wayne) 2 
Data Quality (Dave C) 1 (Blindly re-learning ISO)
Dataset Interoperability 
Airborne (She) 3
Product Designer (Ted Habermann) 3 (Tool in search of problems, lock-in)
Data-Intensive Architecture (Thomas Huang) 2 ()
Time Series (Bill Teng) 4
Data Recipes (Suhung Shen) 4
Cloud Computing (Brian Wilson) 5
Browser-based Visualization (Charles Thompson) 
ACSI ASA (She) 1 (too meta)
ASCII (Keith) 
************************************************************************
1) Complete recommendations for best practices for datasets with Grid-like
2) Develop best practice recommendations for datasets with Swath-like structure
3) Improve working requirements and tests for interoperability and compliance
4) Recommend metadata practices/extensions to harmonize conventions with netCDF/HDF capabilities
5) Continue to help develop CF metadata for groups
************************************************************************
Dear Admissions Committee,

I write to ask the admissions committee to reconsider Bruce Yang.
Some members accept that the candidate's negative qualities could
be offset by his specific skillset. I agree. I have spoken with him
numerous times in the past year. His English does not discernably
differ from some of our first year students, and it's improving. 
Some reasons he left his last program stemmed from his personal life,
and some were due to academic misunderstandings. Bruce has learned
from these mistakes and is highly motivated to rise to his potential
at UCI. 

The other candidate I was interested in, Beth Elliott, accepted UCD.
So for me this year it's Bruce or no one.

Best,
Charlie
************************************************************************
I'm sure your defense will go well. You're almost there.

The general area is parallelized regridding and arithmetic.
A specific task would be to implement OpenMP or MPI in the processing
and/or I/O loops of any operator. I don't expect any work done before
you start, so we first need to agree on a start date. When is best for
you?

c
************************************************************************
Not yet sent:
Dear Mark,
I'm making progress on regridding on NCO.
I want OLCF to upgrade NCO from 4.3.1 to 4.4.8.
Please second this by visiting 
https://www.olcf.ornl.gov/support/software/software-request
Thanks!
Charlie
************************************************************************
Dear Yasmeen,

I wish to apply for the NSTP program again this year.
Attached is last year's form FYI, and a draft of this year's.
Please check the accuracy and completeness of the 2015 application.

I'm unsure whether to include myself as a "research group member".
Also unsure what, if anything to put for Dingying and Pedro in the
# Qtrs columns. Is Outside Fellowship anything except TA? 
Also the 2014-15 approved course load is for the same year as last
year's form. Maybe they meant 2015-2016?

Comments welcome. When do you need the final form from me?

Thanks,
Charlie
************************************************************************
I appreciate everyone who found time to wish me Happy Birthday. After the big Hawaii Five-O blow-out last year, a quiet birthday with my lover was perfect. May you all have many evenings like it, with savory food, balmy ambience, panchromatic drinks, candles, romance, live music infusing the gospel of boogie into throughout your nervous system, and impromptu, unskilled dancing. All bookended by a relaxed, non-teenagery talk with Olivia and a sweet rendition of Happy Birthday by Ruby, my kids.
Thanks again Bridget, Mary Ellen, Franny, Marcy, Claudia, Ann, Bill D., Linda, Carolyn, Cozette, Ashley, Shari, Kevin, Justine, Ken, Ron, Anji, Mary, Susan, Amy, Julia, Scott, Sarah, Christine, Bill Z., Carol, Stephanie, John C., Danielle, John M., Ed, Brian, Dan, Erika, Maria, Manuel, Petra, Betty, Ruth, Carter, Peter, Isabella, Lizzie, and Chris.
************************************************************************
Greenland has experienced a negative summer albedo trend across the
entire ice sheet. This is widely known and has been shown with
independent data such as Box et al. 2012 Figure 7a who use MODIS.
The negative trend is stronger in the ablation zone, yet still
negative in the accumulation zone including Summit.
It would be surprising if AVHRR showed anything different.
Hence lines 298-299 must be removed or better explained.

************************************************************************
1. Kavli Frontiers Fellow, US National Academy of Sciences: 2009, 2010, 2011
2. NASA New Investigator Program (NIP) Award, 1/01--12/04
3. Outstanding Student Presentation in Atmospheric Sciences
   Section, Fall AGU Meeting, San Francisco~CA, 12/95
************************************************************************
Here's my diagnosis. I'm not sure what to do about it.
I encourage any reading this to respond with suggestions.

1. The slowdown probably originated in NCO 4.4.7, which adopted a new
default policy to preserve the existing chunking of input data unless
told otherwise explicitly. Your input is netCDF3 (no chunking) and
your output is netCDF4 so chunking is required for record variables.

2. Since there is no previous chunk size NCO assigns chunksizes equal
to dimension sizes. The big variable gets assisnged one chunk with
dimensions 1872x145x192 = ~210 MB. This single chunk is too big to 
succeed with HDF5. I did not let it run to completion.
Perhaps this is also a netCDF4/HDF5 library issue?
My understanding of the tradeoffs of chunk sizes is limited.
I thought that a single big chunk was effectively the same as storing
the data contiguously, i.e., unchunked. Appears I am mistaken.
What is a good algorithm to determine the largest chunksizes
that HDF5 chunking can handle in a reasonable amount of time?

3. Your workaround works, as would most other chunk maps,
because none are insane enough to create 200 MB chunks.
Your dataset, BTW, is exactly the shape that would benefit from
the new "rew" chunk map, i.e., balanced chunking. Consider using
--cnk_map=rew.

4. I can add a "limiter" to the overall chunksize that would prevent
this in future versions of NCO. Or I can do something else.
Suggestions welcome.

Charlie
************************************************************************
Hi Ted,

That' s great news. Here's the information you asked for:

Will Call: Charles S. Zender
Four tickets (me, Lizzie, Oliver, Marina)
First choice: Fri evening Apr 10
Second choice: Sat evening Apr 11
CC: 4870420001353084 exp. 01/18 cvn 011
Charles S. Zender

This will be my first musical in NYC!
I am so deprived.
Does Lincoln Center count as a "Broadway" show?
Or will I need to come back? 

Many thanks,
Charlie

On 3/9/15 09:08, Ted Sperling wrote:
> Hi, Lizzie and Charlieâ€¦. good news! They can accommodate us on the 10th. 
> Charlie, Iâ€™ll need the following info from you to process the ticket order.
> 
> Will call name
> Number of tickets
> Performance date and time (and second choice if possible)
> Credit card number
> Exp. Date
> Security code (3 digits on MC or Visa, 4 digits on AmEx)
> Name on card (if different from above)
> 
> I look forward to seeing you at the show!
> 
> Best,
> 
> Ted
************************************************************************
It turns out that Rich's issue was a known "feature" that I had
documented in the manual, then promptly forgot:

http://nco.sf.net/nco.html#xmp_xtr_xcl

To prevent this non-intuitive behavior from surprising anyone else
(including me!) again, I've just added a warning that explains why
"-C -x -v crd_nm" may be necessary.

zender@roulee:~/nco$ ncks -O -x -v time ~/nco/data/in.nc ~/foo.nc
ncks: WARNING Explicitly excluding (with -x) a coordinate variable (like "dgn") from the extraction list does not always exclude the coordinate unless the -C option is also invoked to turn off extraction of coordinates of other variables. Otherwise, a coordinate you wish to exclude may be extracted in its capacity as coordinate-information for other extracted variables. Use "-C -x -v crd_nm" to guarantee crd_nm will not be output. See http://nco.sf.net/nco.html#xmp_xtr_xcl for more information.
zender@roulee:~/nco$ ncks -O -C -x -v time ~/nco/data/in.nc ~/foo.nc
zender@roulee:~/nco$ 

Chris, as you say, this "-C" workaround will also handle the situation
you encountered. The better solution, though, is to have ncwa
eliminate dimensions from the "coordinates" attribute as it collapses 
them. The side effect you mention is subtle. I had not thought of it.
The software solution can piggyback on the "cell_methods"
infrastructure we introduced a year or two ago. This is now
TODO nco1118.

Let me know if you have other suggestions to improve this plan.

cz
************************************************************************
Dear DIWG,

Thank you for your participation this past year.
Let's wrap-up what we've done so others can benefit.
Two things for you to do are review our recommendations
and comment on the proposal for 2015-16 DIWG.

1. Recommendations:
Since the last telecon we have refined our recommendations.
Please give these a look and make any comments on the Wiki.
As they say, silence equals consent.

I reproduce the current recommendations below so if they appear OK
then no reason to log-in. The drafting location of these
recommendations is just above the comments section here:

https://wiki.earthdata.nasa.gov/display/ESDSWG/Guidelines+for+Creating+Group+Structures+in+Earth+Science+Data+Products

Once these are finalized, they will be placed on their own page,
and Peter and Mary Jo have volunteered to "posterize" them for
the ESDSWG meeting. (Some accomplishments, like Ed's CF/ACDD/XXX
checker and progress on CF, are not reflected in recommendations).  

2. 2015-16 DIWG:
The proposal for the next year of DIWG is here:

https://wiki.earthdata.nasa.gov/pages/editpage.action?pageId=47122767

Please visit and give feedback, make changes, and add your name to
the Proposer's section if DIWG continue to interest you. 
Final 2015-16 plans will be agreed on at the ESDSWG breakout.

Hope to see you there!
Charlie+Peter
************************************************************************
First, though:

Create a branch (called dyw), checkout that branch ('git checkout dyw'
in your local clone of the nco repository), work on that branch, push
your changes to github (git push) as you make them, then issue a pull
request for me to merge your completed changes to the master branch.

You can try all of this without me. On Monday we'll go through
whatever problems cropped up with this procedure. OK?

************************************************************************
Hi Bruce,

Five faculty besides me have examined your application.
None of the other recommend admitting you.
Below are some of their concerns.

You may write a succinct and clear response to their concerns.
If you decide to do this...
Do it on your own---do not ask others to ghost write or edit it.
Do not quote any of their comments.
Do address their concerns and demonstrate you understand them.

Send me the letter as a signed PDF and I will give it to them.

Good luck,
cz

By AAA on Fri, 27 Feb 2015, 1:21 PM [Probable deny]
There are some red flags with this student, and it is not very clear
what happened in his previous graduate program as mentioned by other
members of the committee. If Charlie really wants him because he has
the skills that he is looking for, I would support it, but I feel like
there are lots of good students applying to the program, and there are
too many red flags for Bruce. 

By BBB Fri, 27 Feb 2015, 7:34 AM [Probable deny]
I am also concern on why he left his previous graduate program and why
there is no reference letter from his previous graduate advisor. 

By CCC on Thu, 26 Feb 2015, 7:28 PM [Probable deny]
Hard call - I would support admitting if Charlie really wants him
based on his publication record, but agree that the English limitation
is problematic. 

By DDD on Thu, 26 Feb 2015, 3:54 PM [Probable deny]
I have reservations based on my interaction with this student last
year, specifically the poor English skills, which were in stark
contrast to his publication record. 

By EEE on Fri, 20 Feb 2015, 11:03 AM [Probable deny]
I have issues with why the student left a PhD program in the past, why
he does not talk about his current occupation, and comments from ESS
UCI colleagues based on an interview last year that his english is
poor (despite being in the US since 2009). I would not admit this
student based on these concerns.  

By Charles S. Zender (zender@uci.edu) on Tue, 10 Feb 2015, 6:14 PM [Admit] 
Yes, I recommend admitting Zhifeng Yang.
************************************************************************
Hi Sheri,



Hi Charlie,
As a collaborative partner on the proposal we'll need the following
documents from you with a March 10th deadline. Please note that your
final approved budget is due March 10th, however, I'll need a copy of
your draft budget by COB, March 6th. 

    SOW
    Budget and budget Justification
    C&P (for senior personnel)
    Biosketches (for senior personnel)
    Facilities & Equipment and Other Resources
    COI (NSF template attached)
    Negotiated indirect cost rate agreement
    A single page description of relevant prior support

Please let me know if you have any questions.

Thank you,
Sheri
************************************************************************
All the good news in one email

Oliver has a semi-regular part-time job at the Farmer's Market.
He works Saturday mornings as a gopher for the market manager.
Duties include manning (doh!) the information booth, traffic
direction, driving people around in golf carts.
Most importantly, he interacts with people and community. 


************************************************************************
Hi Ethan,

Your current draft summary says:

"The University of California, Irvine group will test new standards
features in the NCO (netCDF Operators) toolkit with a particular focus
on using hierarchical group structures to support advanced data
organization (e.g., organizing ensemble members)."

I suggest this drop-in replacement:

"The University of California, Irvine group will focus on extending CF
to exploit hierarchical group structures in the enhanced data model,
and will prototype support for these extensions in the NCO (netCDF
Operators) toolkit."

In our first correspondence you said the program would support work on 
conventions, not on software development. Your latest message asks me
to put resources into NCO development. While I'm happy to request
funds for this, I first want to be sure that's your intent :)

Attached is my budget estimate for 1 month summer salary per year.
The direct costs, ~$23k/yr, are pre-overhead. Will UCI be a
sub-contract to UCAR (UCAR overhead + sub-contract fee first year) or 
will UCI submit this as a collaborative proposal (UCI overhead both
years)?

I would estimate trips to Boulder cost $1500 each for 3-night stays.
Probably should be 5 nights for the workshop trip.
So will add $7k for two trips per year for two years.

Do you want to talk?
Otherwise will work on the UCI project description today.
I will put in some software development, per your message, but keep
it light until we talk.

c

Proposal title is
EarthCube IA: Collaborative Proposal Advancing netCDF-CF for the
Geoscience Community.
9/1/15-8/30/17
************************************************************************
Hi Cynthia,

A few more things. First, the primary institution is Unidata.
UCI will be a sub-awardee (less work for you?).
The Unidata financial contact is Sheri Ruscetta <ruscetta@ucar.edu>.
The PI is Ethan Davis <edavis@ucar.edu>.

Proposal title:
EarthCube IA: Collaborative Proposal Advancing netCDF-CF for the
Geoscience Community

Dates: 9/1/15-8/30/17

Once you make these changes and tweak the budget as in the previous
request, I'll take another look at it. Then you tell me what to do.

Thanks,
c
************************************************************************
Hi Cynthia,

For the NSF EarthCube proposal please:
1. Fix adding problem: FY2015 should cost more than FY2014
   Fix colum Headers so years match dates.
   Eliminate Year 3 (it's a two year proposal).
2. I hope/expect raise for Prof IV kicks in on 20160701
3. Add two trips per year to boulder:
   Year 1: $3.5k, Year 2: $3.5k. Travel total = $7k.

I will have more info for you later today or tomorrow...

Thanks very much,
c

************************************************************************
Hi Lizzie,

Looks like I can buy tix today for 4/3 and not 4/10.
I think you told me that tix for 4/10 go on sale 3/10.
The tix for 4/3 are only available in the side sections.
Center is sold out, does not show, or reserved for other reasons.
Unless I hear more from you before you go to Myanmar, I'll buy four
tix for 4/10 on 3/10 and likely they will be side seats.
Guidance from your brother on whether/how better seats are available
would be appreciated.
Thanks!
Charlie
************************************************************************
Hi Trish,

Thanks so much for using Oliver as a gopher last weekend.
He said it went fine and he wants to keep doing it.
I also think he likes people asking him for answers.
Wearing that official orange vest made him an expert :)

Robynn, my-ex, whom you probably met years ago, will bring Oliver
about the same time (~8:15 AM) this Saturday. Unless we hear otherwise
from you, we'll bring him every Saturday. Obviously call me if there
are other problems. I hope your new gopher helps the market in some
small way. Know that getting out of the house and socializing with
strangers will help Oliver in a big way :)

Best,
Charlie
************************************************************************
diwg.txt

We recommend that the dimensions in datasets in grid structures be
ordered in a manner that makes the datasets easy for expected end
users to read.  The Aura File Format Guidelines document suggests that
the dimensions in HDF-EOS files be ordered based on how rapidly the
dimensions change, with the most rapidly changing dimension first when
coding in Fortran (e.g. (XDim,YDim,time)), or with the most rapidly
changing dimension last when coding in C (e.g.(time,YDim,XDim)).  The
Fortran- and C-coded dimension orders are equivalent in the HDF-EOS
product files.  

************************************************************************
git.txt
Dear NCOers,

The NCO repository has moved to Subversion.
And that only lasted a few days.
Now the NCO repository is on GitHub!
Yes, in the past week it leapfrogged from CVS->SVN->Git.
10110 changesets since 1998, which was, I think, when I switched from
RCS (or SCCS) to CVS.
This was prompted by a week-long lock-out on SourceForge's CVS.
And probably long overdue.

Code access now is through:

git clone https://github.com/czender/nco.git

I am a bit nervous because I never really used git before.
I tried to keep all the CVS history, but did I succeed?
I don't see any of the tags, like nco-4_4_8, although I know they were
all carried over fine into SVN.  

Please give the repository a try sometime, and tell this novice 
how to improve it. I don't mind re-generating the master branch
from SVN once or twice to get it right on Git.

The base code now is vanilla 4.4.8. Send me a pull request for some
new code so we can see the git magic work. I'd like to finalize the
transition within one week, say by 2/27. That means be prepared to
resend pull requests until 2/27, in case I have to rebase the master
branch before then. I don't anticipate that happening, though. 

Best,
Charlie
************************************************************************
Hi Ethan,

Thanks for contacting me about this proposal opportunity.
Yes, I'm interesting in participating!

I am happy to support your ideas for improved DSG and irregular region
support via the enhanced data model in CF2.x. My primary interest in
CF2.x is to standardize treatment of enhanced model features to
exploit treatment of ensembles and dataset intercomparisons.
Basically stuff that, as you noted, involves exploiting groups.
Depending on my level of involvement, I can either cheerlead for your 
propossed convention development, advocate for mine, or both.

My minimal involvement level is one-month summer salary support
each year. Developing use cases, soliciting input, and drafting CF
lingo is feasible with this level of support. Implementing or
prototyping features in NCO would require additional support.

Unidata has great resources to hold workshops, so my only suggestion
other than the CF extensions themselves, is to hold a CF2
brainstorming/drafting workshop open to all in Year 1, with some
travel support doled out to ensure some depth/breadth of community
participation.

Happy to talk more by phone or email,
c
************************************************************************
Hi James, Aaron, and Michael,

Clearly she is an effective and articulate speaker who handles
questions well. I have no doubt she'll be a good teacher.

We saw her seminar from our respective vantage points.
The work she presented is easy for me to appreciate and to criticize.
I appreciate that she is modifying old tools to ask new questions.
Exoplanetary atmospheres are fascinating and, as she showed, we can
begin studying their behavior with simplified (e.g., aquaplanet) GCMs.   

I'd be more comfortable with someone who demonstrated expertise in the
physics of planetary science or climate or both. 
She runs "off-the-shelf" tools (e.g., GCMs), and modifies their
boundary conditions (e.g., spectral flux, land/ocean/ice distribution). 
Will she always rely on domain experts to understand the models'
limitations from a physical standpoint or will she develop areas where
she actually understands from first principles what the models do, 
don't do, and why? 

On the plus side she is an "integrator" who uses others' models to ask 
interesting questions. It's a viable niche for important science.
The weaknesses I sensed in her understanding of climate processes were
perhaps forgivable from a postdoc astraddle two disparate fields.

However I "get" that one can't expect seamless expertise in planetary
atmospheres and Earth's climate in young candidates. From Astronomy's
point of view she might display a whole other set of promises and
flaws. That's the difficulty with new interdisciplinary science! 
It's too easy to criticize from distinct domains and miss the bigger
picture that a new field is evolving. And she is an ambitious pioneer
in this fertile new field.  

The bottom line is that given PPFP hiring incentive, I think pursuing
a position she could apply for is warranted. There are many in ESS
with whom she could collaborate. If this progresses further then at
some stage we should ask her if she is interested in a formalized
connection with ESS (i.e., joint or JWOS). 

Charlie
************************************************************************
sf.txt: svn
https://sourceforge.net/p/forge/site-support/5518/
So, the basic steps are the same as before, backup, convert, upload, import.

1. See this documentation for instruction on how to get a local backup of
your CVS repository (which you will use for the conversion)
https://sourceforge.net/p/forge/documentation/rsync%20Backups/?version=8#cvs

2. Then use a utility like cvs2svn to convert the repository (if using
cvs2svn, you should use the "--dumpfile" option to output as a dump) 

3. Upload the converted repository to your project home directory
https://sourceforge.net/p/forge/documentation/File%20Management/

4. Import your SVN repository using one of these methods
https://sourceforge.net/p/forge/community-docs/svn%20import/

cvs2svn -v --dry-run --encoding=ascii --encoding=latin_1 --dumpfile=/home/zender/nco_20150216_dumpfile /data/zender/nco_20150216 > ~/cvs2svn.txt
cvs2svn -v --dry-run --encoding=ascii --encoding=latin_1 -s /home/zender/svn /data/zender/nco_20150216 > ~/cvs2svn.txt

rsync -aiv nco_20150216_dumpfile zender,nco@web.sf.net:
ssh -t zender,nco@shell.sourceforge.net create
mv /home/project-web/nco/nco_20150216_dumpfile .

# initialize repository
svnadmin create /home/project-web/nco/svn
# do the hard work
svnadmin load /home/project-web/nco/svn < /home/project-web/nco/nco_20150216_dumpfile
timed-out failed after revision 9337
# check-out
svn checkout --username=zender svn+ssh://zender@svn.code.sf.net/p/nco/svn/ ~/nco-svn
# import to sf from local repository
cd existing-nco-svn
svn import svn+ssh://zender@svn.code.sf.net/p/nco/svn/ -m "Initial commit"

adminrepo --checkout svn
rm -rf /nco_20150216_dumpfile/nco/*
svnadmin create /nco_20150216_dumpfile/nco/
svnadmin load /nco_20150216_dumpfile/nco < nco_20150216_dumpfile
adminreppo --save

svn checkout svn+ssh://localhost/home/zender/svn/trunk/nco nco-svn
svn checkout svn+ssh://glace.ess.uci.edu/home/zender/svn/trunk/nco nco_glace # works!
svn checkout http://glace.ess.uci.edu/svn/trunk/nco nco_glace # 
svn checkout --username=zender svn+ssh://zender@svn.code.sf.net/p/nco/svn/trunk/nco nco_sf
svn checkout file:///home/zender/svn/trunk/nco ~/nco_svn # works

sudo aptitude install git-core git-svn ruby rubygems
sudo gem install svn2git
/bin/rm -r -f ~/nco/git/nco
mkdir -p ~/nco/git/nco
cd ~/nco/git/nco
svn2git svn+ssh://localhost/home/zender/svn/trunk/nco --authors ~/nco_authors.txt --verbose # fails
svn2git svn+ssh://localhost/home/zender/svn/trunk/nco --rootistrunk --authors ~/nco_authors.txt --verbose # fails
svn2git file:///home/zender/svn/trunk/nco --rootistrunk --authors ~/nco_authors.txt --verbose # fails
svn2git svn+ssh://glace.ess.uci.edu/home/zender/svn --rootistrunk --authors ~/nco_authors.txt --verbose # fails
svn2git http://glace.ess.uci.edu/svn/trunk/nco --rootistrunk --authors ~/nco_authors.txt --verbose # fails
svn2git http://glace.ess.uci.edu/svn/nco --rootistrunk --authors ~/nco_authors.txt --verbose # fails
svn2git file:///home/zender/svn/trunk/nco --authors ~/nco_authors.txt --verbose # fails
svn2git file:///home/zender/svn --authors ~/nco_authors.txt --verbose # fails
svn2git file:///home/zender/svn/trunk/nco --trunk trunk --tags tags --nobranches --authors ~/nco_authors.txt --verbose # fails
svn2git file:///home/zender/svn/trunk/nco --trunk / --tags tags --authors ~/nco_authors.txt --verbose # works
svn2git file:///home/zender/svn/trunk/nco --trunk / --authors ~/nco_authors.txt --verbose # works
svn2git file:///home/zender/svn/trunk/nco --trunk / --tags tags --nobranches --authors ~/nco_authors.txt --verbose # works
svn2git file:///home/zender/svn/trunk/nco --metadata --trunk / --tags tags --nobranches --authors ~/nco_authors.txt --verbose # works
svn2git file:///home/zender/svn --authors ~/nco_authors.txt --exclude CVSROOT --no-minimize-url --metadata --trunk trunk/nco --verbose # fails
svn2git file:///home/zender/svn --authors ~/nco_authors.txt --no-minimize-url --metadata --trunk trunk/nco --verbose # fails
svn2git file:///home/zender/svn --authors ~/nco_authors.txt --metadata --trunk trunk/nco --verbose # fails
svn2git file:///home/zender/svn --exclude CVSROOT --no-minimize-url --metadata --trunk trunk/nco --verbose # works
svn2git file:///home/zender/svn --authors ~/nco_authors.txt --exclude CVSROOT --no-minimize-url --metadata --trunk trunk/nco --verbose # works
svn2git file:///home/zender/svn --authors ~/nco_authors.txt --exclude CVSROOT --metadata --trunk trunk/nco --verbose # works
svn2git file:///home/zender/svn --authors ~/nco_authors.txt --exclude '.?CVSROOT.?' --metadata --trunk trunk/nco --verbose # infinite loop?
svn2git file:///home/zender/svn --authors ~/nco_authors.txt --exclude '^CVSROOT.*' --trunk trunk/nco --nobranches --verbose # 
git remote add origin https://github.com/czender/nco.git
git push --set-upstream origin master
git clone https://github.com/czender/nco.git

# test on smaller repository
cd
/bin/rm -r -f f_new
cp -r f f_new
cd ~/f_new
git init
git add *.F90
git commit -m "Add F90 files"
git remote add origin https://github.com/czender/f_new.git
git push --set-upstream origin master
git clone https://github.com/czender/f_new.git

# direct git import without svn2git
git svn clone file:///home/zender/svn --authors-file=~/nco_authors.txt --no-metadata -s nco_direct_from_git
************************************************************************

I had a similar problem

https://sourceforge.net/p/forge/site-support/9688

SF still has not read my support ticket after 1 week.
The following procedure did fix my problem.
I figured it out through reading old tickets, trial, and error.
Maybe it will work for you.
Replace nco and zender with your project and login ID:

rsync -av nco.cvs.sourceforge.net::cvsroot/nco/* . # Backup CVS repo
ssh -t zender,nco@shell.sourceforge.net create # Create login shell
sf-help --web # Print location of hidden directories
cd /home/project-web/nco # Apparently this is my repo
adminrepo --unlock cvs # This gets rid of the dreaded locks
adminrepo --save # This did not actually seem to help

************************************************************************
Charles Zender â€“ atmospheric physicist, expert on trans-continental
air pollution, dust storms, and snow. 949-891-2429 (cell and
office are same number), zender@uci.edu

************************************************************************
Dear CISL,

Apparently I have lost access to the CISL computers on which I develop
and distribute NCO. You may know that NCO has been widely used on CISL
computers since forever. My support for NCO at CISL depends on having
a project number, and I cannot test the latest version, to be released
this week, without one. NCO is used by the community and is embedded
in analysis scripts of various CESM working groups. 

Please assign NCO a project number or call me and discuss how to
proceed. I do not currently have an NSF project, yet it is hard to
believe that CISL does not have the resources to assign a project
number to such a widely used tool. Don Middleton and John Clyne and
Dick Valent know what NCO is in case the person reading this does
not. One of them might be willing to add me to one of their
projects. NCO development and testing does not require many resources
in terms of GAUs, yet does require interactive access to all the
machines and storage arrays on which users run it.

Thank you,
Charli
************************************************************************
Hi Jialin,

I think you would be very productive and gain invaluable experience
improving parallelism and regridding of our scientific workflows.
I will have a postdoc position open for you when you are ready.
Let me know when you have a more definite date to start.

Charlie
************************************************************************
Hi Wenshan,
Here are some questions your work could address:
What are the relative contributions of CRE and advection in recent
melt events?
Given that, what do CMIP5 simulations suggest about the
extent/frequency of future melt events?
What is the PDF of CRE or melt events and extent in the best CMIP5
models? 

************************************************************************
fodor.txt
Hi Regina,

Thank you so much for contacting us about this opportunity for Olivia.
(And congratulations on your belated name change.) 

************************************************************************
jialin.txt
Hi Jialin,

This does not translate well:

> Like what you have told me, the NSF funded project is somehow too
> abstract,

What do you mean by it?

> if I can have some practical experience in your group, I believe my
> faculty job searching would be much easier and outstanding.

And what do you mean by "practical experience"?

Charlie
************************************************************************
diwg:

A Time dimension is required for a single grid product file that contains many time intervals worth of daily, weekly or monthly averages. In contrast, grid product files that are distributed as daily, weekly or monthly granules, could be defined without a time dimension because each file records the specific time interval being provided in both the file name and file-level attributes. We nevertheless recommend that a Time dimension be defined and used in all data fields that vary in time, regardless of whether multiple time slices are stored in the file. More specifically, we recommend that Time be defined as a record dimension, not a fixed length dimension. This practice allows downstream users to more easily and efficiently aggregate data across separate files because HDF5 and netCDF4 storage geometries and APIs are designed to more easily extend record dimensions than fixed-length dimensions. 

Unfortunately not all software understands multi-dimensional datasets with Time as one dimension. Some GIS software, in particular, may fail when it encounters datasets with more than two dimensions. Thus it is sometimes important to be able to remove the Time dimension from datasets. The NCO ncwa operator can eliminate the time dimension from an entire file with a single command.


************************************************************************
Hi Dingying,
Next LSD problem, and last one that involves code, is to add an 
attribute to the output of any variable that is rounded.
Attribute name is "least_significant_digit" (NC_CHAR).
Attribute value is, obviously, lsd (NC_INT).
Write the attribute at the same time ncks writes/copies
the rest of the variable's attributes to the output file.
Use the nco_aed_prc() routine to implement the attribute write.
Examples are in the code.
I can meet after 3 (let me know) if you have questions on this.
Thanks!
c
************************************************************************
Atheists compete with Christers to lend the most through Kiva micro-finance loans at http://www.kiva.org . Team A+ is for Atheists, Agnostics, Skeptics, and Freethinkers. Their closest competitor is, appropriately, the Kiva Christians (KC). A+ narrowly beats KC in all-time lending by ~$19M to ~$15M. Recently, though, KC has loaned a few $100k per month more than A+. Support your tribe and show who's more charitable.
************************************************************************
Hi Kevin,

I like the idea of a task to demonstrate programming chops.
Here is a user request for an NCO feature that a competent C
programmer could do:

https://sourceforge.net/p/nco/discussion/9829/thread/89f96b4c/?limit=25#ea27

A more advanced challenge:
The projects that support NCO are aimed at parallelization and
regridding. Parallelizing code can be difficult. Fixing the OpenMP
section of any arithemetic operator, e.g., ncwa, would demonstrate  
the desired skill in this area.

Bonus points for a patchset that adheres to NCO coding style.

Best,
Charlie
************************************************************************
Howdy family,

Many developments with O. in the past week.
Much of this appears on his (keep reading) Tumblr blog
http://leafbitch.tumblr.com
The Tumblr blog can be hard to follow, so I've excerpted some relevant
entries below. 

Olivia has "come out" as a transgendered boy.
He wishes to be called Oliver, to be referred to with masculine
pronouns, and considers himself gay or pansexual in that he
prefers masculine-looking people. To clarify, so far this is mostly 
about gender identity, not sexuality.

We are all getting used to the change still.
Nothing legal, medical, or irreversible is currently planned. 
He is still "transitioning" and does not have all the answers. 
Ask him if you want to know more.

Please support Oliver as he explores his personality.
My love, concern, and support for O. are permanent and unchanged by
the flavor of the day, month, or year. My respect only grows as he
confronts new issues and challenges with courage and originality.
He has been on a remarkable journey.

Although sometimes seems diffident to our love, we are his family.
He knows he is fortunate to have so much support and love.
He has friends and acquaintances who must get by with less.
He does not take us for granted, yet he can have difficulty showing
his appreciation in warm ways. That's OK in unconditional love.
Consider the issues we never faced that he is grappling with at 15.
Thank you so much for your support and understanding.

xo,
c

Tumblr excerts:

************************************************************************
jialin.txt
Hi Jialin,

Congratulations on your progress!
You must be excited to be so close to earning your PhD.

There is a postdoctoral opportunity in my group to work on the same
projects we discussed last year. If you are interested, your main
challenge would be to convince me that you will devote yourself to
this position for two years or until you receive a faculty offer.
Based on my past experience with you, I feel the risk of your backing
out is high.

In any case please send me a copy of your thesis when ready.

Best,
Charlie

> Hi Charlie,
> How are you doing?
> I have defended the proposal in Oct. 2014
> I will defend the dissertation in March, graduate in this May.
> (the department chair, one of my committe member, said that I was one of the best students she has ever seen)
> I want to apply a postdoc or research faculty position in your group. Please let me know if there are opportunies.
> Wish I can work with you to support the NCO into a more powerful and popular toolset and eventually a large scale scientific service platform. Thanks in advance.
> (I'm currently traveling in China, wont be back untill the weekend, so the phone may or may not be reachable)

************************************************************************
cf.txt
Subject: New cell_methods operations: mabs/mibs/mebs?
Dear CF-ers,

The statistics mabs/mibs/mebs stand for "Maximum absolute value",
"Minimum absolute value", and "Mean absolute value", respectively.
They are similar to max/min/mean statistics, and they can be useful
in characterizing data when one wants positive-definite metrics.
mebs (unlike mean) does not allow positive and negative values to
compensate eachother. Unlike rms, mebs not does weight outliers
quadratically. NCO (version 4.4.8) implements mabs/mibs/mebs as
fundamental statistics (like max/min/mean/rms), and annotates the 
cell_methods attribute of variables reduced by these statistics with
the strings "maximum_absolute_value", "minimum_absolute_value", and
"mean_absolute_value".  I suggest CF adopt this, or some variant
pursuant to discussion. 

So I guess this is a request for discussion.
The relevant portions of CF are
http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/build/cf-conventions.html#cell-methods
http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/build/cf-conventions.html#appendix-cell-methods
The modifications that would be needed seem straightforward:
mention mabs/mebs/mibs in the text and then enlarge the existing
cell_methods table table by three rows. 

There appears to be an error in the draft 1.7 document. The sentence
describing Appendix E (the cell-methods appendix) says "In the Units
column, u indicates the units of the physical quantity before the
method is applied." Actually the units column entries are those valid 
_after_ the method is applied. This can be addressed independently
of the rest of the cell_methods suggestions proposed here.

Best,
Charlie
************************************************************************
cf.txt
Subject: Attributes to describe precision resulting from lossy compression?
Dear All,

The latest version (4.4.8) of NCO contains a Precision-Preserving
Compression (PPC) feature that might benefit from wider discussion
before its associated metadata are finalized. If you are interested
in precision, compression, or just procrastination, please join a
discussion on changes or improvements to the scheme I've devised. 

More documentation on PPC algorithms and performance details is at 
http://nco.sf.net/nco.html#ppc
However, I think any changes to CF would focus on definitions (of
precision) and implementation. For data that are rounded (quantized),
users want to know what that means, not necessarily how it was
performed.

The meaning of data precision, and thus what is means for data to
be "rounded" or "quantized" could be clarified in CF with something
like the text drafted below. These changes adequately represent, I
think, an existing metadata annotation for precision used in nc3tonc4
by Jeff Whitaker, which NCO has adopted (called DSD below), as well
as an annotation for a new method of quantization (called NSD below)
introduced in NCO. You will see that it boils down to adding an
attribute that indicates the type and degree of imposed precision.
A possibility that I considered before discarding was to specify the
absolute precision in units of the stored variable (rather than the
number of significant digits). There are arguments both ways... 

The suggested CF changes below are a minimal way of specifying how
data have been quantized. A more general metadata framework for
precision might include distinctions for intrinsic precision of
measurement/model (in addition to precision due to post-processing or
rounding), notations helpful for propagating errors, and how to
specify precision lost due to packing/unpacking. None of that is
in the below draft, which simply extends CF to cover precision imposed
by NSD and DSD quantization. If you want or don't want CF to recommend
attributes describing precision and/or lossy compression then please
comment...

Best,
Charlie

Current CF:
http://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/build/cf-conventions.html#packed-data
"Methods for reducing the total volume of data include both packing
and compression. Packing reduces the data volume by reducing the
precision of the stored numbers. It is implemented using the
attributes add_offset and scale_factor which are defined in the
NUG. Compression on the other hand loses no precision, but reduces the
volume by not storing missing data. The attribute compress is defined
for this purpose."

Proposed CF:
"Methods for reducing the total volume of data include packing,
rounding, and compression. Packing reduces the data volume by reducing
the range and precision of the stored numbers. It is implemented using
the attributes add_offset and scale_factor which are defined in the
NUG. Rounding preserves data values to a specified level of precision,
with no required loss in range. It is implemented using bitmasking or
other quantization techniques. Compression on the other hand loses no
precision, but reduces the volume by not storing missing data. The
attribute compress is defined for this purpose." 

...

"Packing quantizes data from a floating point representation into an
integer representation within a limited range that requires only
one-half or one-quarter of the number of floating-point bytes.
For values that occupy a limited range, typically about five orders of
magnitude, packing yields an efficient tradeoff between precision and
size because all bits are dedicated to precision, not to exponents.
A limitation of packing is that unpacking data stored as integers
into the linear range defined by scale_factor and add_offset rapidly
loses precision outside of a narrow range of floating point values. 
Variables packed as NC_SHORT, for example, can represent only about
64000 discrete values in the range -32768*scale_factor+add_offset to
32767*scale_factor+add_offset. The precision of packed data equals the
value of scale_factor, and scale_factor must be chosen to span the
range of valid data, not to represent the intrinsic or desired
precision of the values. Values that were packed and then unpacked
have lost precision, although there is no standard way of recording
this other than recording the history of the data processing. [One
solution to this would be to record the former scale_factor of
unpacked data in a precision attribute, e.g., "maximum_precision". 
Any champions for this?]

Rounding allows per-variable specification of precision in terms
of significant digits valid across the entire range of the floating
point representation. The precision specification may take one of
two forms, either the total number of significant digits (NSD), or
the number of decimal significant digits (DSD), i.e., digits following 
(positive) or preceding (negative) the decimal point. The attributes
"number_of_significant_digits" and "least_significant_digit" indicate
that the variable has been rounded to the specifed precision using NSD
or DSD definitions, respectively. The quantized values stored with
these attributes are guaranteed to be within one-half of a unit
increment in the value of the least significant digit. Consider, for
example, a true value of 1776.0704. Approximations valid to a 
precision of NSD=2 (or DSD=-2) include 1800.0 and 1750.123, both of
which are within 50 (one-half a unit increment in the hundreds digit)
of the true value.  Approximations valid to a precision of NSD=5 (or
DSD=1) include 1776.1 and 1776.03, both of which are within 0.05
(one-half a unit increment in the tenths digit) of the true value.

8.2 ...
************************************************************************
Hi Chris,

> I'm going to have a scientist supporting the SLD ACCESS project
> starting Feb. 1.  I'm assuming we're still starting with the
> regridding aspect.

Yes, as we discussed at AGU, starting here makes sense for our
new programmer.

> What would be the most useful input you could get from us?  Sample
> datasets? Something else?

Sample datasets would be a good start.
Pick a filetype or two that DISC processes all the time.
Send us a few sample netCDF/HDF files with real data.

We'll use these to figure out how best to specify the input output
grid specifications in a manner that works with our regridding
library. 

c
************************************************************************
the best way that you can help with lsd now is to extend
the option parsing so it is more flexible
allow mixtures of 
'/' preceding 
variable names in the root group, 

************************************************************************
Hi Linda,

Getting back to you about the insurance now I know more details.
For the Lexus, RIP, my total annual premium was $346, composed of
$304 for Liability and $42 for Uninsured motorist.
Those were the good ole' days :)
My rates have risen because I lost my good driver deduction (20%) for 
three years, I have a more valuable car, and I added collision
and comprehensive insurance for damage to my new car.

My new Subaru total annual premium is $1294:
$335 liability (bodily and property)
$754 physical damage (collision and comprehensive)
$ 49 uninsured motorist.
It's the first time I've ever been at fault in an insured accident,
and I was expecting my rates to rise but not to skyrocket!

I paid the whole year in insurance so I'm set but not too proud
to accept help. Anything you want to contribute to the collision would
be appreciated. This near miss reminded me how much our lives depend
on our cars, so it makes sense that you care about this.

So far my new Subaru has been great. I feel much safer in it, because
I splurged and bought all assistive driver technology safety options.
And I'm parking it in my garage. Never done that before because I
never had a nice car. My Subaru still smells new :)

It was good to see you and Teresa at Xmas. Your pies were devoured!
So glad that you joined the march and craziness.
Great way to end 2014, a very tough year.

xo
c
************************************************************************
http://www.grad.uci.edu/funding/fellowships-awards/recruitment-fellowships
************************************************************************
Hi Morgan,
I would like to repeat my Fall courses next Fall:
ESS 200 Global Physical Climatology
ESS 110 Environmental Controversies
Thanks!
Charlie
************************************************************************
netcdf.txt
ncks -O -4 -v three_dmn_rec_var,four_dmn_rec_var --cnk_min=0 --cnk_map=rew --cnk_plc=all  ~/nco/data/in.nc ~/foo.nc
ncks --cdl --hdn -m ~/foo.nc

Hi Aidan,

NCO version 4.4.7 (and later) implement the Rew chunking algorithm:

ncks --cnk_plc=all --cnk_map=rew in.nc out.nc

http://nco.sf.net/nco.html#cnk
The C-code is in nco_cnk.c.

NCO only utilizes this algorithm for 3D variables.
4D variables in the same file will not be chunked by this algorithm,
though NCO will preserve their chunking from a previous algorithm.
We're open to suggestions on how to make this more usuable.

Followups on NCO-related implementation to NCO list please:
http://sf.net/p/nco/discussion/9829

Best,
cz
************************************************************************
Hi Craig,

Welcome back to the outside world my friend.

I can only imagine some of the highs you must be feeling...
Are you still going through your "when I get out" list?
Real food, wilderness, home-cooked food, and hopefully kissing etc.
And, of course your family might be glad to see you :)

You served so much time! Unimaginable.
Hope you feel wiser and more loving than ever.
And are making the best of it all.

I understand from Curtis you're in a halfway house for some months,
unless/until you can get a (full-time?) job.
"They say" the transition can be hard. We're all rooting for you.

Best,
Charlie
************************************************************************
Hi SF,

I am the admin for the nco project nco.sf.net.
I am trying to add CVS write access to the permissions of a new developer.
Instructions say: 

"Login as an administrator and go to the Project Summary page.
From the Admin menu pulldown, select the Members option.
Click on the name of the desired developer."

There is no "Members option" in my Admin menu pulldown.
How do I do it?

Thanks,
Charlie
************************************************************************
Hello Tulsi,

I'm a volunteer alumni interviewer for the Harvard Admissions Office.
This is to confirm our rendezvous for an informal interview.
We'll meet at my office at UCI: 3323 Croul Hall at 4:00 PM Tue 1/13.
Interviews typically last about 45 minutes.
Please bring your completed interview form to help spark
the conversation (interviewers do not see your Harvard application).
If you have questions or your plans change, please call anytime.

See you soon,
Charlie
************************************************************************
Hi Ana,

> Please let me know if you would like to use your startup funds
> to cover the donation and reception costs.

Yes please.

> Additionally, given that the AGU meeting started on 12/15/14, I need
> to explain on your travel voucher why travel reimbursement is
> requested on 12/12 and 12/13. Could you please let me know if you
> were on University Business on 12/12 and 12/13 and if it was related
> to AGU? 

Yes, I traveled to SF on 12/12 to confer with another scientist 
(my former postdoc Dr. Xianwei Wang who now works in China) on 12/13
and 12/14 about my NASA project and the topic of my AGU poster.
We met during the weekend because there was not enough time during
AGU. It was university business directly related to the science
presented at AGU and funded by the same grant. 

Thanks!
Charlie
************************************************************************
Dear Dean,

As you know I have called-in to some of the Workflow and Diagnostics
telecons, yet I am not "a regular". Sometimes the reason for not
participating more regularly is/was travel or teaching conflicts.
Also DOE has not funded my project yet (!), so I am still unsure of my 
ACME status. I look forward to participating more regularly and
effectively once funded.

Best,
Charlie
************************************************************************
cat > ~/lynnes.nco << 'EOF'
defdim("Lat", 180);
GLDAS_NOAH10_M_020_swe_tmp[$time,$Lat,$lon]=GLDAS_NOAH10_M_020_swe.get_miss();
GLDAS_NOAH10_M_020_swe_tmp.set_miss(GLDAS_NOAH10_M_020_swe.get_miss());
z = GLDAS_NOAH10_M_020_swe;
res_lat = 1;
res_lon = 1;
GLDAS_NOAH10_M_020_swe_tmp(:,30:179:1,0:359:1) = z(:,:,:);
Lat[$Lat] = 0.;
Lat(30:179:) = lat(:);
for (j=29; j>=0; j--)
Lat(j) = Lat(j+1) - res_lat;
for (j=0; j<0; j++)
Lat(j+180) = Lat(j+179) + res_lat;
for (i=-1; i>=0; i--)
lon(i) = lon(i+1) - res_lon;
for (i=0; i<0; i++)
lon(i+360) = lon(i+359) + res_lon;
Lat@units="degrees_north";
Lat@standard_name="latitude";
lon@units="degrees_east";
lon@standard_name="longitude";
GLDAS_NOAH10_M_020_swe = GLDAS_NOAH10_M_020_swe_tmp;
EOF
ncap2 -O -S ~/lynnes.nco ~/pad_bug_in_no_pck.nc ~/foo.nc
************************************************************************
Here are the expenses I've accumulated in the last few months:
$ 28 Olivia UHS pictures
  32 Ruby room parent contribution
 102 Ruby lunch fund 10/17
 143 Ruby ACE fashion 10/22
  36 Ruby VV yearbook
 102 Ruby lunch fund 11/19
----
 443

Half of that is 221.50.

Also, I paid you for my health insurance for the whole year, 
but you only got charged for it through 7/31. After that I
paid for my health plan myself. The overpayment was $70, 
computed as the difference between your new and old health
payments for five months:
5.0*(family-(self+kids))=5.0*(35.21-21.20)=~$70

So the total total is 221.50+70=$291.50

Thanks!
************************************************************************
Meow meow meow. 
My person is traveling to San Francisco on Fri 12/12, and returning Thu 12/18.
This is for a meeting (AGU), then you all will go to SF the next week :)
Please give me food/water and cleanliness from Sat 12/13 until Wed 12/17. 
Who will care for me while you're all in the Bay Area from Mon 12/22 until 12/28?
And please confirm you got this message.
Purr purr purr.

Hey Barry. Thanks for the invite. I can't make any of those days :(
Have become a little sick again, but will brute-force my teaching
Tue/Thu, before traveling to SF Fri-Thu. 
I want to hang out and help you move for a few hours before holidays.
Is tomorrow night possible?

************************************************************************
nco: md5
Pedro,
I wonder whether MD5 functionality in NCO is broken.
Many operators use the nco_cpy_fix_var_trv() call, and this function
does not receive the value of MD5 the user may pass into main().
Other operators use nco_prc_cmn(). Same thing. MD5 is not passed
into this routine.

************************************************************************
Thank you all for putting together such informative presentations.
I hope the activity increased your understanding of the issues that
face corporations trying to stay economically and environmentally
competitive.

The People's Choice votes have been tallied and there is a tie
between the Moving Forward and Plastics/JetFuel proposals!
The executive judges (me, myself, and I) were more persuaded by the 
Hydrogen/Algae proposal. So let there be snacks (pending
availability) for all on Thursday.
cz

************************************************************************
Hello Gerry,

Please post any followups, along with the original question, to
Sourceforge not to my email. 

Estimate the time/size tradeoffs by trying a few commands:

ls -l /data/zender/hdf/wrfout_v2_Lambert.nc
time ncks -O -4 -L 1 wrfout_v2_Lambert.nc ~/foo.nc
ls -l ~/foo.nc
time ncpdq -O wrfout_v2_Lambert.nc ~/foo.nc
ls -l ~/foo.nc
time ncpdq -O --pck_map=hgh_byt wrfout_v2_Lambert.nc ~/foo.nc
ls -l ~/foo.nc
time ncpdq -O -4 --pck_map=hgh_byt wrfout_v2_Lambert.nc ~/foo.nc
ls -l ~/foo.nc
time ncpdq -O -4 -L 1 --pck_map=hgh_byt wrfout_v2_Lambert.nc ~/foo.nc
ls -l ~/foo.nc

Not sure if clearing disk cache first will affect these numbers.

Best,
cz

On 12/01/2014 09:57 AM, Gerry Creager - NOAA Affiliate wrote:> Charlie
> 
> Need a little advice, and since you're the real Guru for NCO, and I 
> believe that's the right tool for our job, I wanted to bug you...
> 
> We've a relatively small high performance disk system for our relatively 
> small HPC (Cray XT-6) system. I'm installing another 250 TB of somewhat 
> slower storage in a month or so, and I'm planning to compress and tier 
> our storage. Our specific application is WRF, and we want to reduce the 
> wrfout file size, as we're doing ensemble forecasts and large grids. 
> And, of course, no one wants to limit the number of variables written in 
> the output files.
> 
> 1. Use native NetCDF4 compression. We were expecting to see ~50% file 
> size reduction but we've not seen that so far, more on the order of ~20% 
> by back-of-envelope calculations. I'm mostly taking users' claims on 
> this, as I've not had time to test it.
> 2. Use NCO to do the compression, and keep the data at ~8bit or 16bit 
> resolution. We, probably, couldn't use those files for WRF restarts, as 
> the results would differ pretty significantly from what we'd get with 
> full-resolution data, but we could interpret the data for forecasting 
> purposes with compression to that extent.
> 
> If we use option 2, what is the likely penalty for this in terms of 
> time? Assuming file sizes of ~250 and ~440 MB each, about how long would 
> you expect compression to take?
> 
> Thanks
> Gerry

************************************************************************
cf.txt
New UDUnits "byte" and "octet" units

This may interest those who produce/analyze data and metadata
concerning information itself. It was prompted by Maarten Sneep's
suggestion that "photon" be added to UDUnits (like "molecule" was).
I support that suggestion.

A few months ago Jeff Lee and I worked to improve CF-compliance of 
Level 1 and 2 data for NASA's forthcoming ICESat2 mission.
These products archive some measurement rates in terms of Bytes
and related units, which were not in UDUnits at the time.
After some back and forth, Steve Emmerson of Unidata kindly added
added support for "octet" and "byte" in UDUnits 2.2.16, released
in June 2014 (same release that introduced "molecule").
Both "byte" and "octet" are convertible with "bit":

zender@roulee:~$ udunits2
You have: kilobytes
You want: bytes
    1 kilobytes = 1000 bytes
    x/bytes = 1000*(x/kilobytes)
You have: bytes
You want: bits
    1 bytes = 8 bits
    x/bits = 8*(x/bytes)

With these additions, instruments can archive data acquisition and
transmission rates, storage capacities, etc. with commonsense,
UDUnits-compatible units. We can annotate algorithm and storage
performance. Now UDUnits has "molecule" and "byte" and provided
someone provides a compelling rationale, they are likely to add
"photon" as well.

Charlie
************************************************************************
Subject: Proposal for standard names: radiance and irradiance as measured from satellite

Hi Maarten,

The proposal seems well-formed to me. In my opinion the names would be
improved by moving the location of measurement from the beginning to
the end, in this case (after incorporating suggestions from Jonathan
and Heiko), 

photon_irradiance_per_unit_wavelength_incoming_at_top_of_atmosphere
photon_radiance_per_unit_wavelength_outgoing_at_top_of_atmosphere

This would put the direction and measurement location after the units.
In my opinion what is measured is more fundamental than, and should
precede in the standard_name, where it is measured.
However, a glance at the radiation-related standard_names shows that
the opposite convention predominates CF, e.g.,

toa_outgoing_radiance_per_unit_wavelength
surface_net_upward_radiative_flux

Thus CF radiation units sort alphabetically by location (toa, surface)
first, rather than by what is measured/modeled. What are the reasons
for this (if it is intentional)? This question is not directed at you
or your proposal, Maarten. I am trying to better understand the
rationale.


************************************************************************
We are planning a Harry Potter-themed Thanksgiving holiday weekend. 
Do any neighbors have a complete set (DVD or Blueray) of the movies
that I can borrow or rent that weekend? Please reply off-list.
************************************************************************
If we take M's $1427 estimate for Mom, add to it the $45 for the group
picture (I paid) we get 1427+45=1472. Any other expenses to report?
If so, speak up! If not, can we say that we'll each pay 1472/4=368 
for Mom's trip? The we'll all pay Michelle 368 minus what we already
paid separately, so I'll send M 368-45=323 for Mom, plus my beatles
tickets. 
************************************************************************
miller.txt
Letters already submitted to: Columbia, NDSEG, NSF, PSU, UFlor., UMinn., UMary., WSU
paaswerd: Rvw_Mil73
Subject: Letter of Recommendation for Andrew Miller
Dear XXX,

Attached please find my letter recommending Andrew Miller to your program. 

Sincerely,
Charlie Zender
************************************************************************
Hi All,

This year both girls made Xmas lists on Google Docs.
Ruby's is typed simply, without many hyperlinks.
Olivia hyperlinked everything to the exact item she wants.
Here are both links:

https://docs.google.com/document/d/1v2qQSeYZmMvevLCLuQeis-LS6EEbSs9G9qHb4KtYZ8Y/edit?usp=sharing
https://docs.google.com/document/d/1ZufRkJ0AUObcTzKfLulxw4tVORhU7VOR5GXnHbq2gc4/edit?usp=sharing

Hope this helps you with ideas!
xo,
c
************************************************************************
Hi Rachel,

Thank you for persisting with this manuscript. 
I bet it will be a relief for it to off your desk forever :)

I noticed a typo in my initials in the responses.
I am C.S. Zender not C.Z. Zender.
Please change my initials responses accordingly.

Best,
Charlie

************************************************************************
Hi All,

A quiet Saturday morning.

Olivia seems to be settling down.
Still some she is shut-down, non-verbal, overwhelmed, reactive.
No episodes of multi-day lethargy and depression though.
She spends much of her free time writing, reading, and editing
Harry Potter related fan fiction and pictures (below).

I have begun to let Liv sleep in her own room (with door open), and 
to remain at home alone for daylight times up to many hours. 
Twice weekly we attend a great teen/parent support group.
Sessions can be tough. Our communication always improves afterwards.  
Weekly she has 1-on-1 sessions with a therapist she likes.

Some of her fan fiction is at
https://www.fanfiction.net/s/10769009/1/A-Night-Swim
She does not want her family/friends to read it and has since changed
her username and "published" more. It's fine if the other 7 billion 
people on the planet read it though. Just not you.
So don't mention to her if have you read it. 
If you somehow find her newer stories please tell me where...

Ruby has, not coincidentally, renewed her interest in Harry Potter.
She plays a lot on Pottermore website, where we all have accts.
She is reading HP+half blood prince. 
This is a big book and big step in reading for her.
Y'all might consider joining Pottermore to get "sorted" (not sordid)
and discuss HP nuances with girls next time you meet. 

I reserved a new black 2015 Subaru Outback. Very excited! ETA 12/5. 
In meantime, alternating weeks of car rental car and bike.
Planning many exciting travel/trips for holidays and 2015.
Will send holiday/travel info in separate email.

xox,
c


Holidays: You're all invited to join Shari and me and kids for
Thanksgiving feast either Weds. 11/26 or Fri. 11/28 because Robynn has
kids 11/27, when S&I will be in Yorba Linda at her parents/family
feast. you're invited there too). We'll pick the day (Weds or Fri)
based on preference of any visitors so let me know soon. 
Plenty of rooms/beds (guest room, ruby's room, fold-out couch, cot).
Last year we saw Cirque du Soleil. This year I don't yet have a
weekend plan, so suggestions welcome...

Coming to SF for romantic weekend with Shari 12/12-12/14 and AGU
meeting 12/15-12/17. And then...

Will bring girls to MV/Cup for 5-7 days over Xmas.
Will buy tix this weekend. Not yet sure who stays where:
Can Ruby stay with John/Yan? 
I like Mom's weight room floor---is that open?
(Just received Mom's message: in this case please move cat box soon)
Where is good spot for Liv?
Also happy to put whole family in RV or take-over MV guest room if
that works better. 

xox,
c
************************************************************************
Hi Pedro,

Things are better yet -X behavior still has bugs.
I added an ncks regression test for non-coordinate dimension auxiliary
coordinate variables like gds_var_ncd(gds_ncd).
Such variables behave like hs in the the wavewatch.nc file.
To make it easy, I gave gds_var_ncd the same values as gds_var.

Three examples of incorrect behavior:

# 1. Should not extract gds_ncd, lat_gds_ncd, or lon_gds_ncd
# because those variables are not associated with gds_var
ncks -O -v gds_var ~/nco/data/in.nc 

# 2. Should extract lat_gds_ncd and lon_gds_ncd because they 
# are associated with gds_var_ncd
ncks -O -v gds_var_ncd ~/nco/data/in.nc

# 3. Does not hyperslab gds_var_ncd as requested
ncks -O -X 150.,210.,-15.,45. -v gds_var_ncd ~/nco/data/in.nc

Please look into these issues.

Thanks!
Charlie
************************************************************************
bali.txt
Dear Charlie â€”

Dr. Zuzana Bic has passed on your email to me.   We are delighted to
learn that you are excited about the possibilities of bringing your
ESS undergraduates to Bali for a unique cultural immersion program â€”
one that we would be happy to co-design with you.

A program on "Sustainability and Global Changeâ€  would be a perfect
fit for what is going on in Bali â€” sustainability issus are the focus
of much of the global conversation in Bali and throughout that region.
One of our global Balinese advisors is an expert on sustainability and
tourism and we have held many seminars with students on helping them
deepen their understanding of the connection between economics of
tourism (Bali is a 85% tourism driven  economy) and environmental
sustainability.  Our Balinese advisor â€” Agung Prana â€” has been
recognized by the United Nations for his work on sustainability and
speaks around the world on these issues.  Students LOVE him. 

Lets talk about your thinking with this, experiences you would like to
capture in Bali, and other details.  Is this something youâ€™d like to
do in 2015? 2016?  All doable. 

I am away this week in Washington DC at an international education
conference but available by phone later in the week or early next
week. 

Looking forward to our conversation.

Warm regards,

Marcia
Marcia Jaffe
Founder/Chairman of the Board
Bali Institute for Global Renewal
3020 Bridgeway #194
Sausalito, CA 94965 USA
USA â€” 415-302-5905
Bali â€” +62 813 534 89726 
Inside Bali â€” 0813 53489726
Skype â€” marciajaffe
www.baliinstitute.org
************************************************************************
Hi Pedro,

Hope you are doing well.

There are some problems with auxiliary coordinates (-X) that were not
caught by the existing regression tests so I added a new test (ncks #95)
to demonstrate the problems:

First regression:
ncks -O -C -X 150.,210.,-15.,45. -v gds_var,gds_3dvar ~/nco/data/in.nc ~/foo.nc
ncks -C -H -d gds_crd,-1 -v gds_var ~/foo.nc

Here the first command simply fails. It completes, though incorrectly,
when output is printed directly to screen with

ncks -O -X 150.,210.,-15.,45. -v gds_var,gds_3dvar ~/nco/data/in.nc ~/foo.nc

It looks like hyperslabbing is performed on the associated coordinates
lat_gds and lon_gds but not on gds_var itself. It should only extract
gds_var[4] and gds_var[6].

Second regression:
I did not put in a test for this as I cannot reproduce it with a small
file. Basically this command does not hyperslab the input variable hs,
it just dumps the entire contents of hs:

ncks -O -X 9.,10.,43.,44. -v hs ~/wavewatch.nc ~/foo.nc

Moreover, it does not extract the auxiliary coordinates longitude and
latitude into the output file, meaning something is wrong with adding
to the extraction list auxiliary coordinates listed by the
"coordinates" attribute. The input file is available at 
http://glace.ess.uci.edu/tmp/wavewatch.nc

No one has touched auxiliary coordinates code since you left.
Except me and that was just to clean-up indentation etc.
I think these auxiliary coordinate issues remain from your work.
It just so happens that the regression tests never caught them.
Please try to solve these issues.

Thanks!
Charlie
************************************************************************
Zuzana Bic <zbic@uci.edu>
Dear Zuzana,

Thank you for speaking with me after the SAC lunch Thursday.
I later went online to learn more about your Public Health in Bali
program. It fired my imagination! I would love to take our ESS
undergraduates into the field one summer to study sustainability
and global climate issues. Would you mind if I contacted the 
Bali Institute for Global Renewal about possibly hosting a program
on Sustainability and Global Change (SGC) one summer? 

I began to think of how this might work. The logistics are daunting
and you said that a partner institution is key, and BIGR seems ideal. 
You proved it can work for UCI students. Bali could serve as a
wonderful laboratory for environmental issues such as palm oil
cropping, sea level rise, and ocean acidification. I would not want
an SGC program to conflict with your program (you have first dibs!), 
so before contacting BIGR I want to know if/when you have plans for
future programs there?

Happy to talk anytime instead of emailing. 949-891-2429.

Best,
Charlie
************************************************************************
Nina Fojaco Reed <nina@baliinstitute.org>
Dear Nina,

I just made acquaintance with Dr. Zuzana Bic here at UCI and was very
impressed with the summer program on public health you two organized.
I wonder if your organization, the Bali Institute for Global Renewal,
might be interested in hosting an Environmental Institute one summer? 

I'm a UCI professor in environmental sciences (ESS Department
www.ess.uci.edu) and advocate of study-abroad programs. I could
(with input from BIGR) develop an enriching program for our
undergraduates in the areas of sustainability and tropical climate.
Sustainability and global climate issues such as palm oil cropping,
sea level rise, and ocean acidification would be vividly experienced. 
Thinking of a summer (2016?) when there would be no conflict with
Zuzana's course.  

If there is any interest on your end, please let me know and feel free
to call 949 891 2429, and we can discuss further.  

Best,
Charlie
************************************************************************
squishy sand $22 on amazon
************************************************************************
Please ssh to gplogin1.ps.uci.edu or gplogin2.ps.uci.edu and type passwd to
change your initial password. Password changes take up to 5 minutes to take
effect.

The account for CATHERINE AMANDA GARCIA <catgar@uci.edu> has been created with:
    username: catgar
    initial password: IS'=+\+O(5#4
************************************************************************
Group,

Please ignore my warning on 20141029 about unreadable files after
chunking with NCO, nccopy, and h5repack. Although chunking can create
unreadable files in some circumstances, the specific problems that I
reported were false positives, i.e., I incorrectly diagnosed them.
NCO, nccopy, and h5repack work well for the examples given (below).
An unlikely coincidence of issues fooled my diagnosis with ncwa.
I'll try to be more thorough/careful before responding about potential
problems on this list. Apologies for any inconvenience this caused.

cz
************************************************************************
Dear DIWG members,

Peter created posted a summary of yesterday's Telecon:

https://wiki.earthdata.nasa.gov/display/ESDSWG/NASA+ESDS+DIWG+Telecon+2014-10-29

The last line is important:

"As a group we will begin to focus on what we can deliver in time for
the Spring ESDSWG Meeting"

Many of us work on interoperability issues within and without DIWG.
DIWG will focus more on efforts that we _as a group_ are better
positioned to make progress on than other groups. This does not lessen
the importance of participating in interop. efforts outside of DIWG.
DIWG encourages and supports those efforts!

Our diverse members see all ends of the data lifecycle: dataset
design, formatting and storage, access + distribution, analysis.
This overview gives DIWG great value as integrators. We saw yesterday 
how our work on data product design, compliance checking, and best
practices for geophysical grids in hierarchical files ties together.

Please look at the ongoing efforts in these areas and contribute your
input, comments, and assistance so that our 2014 accomplishments are
even more meaningful (an reflective of your interests) than in 2013.

Ways to help:
1. Test and give feedback on the ncdismember compliance/checker:
   https://wiki.earthdata.nasa.gov/display/ESDSWG/DIWG%3A+CF+Conventions+and+Compliance
2. Test and give feedback on Ed's compliance/checker (once made
   public), and integrate that into the compliance checking wiki
3. Contribute to the Group Structure Guidelines:
   https://wiki.earthdata.nasa.gov/display/ESDSWG/Guidelines+for+Creating+Group+Structures+in+Earth+Science+Data+Products
4. Clean-up the Wiki so it is easier to follow and extend

Finally please reserve Weds 12/3 at 2 PM for the next DIWG telecon.
We'll circulate a poll closer to that date, but that's the default.

Best,
Charlie
************************************************************************
Dear Antonio,

Forgive the delayed response. We prefer that NCO issues be posted on
Sourceforge forum as described on nco.sf.net. Thank you for doing so
later and for providing the test file. The issue you first raised on
the netCDF group list occurs not only with NCO on Windows, but also
with NCO and with nccopy on all OSs attempted. The output file
produced by the chunking allows simple reads (e.g., with ncks), but
produces an exotic menagerie of error when read by more sophisticated
applications (e.g., Java or ncwa). Since NCO and nccopy are completely
independent implementations of chunking that rely on the same library,
it's a good bet that there is a problem with the library. The
information below will help Unidata investigate this and hopefully
produce a fix that works with both NCO and nccopy.

Best,
Charlie

These two commands produce files that crash when ingested by ncwa 
(all OSs) and, according to the OP, lib Java on Windows. 

ncks -4 --cnk_dmn latitude,4 --cnk_dmn longitude,4 \
     	   --cnk_dmn time,512 ~/2014.nc ~/2014_ch.nc
nccopy -c time/512,latitude/4,longitude/4 ~/2014.nc \
       	   ~/2014_ch_nccopy.nc 

zender@roulee:~$ ncwa -O ~/2014_ch.nc ~/foo.nc
Bus error

zender@roulee:~$ ncwa -O ~/2014_ch_nccopy.nc ~/foo.nc
*** Error in `HDF5-DIAG: Error detected in HDF5 (1.8.13) thread 0:
  #000: H5Dio.c line 149 in H5Sselect_hyperslab(): not a data space
HDF5-DIAG: Error detected in HDF5 (1.8.13) Aborted

Input file is available at

http://glace.ess.uci.edu/tmp/2014.nc

************************************************************************
Hi Chris,

Meant to respond to this on Friday but suffered a major car wreck
Thursday night. No injuries except my ego. Renting a car today...

Your description of Features 1 and 2 is clear and and doable.
My idea of these features differs only in minor ways:
************************************************************************
lynnes.txt
How does this look for starters?

Feature #1:  Spatial Subsetting of Swath-Like Data

Swath-Like coordinate systems are a variety of irregular grids.  In this case, the data arrays are typically laid out on coordinates relative to the sensing geometry and orbit. For instance, the AIRS Level 2 data "footprints" (e.g., http://airsl2.gesdisc.eosdis.nasa.gov/opendap/Aqua_AIRS_Level2/AIRX2RET.006/2003/002/AIRS.2003.01.02.003.L2.RetStd.v6.0.7.0.G13185225558.hdf) are laid out on GeoTrack and GeoXTrack arrays (along-track and cross-track). In addition, the file includes Latitude and Longitude arrays, also laid out on the GeoTrack and GeoXTrack, so that each footprint has a corresponding Latitude and Longitude.

Spatial subsetting is specified on the command line similarly to regularly gridded data, i.e., -d Latitude,south,north -d Longitude,west,east.  However, the NCO program handles these somewhat differently:
1 - Instead of looking for dimensions lamed Latitude and Longitude, the program looks for variables Latitude and Longitude whose coordinates are in common with the data variable's coordinates.

*** The NCO algorithm will look for Lat and Lon variables in any case as
    it does now. Should the variables be 2-D then NCO will invoke the
    new SLD algorithms for bounding boxes. 1-D coordinate variables
    will be treated with the default algorithms. ***

2 - In order to retain the original data array structures, albeit reduced, the program masks out (sets to _FillValue) the data values outside the latitude/longitude rectangle and crops the data array to the minimum bounding rectangle specified by the latitude and longitude bounds.
As with current regular grid support, if the variables are not specified, all variables that share the same coordinates as the Latitude and Longitude arrays will be subsetted.

*** Yes. FYI NCO uses "subset" to mean cull variables, while "hyperslab" means cull points ***

In addition, a new way of specifying subsets is also supported, --point-radius Latitude,Longitude,point-lat,point-lon,radius

Charlie:  this last spec. is a bit clumsy, but may be the most used use case for subsetting level 2 data (sometimes called cookies); I'm open to more intuitive or common ways to specify the command line arguments.

*** This cookie switch seems fine as is ***

Feature #2:  Regridding Swath-Like Data

Regridding swath-like data into regular projections can be done in many different ways. The key factors that go into a regridding algorithm are:
(1) When multiple values fall in a grid cell, what algorithm is used to compute the grid cell value?
(2) How are data quality aspects taken into account?
(3) How are the spatial point spread functions of the data taken into account?
Much of the above can be accommodated as a weighted aggregation function. For instance, take a dataset like surface temperature represented on a 2-D array, across-track and along-track. Let us say it also has a quality-flag array on the same coordinates with quality values 0=bad, 1=OK, 2=good.  A common approach is to average the values that fall into the regridded grid cell, but to mask out any whose corresponding quality_flag = 0. Another common approach is to compute a weighted average using the quality_flag as the weighting function. Yet another approach is to also consider how much of a data value's point-spread function lies within a cell in computing the weighting.  Thus, all of these methods are conceptually similar to those used in ncwa to compute masked or weighted averages, the weightings being derived from companion arrays with either the same dimensions, or in some cases a reduced set of dimensions, broadcast across the remaining ones.  Thus, we use the same calling arguments from ncwa to specify the masking or weighting arrays to be used in these array calculations.

However, the aggregation function can also vary considerably:
1) average (the most common, esp. with weighting)
2) geometric mean
3) select maximum or minimum value in cell
4) select value with maximum or minimum value in companion array (e.g., the data value of the footprint with the smallest Satellite Zenith Angle)

*** When faced with multiple choices of algorithms like this, my sense
    it to implement one (e.g., averaging) as the default and then add
    the others as time permits. Of course it's useful to know this in
    advance so the infrastructure is built to accomodate this
    flexibility. I woul add that regridding will/must_definitely
    include the standard options of bilinear interpolation (which
    ncap2 does now) and flux-conservative (a much requested option). ***

Best,    
Charlie    

************************************************************************
Car accident
Salvage shops:
Pick Your Part
Copart
Body Shops:
Speciality Body Works
330 Paularino Ave
Costa Mesa
714 557 0780
Jim Patopoff
jim.patopoff@specialtybw.com
Insurance:
AAA CDW wth $500 ddc: 226
AAA CDW wth $100 ddc: 270
Lexus value:
BB private party b4 accident $2647
9282. car: AAA CSR Kip Corley <corley.kip@aaa-calif.com> 714-435-8416, Claim # 011680975
20150112 Dana Barnhart 562-377-5225
I lost 1 point for accident and 1 point for bodily injury
"Good driver discount" (GDD) is 20.0%
GDD is retained  after losing 1 point, forfeited after losing 2 points
Remainder of $450 increase is due to activity surcharge
Activity surcharge ends and GDD reinstatined after 3 years
************************************************************************
Dear Zach,

Thanks for your interest in our research.
I am always looking for talented, motivated, independent students.
I do wish and expect to hire one student next year, although the
specific area depends on the quality of the applications.
Your CV shows you have a strong background in programming, analysis,
and forecasting, so I urge you to apply and, before you do, to look at
the interests of other ESS professors in case I am unable to offer you
a position. 

My group specializes in computer simulations of climate so my students
should, in addition to having competence in atmospheric physics,
have strong programming skills before arriving. It is most
advantageous to be comfortable working in a C/Fortran/Linux
environment before you arrive. Please describe your relevant skills on 
the UCI application.
Applications are due January 15, 2015 for entrance in Sept., 2015.
Please see the ESS Graduate research page for more information.

Good luck,
Charlie 
************************************************************************
Hi All,

I have a $100 credit on United that expires (purchase tix by) 11/3.
Anyone booking a ticket on UA before then is welcome to it.
Electronic Travel Certificate (ETC): 9Y5GA11UU4
Put offer code 13TCVA9Y5GA11UU4 in the box on the United.com home page
when you are ready to shop for your flight. 
You will be prompted for my last name.

c
************************************************************************
Dear Steve and Curt,

Just learned today that my AGU abstract, intended for your session,
was submitted incompletely back in August. Apparently I never pressed
a "Confirm Submission" button at the end of the process. Of course AGU 
still charged me for submitting the abstract and sent me a receipt :)
Although I thought it had been submitted, you probably never saw it. 

Charlie
************************************************************************
Hi Peter, Steve, and Dan,

Checklist for interoperability datasets:
Guidelines for Groups?

HDF5WG Executive Summary (Charlie).
Status of MABEL/ICESat2 structure (Jeff Lee).
DIWG software at Innovations Lab/Earth-data Code Collaborative?
Other status reports (see action items below from last telecon)?
New CF checker (Ed Armstrong, 20 to 30 minutes).
************************************************************************
Hello AK Service,

Just spoke to Lawrence about my Hoist v5 gym repairs.
Serial #: 05-03-032718.

Charlie Zender
3 Whistler Ct.
Irvine, CA 92617
Phone: (949) 891-2429

Attached pictures show:

1. Wide shot of gym with twisted vertical cables
   hoist_twisted_cables.jpg
2. Serial number
   hoist_serial_number.jpg
3. "leg curl" (my words) portion of the gym. This is where the
   broken/split pulley is. The cable with shot housing is the
   one the runs out to here.
   hoist_leg_curl_wideshot.jpg
4. The pulley that is split/broken. Pulley is 3-4 inches diameter.
   It seemed to split because the housing of the cables wore-off.
   The exposed steel braided cable then dug through the pulley over
   time, eventually splitting it.
   hoist_leg_curl_split_pulley.jpg
5. Cable described above with housing that has worn-off/torn-off
   hoist_leg_curl_shot_cable_housing.jpg

Please send me an estimate for repairing this gym.
And answer these questions if possible:
   What caused the cable-housing to tear-off?
   Why are the vertical cables twisted?
I/we do not want to see these problems recur after repair.

Thanks!
Charlie
************************************************************************
ncgen fails to create netCDF4-classic output of in.cdl

Hi,

ncgen cannot generate a netCDF4-classic output from our standard
testfile in.cdl (attached). In summary:

ncgen -k hdf5-nc3 -b -o in_4c.nc in.cdl # broken
ncgen -b -o in.nc in.cdl # works
ncks -O -7 in.nc in_4c.nc # works

Wondering if this is an ncgen bug or am I doing something wrong?

Thanks!
c

zender@roulee:~$ /usr/local/bin/ncgen -k hdf5-nc3 -b -o ${HOME}/nco/data/in_4c.nc ${HOME}/nco/data/in.cdl
ncgen: NetCDF: Attempting netcdf-4 operation on strict nc3 netcdf-4 file
        (genbin.c:502)
zender@roulee:~$ /usr/local/bin/ncgen -b -o ${HOME}/nco/data/in.nc ${HOME}/nco/data/in.cdl
zender@roulee:~$ ncks -O -7 ${HOME}/nco/data/in.nc ${HOME}/nco/data/in_4c.nc
zender@roulee:~$ 

************************************************************************
Hi Lizzie,

On 10/09/2014 09:10 AM, Elisabeth Sperling wrote:
> So great to hear from you, too!
> 
> Glad youâ€™re getting some decompression time, and that O is seeming less anxious. Iâ€™m imagining the strain of guarding her from herself 24/7. Wondering if youâ€™re sleeping ok and taking care of yourself. Wondering what itâ€™s like to be her, and how she can find her feet more securely, and what insights Robin might have about it all. Wish we could hang out and I could be more of a listener for you. Maybe another Skype sometime? So glad Iâ€™ll see you in April.
> 
> Tomorrow Iâ€™m going to Albuquerque to visit with Eva and Kathleen againâ€¦ this time the excuse is Kathleenâ€™s 50th, which happened in Augustâ€¦ plus the hot air balloon festival. Wish I could just drive from there for a visit with you!
> 
> My Spring Break is in March, so Iâ€™ll be here in April and working. Still feeling shaky about John and me. Pretty sure you can stay with me in any case, but on the cautious side, probably would be wise to make back-up, cancelable reservations. My freshman year roommate, Preeta, has been staying with me for the past couple of months, and itâ€™s been wonderful.
> 
> Hereâ€™s the schedule for The King and I:
> 
> http://www.lct.org/shows/king-and-i/schedule/
> 
> Itâ€™s great that youâ€™ll be able to come to previews right before the official opening. I can bring you all backstage to meet the lead actress, whoâ€™s amazing, basically queen of Broadway right now, and likely we can walk around the set. Letâ€™s look into prices when they go on sale (Oct. 19th) and Iâ€™ll see if I can get you a discount.
> 
> And letâ€™s plan an artistic tour of NYC! Tell me again, is O interested in all kinds of art, or mostly performing arts?
> 
> I recently made friends with this person, Heather Willems:
> 
> http://www.imagethink.net (watch the video)
> 
> Could be fun to visit her office.
> 
> I recommend seeing Wicked, too, if O hasnâ€™t seen it.
> 
> And we should definitely go see Mitsu in his loft! Maybe heâ€™ll have one of his art exhibitions while youâ€™re here! Maybe we could also see Justine, Tony and their teenage daughter.
> 
> Iâ€™ll think some more. So exciting!
> 
> xo L.
************************************************************************
Hi Peter, Steve, and Dan,

Let's plan for the next the DIWG telecon.
We can discuss progress or problems on action items from the last
meeting, that include

HDF5WG Exec. Summary (Charlie)
Status of MABEL/ICESat2 structure (Jeff Lee)
DIWG software at Innovations Lab/Earth-data Code Collaborative?
Other status reports (see action items from last telecon below)?

And we can reserve 20-30 minutes for

New CF checker (Ed Armstrong)

Any other items you can think of?
Would you please circulate a doodle poll to find the best time?
Let's aim for the week of 10/27-10/31.

Thanks!
c

New Action Items from 20140627 Telecon:
Charlie Zender will send an update to Steve Olding regarding how we are going to proceed with the 2013 HDF5WG Recommendations.
Siri Jodha Khalsa will add text to the wiki to clarify our 2013 HDF5WG Recommendation regarding the ISO Conventions.
Mary Jo Brodzik and Charlie Zender will continue with the clean up of the wiki.
Peter Leonard will create a wiki page for the HDF-EOS Sub-Group at the same level as the Outcomes.
Peter Leonard will continue to investigate Conventions for Group Structures.
Charlie Zender and Jeff Lee will work together to improve the results from running the ncdismember CF checker on the MABEL products.
Charlie Zender will provide the ncdismember CF checker to Chris Lynnes so that this compliance checker can be included in the Innovations Lab.
Stephen Berrick will provide a description of the ECC (Earth-data Code
Collaborative) to the DIWG.

************************************************************************
HDF5WG Exec. Summary
Ed speak on new CF checker
CF-2.0 ideas
Jeff Lee
************************************************************************
In other news this week...our relationship is now recognized in 96 destinations in 41 states, DC, Puerto Rico, and the near international countries. And now Facebook, which makes it real :) Where should we go?
************************************************************************
Hello Unidaters, 

A user has pointed out a serious problem with ncrename.
As best as I can tell, this traces back to a netCDF4 library bug.
Please investigate and let me know if this is true or not.

The bug apparently affects (at least) versions 4.3.1--4.3.2 of the
netCDF4 library (and snapshots at least until October, 2014).
This bug corrupts values contained in netCDF4 arrays (not scalars)
that are renamed or that utilize dimensions that are renamed. 
In other words, renaming variables and dimensions succeeds
yet it corrupts the values contained by the affected array variables.
This bug corrupts affected variables by replacing their values with the 
default _FillValue for that variable's type.
Attached is a file whose values are corrupted when either the
dimension or the variables or both are renamed.

ncks -O -4 -v lat -C -h -m -M ~/nco/data/in.nc ~/bug.nc # Create test file
ncrename -O -v lat,tal ~/bug.nc ~/foo.nc # Broken
ncrename -O -d lat,tal ~/bug.nc ~/foo.nc # Broken
ncrename -O -d lat,tal -v lat,tal ~/bug.nc ~/foo.nc # Broken
ncks --cdl ~/foo.nc

The only known workaround is to convert the file to netCDF3 first,
then rename as intended, then convert back. I checked renaming of
groups and attributes in netCDF4 files and these operations both
perform as intended. So the problem appears to be only with with 
renaming dimensions and renaming array variables in netCDF4 files.

Thanks!
Charlie
************************************************************************
Dear Steve,

My attempts to submit metrics at these three sites all failed
for reasons explained in 

http://earthdata.nasa.gov/mct
https://esds.reisys.com/ebooks/ebooks/index.jsp ("Go to Metrics")
http://community.eosdis.nasa.gov/metrics
************************************************************************
Dear Mike,
Thank you for the invitation to participate in this panel.
Unfortunately I am already overcommitted for this week.
Fall is my heaviest teaching quarter and is also full of personnel
actions and tenure cases that I am reviewing. 
In short I cannot adequately review these ACMAP proposals.

************************************************************************
Hello Unidata folks,

Today's ncgen 4.3.3-rc2 works fine on the attached file in_grp.cdl.
However, it hangs on the attached file in_grp_bug.cdl.
The only difference between the two is the variable lat_time.
I think lat_time is specified correctly---is it?
If so, then is there a problem with ncgen?

Thanks,
Charlie
************************************************************************
Dear Peter, Siri-Jodha, and (T)ed:

We (DIWG) are close to finishing the Executive Summary of our 2013
recommendations. The remaining short tasks require expertise I lack.
Please visit 

https://wiki.earthdata.nasa.gov/display/ESDSWG/2013+HDF5WG+Recommendations+Executive+Summary

Recommendation #5 needs an executive summary. Siri-Jodha, I believe 
you can do this.

Recommendation #6 needs an executive summary. Ed or Ted, I believe you
can do this. 

 are only three more
things to do. I would like each of you to be responsible for on.e

************************************************************************
Hi Henry and Pedro,

In July a user named Parker Norton posted that NCO was so slow
as to be useless when hyperslabbing netCDF4 files with stride.
I have verified his report but am unsure what the problem is.
The problem can be seen when hyperslabbing this netCDF4 file:

ncks -O -d time,0 ~/ET_2000-01_2001-12.nc ~/foo.nc # works

If we keep everything the same but use stride it breaks:

ncks -O -d time,0,0,12 ~/ET_2000-01_2001-12.nc ~/foo.nc # broken

You can download the 645 MB test file from 
http://glace.ess.uci.edu/tmp/ET_2000-01_2001-12.nc
I have been unable to create a test-case with a smaller file.
I've verified that if one converts the input to netCDF3 first,
and then hyperslabs with the stride argument, everything works.

Note that the above file is only 12 timesteps long, so the 
slowdown occurs whether or not there are any additional strides
to extract. All that is needed to trigger the problem is sending
_any_ non-unity value of stride to the hyperslabber

ncks -O -d time,0,0,1  ~/ET_2000-01_2001-12.nc ~/foo.nc # works
ncks -O -d time,0,0,12 ~/ET_2000-01_2001-12.nc ~/foo.nc # broken
ncks -O -d time,0,0,2  ~/ET_2000-01_2001-12.nc ~/foo.nc # broken
ncks -O -d time,0,,2   ~/ET_2000-01_2001-12.nc ~/foo.nc # broken

I usually track-down performance problems/bugs myself and then if the 
cause appears to be in your code I let you know. In this case I'm 
unsure whether this is an NCO or a netCDF problem, and I need your 
help to decide. Possibilities include
     1. NCO's MSA is poorly designed for stride requests on large
     files. Maybe it calls nc_get_vars() more times than necessary
     (only one call per variable is necessary for this operation). 
     2. netCDF's nc_get_vars() interacts poorly with netCDF4 chunked 
     variables and no one else has noticed this before.
     3. Something else.

Please take a look and tell me what you think. 

Charlie
*************************************************************************
davis.txt
Hi Steve,
I've become interested in the subject of climate change equity, by
which I mean, roughly, what relationships exist between projected
climate change and income? To investigate this quantitatively, I
need gridded datasets of (current and projected) climate change
(which I have from CMIP), population density, and income density.
What datasets do you recommend for population and income density?

I have Gridded Population of the World (GPWv3) density available from
http://sedac.ciesin.columbia.edu/data/set/gpw-v3-population-count-future-estimates/data-download
And I have income density from

Before I proceed further with this, I think I should doublecheck if
there are more reliable/convenient sources for these datasets.

Thanks,
Charlie
*************************************************************************
climate change gini (ccg) ppr:
temperature-increase weighted by population density (continuous)
temperature-increase weighted by average income (per country)
temperature-increase weighted by income per person per km2 (continuous)
ccg index = 0.0 when temperature increase equals global mean (land)
gridded population of the world (gpw) density available in ascii format
http://sedac.ciesin.columbia.edu/data/set/gpw-v3-population-count-future-estimates/data-download
Convert ESRI Arc/Info ASCII grid to netCDF:
sudo yum install gdal # GDAL includes gdal_translate

# Population density
gdal_translate -of netCDF ${DATA}/ccg/glds00ag60.asc ${DATA}/ccg/glds00ag60.nc
ncrename -v Band1,dns_ppn ${DATA}/ccg/glds00ag60.nc
ncatted -a long_name,dns_ppn,o,c,'Population Density' -a units,dns_ppn,c,c,'people kilometer-2' ${DATA}/ccg/glds00ag60.nc
readme_sng=`cat ${DATA}/ccg/readme.glds00ag60`
ncatted -h -a README,global,o,c,"${readme_sng}" ${DATA}/ccg/glds00ag60.nc
ncap2 -O -s 'lat_dlt_dgr=1.0;lon_dlt_dgr=1.0;rds_rth=6.37e3;pi=4.0d*atan(1.0d);lat_dlt_rdn=lat_dlt_dgr*pi/180.0;lon_dlt_rdn=lon_dlt_dgr*pi/180.0;lat_rdn=lat*pi/180.0;lon_rdn=lon*pi/180.0;area=rds_rth*rds_rth*lat_dlt_rdn*lon_dlt_rdn*cos(lat_rdn);ppn=dns_ppn*area;ppn_ttl=ppn.total();lat_dlt_dgr@units="degree";lon_dlt_dgr@units="degree";rds_rth@units="kilometer";lat_dlt_rdn@units="radian";lon_dlt_rdn@units="radian";lat_rdn@units="radian";lon_rdn@units="radian";area@long_name="Cell area";area@units="kilometer2";ppn@long_name="Population per gridcell";ppn@units="people gridcell-1";ppn_ttl@long_name="Total Population";ppn_ttl@units="people"' ${DATA}/ccg/glds00ag60.nc ${DATA}/ccg/glds00ag60.nc
ncks -A -C -d lat,-57.5, -v lnd_frc ~/dst_1x1_m11.nc ${DATA}/ccg/glds00ag60.nc
ncks -m -M --cdl ${DATA}/ccg/glds00ag60.nc
ncview ${DATA}/ccg/glds00ag60.nc

# GDP per density
gdal_translate -of netCDF ${DATA}/ccg/gdp25_15mi.ascii ${DATA}/ccg/gdp25_15mi.nc
readme_sng=`cat ${DATA}/ccg/readme.gdp90`
ncatted -h -a README,global,o,c,"${readme_sng}" ${DATA}/ccg/gdp25_15mi.nc
ncks -m -M --cdl ${DATA}/ccg/gdp25_15mi.nc
ncview ${DATA}/ccg/gdp25_15mi.nc

# Get land fraction and grid info from BDS
bds -x 360 -y 180 -m 11 -o ~/dst_1x1_m11.nc

ppr_LaM13: Global Income Distribution: From the Fall of the Berlin Wall to the Great Recession
           Christoph Lakner Branko Milanovic
*************************************************************************
decade.txt fiona lee
Equity, fairness, and impartiality are cornerstones of academia.
Diversity increases the value and vigor of our scholarship.
*************************************************************************
Dear Barry and Petra,

This is late in coming and seems out-of-date already.
I fell behind last week, then my mom spent a few days here.
She is one needy person.

Your family is going through a difficult, existential time. 
Let me know if there's anything I can do to help.
However clichÃ© it sounds, I mean it.

My house is close to yours and I have a spare bedroom that
you are both welcome to use when my Olivia isn't here.
Girls are here every other week, i.e., 9/1-9/7, 9/15-9/21...
Ruby and I would welcome your company any day/evening.
But Olivia is fragile so I can only offer the guest room
overnight when she's at Robynn's.

Shit, you two will figure what to do and maybe you'll thrive.  
You seem indomitable yet relationships can be exhausting.
Know that you are welcome to rest, hide, vent, and/or drink
here anytime.

Shootin' love to you both,
Charlie
************************************************************************
#if (__GNUC__ * 100 + __GNUC_MINOR__ * 10 + __GNUC_PATCHLEVEL__ ) < 442
*************************************************************************
Hi Pedro,

Thank you for helping to fix this. 
Please address the remaining issues (like long dimension names).

There are more bugs that I have not yet mentioned to you.
Especially with the stride argument (pointed out by Parker Norton).
Once I understand them and have a simple test case I may contact you
about them soon. 

Please tell me if commands work with Windows volume labels like this:
ncpdq.exe -a lon,lat -O C:\path\in.nc C:\path\out.nc
ncks.exe -O C:\path\in.nc C:\path\out.nc
I put in a bugfix to enable this but I was unable to test it on Windows.

Thank you,
Charlie
*************************************************************************
Hi Pedro,

Thanks for making this fix. I have a few comments.

1. The routine nco_dfn_dmn() appears to be buggy.
It is coded to compare dimension short names not full names.
Thus a variable with two different lat dimensions, e.g., /g1/lat and
/g2/lat could be assigned the wrong sizes.

2. Why would the block you inserted after this line
/* pvn 20140824 Always redefine output dimension array */
only be executed for ncks? All data operators support chunking and
need the correct information in dmn_cmn. I changed it to
 if(nco_prg_id != ncwa){...}
and it passes all regressions.

However, looking at the preceding code, it does not appear to be quite
correct to me either. The fundamental issue is that contents of
dmn_cmn[] need to be accurate for all dimensions in each call to
nco_cpy_var_dfn_trv() so the information can be used for chunking.
Your original code does not seem to update dmn_cmn[] contents
for dimensions that were defined in a previous call to 
nco_cpy_var_dfn_trv().

Please examine current code and modify to be correct.
At a minimum, this condition 
 if(nco_prg_id != ncwa){...}
needs to change so chunking will work on ncwa with hyperslabbing
on all variables not just the first time dimensions are defined.

Thank you,
Charlie
*************************************************************************
Hi Pedro,

The array dmn_cmn[] provides crucial information on the output
dimension sizes to nco_snk_sz_set_trv(). Unfortunately, this
information is not always accurate and sometimes causes
nco_cnk_sz_set_trv() to fail. The test case is

ncks -O -D 5 -C -d lat,0 -v one,four --cnk_plc=xst --cnk_map=xst
~/nco/data/hdn.nc ~/foo.nc

The first variable encountered, four, is treated correctly.
The output size of the lat dimension in dmn_cmn[] is 1, which 
is correct due to the hyperslabbing.

The second variable encountered, one, is treated incorrectly.
The output size of the lat dimension in dmn_cmn[] is 2, which 
is incorrect due to the hyperslabbing.

Then the command fails.

Please figure out why dmn_cmn provides incorrect information
and fix it.

This is higher priority than the multiple-record dimension (MRD)
problem I emailed you about last week. That problem is subtle,
whereas the problem caused by dmn_cmn[] for chunking affects more
people. The dmn_cmn[] array is created in nco_cpy_var_dfn_trv()
and is code your wrote. I just need it to provide the right
information to my chunking routine.

Thank you,
Charlie
*************************************************************************
Ruby Bday:
Pick-up pie from Lola 8/29
Bring mango margarita makings

	Invited	Coming (Y/N/M/? = Yes, No, Maybe, No response yet)
Anika	Y	Y	
Chiara	Y	Y
Dominic	Y	M
Francesca Y	M
Luca	Y	Y
Max	Y	Y
Noah	Y	Y
Rachel	Y	N
Sandrine Y	Y

Older:
Marina	Y	?
Ashley 	Y	Y
Justin 	Y	M

Confirmed Family:
Mom, Dad, Nana, Auntie M., Shari, Olivia
*************************************************************************
Napa:
WN  155 dep SNA 16:10 Fri 10/17 arr SJC 17:20 drn 1h10m
WN 3756 dep SJC 20:55 Sun 10/19 arr SNA 22:10 drn 1h15m
20140817: $114+$95=$209 or 6446+5209=11655 pts
*************************************************************************
Subject: ncra/ncrcat (not ncks) fails on variables with multiple record dimensions
Hi Pedro,

Hope you are well. 

I am contacting you for help with an NCO problem that seems to stem
from code you wrote. I have build a test case and it should be easy 
for you to determine if I am right, and hopefully fix the problem.

With the latest snapshot try:

ncrcat -O -h -d time,0 ~/nco/data/mrd.nc ~/foo.nc
ncdump ~/foo.nc

The results on my machine are appended below.
The 1-D record variable time is treated correctly.
However, data for the multi-record dimension variable var_mrd is not
present in the output file. And the size of the second record
dimension, step, is incorrectly zero. 
This is one symptom of the bug. 
Try to fix it.

(NB: You may need to update to latest netCDF ncdump to get reliable
output on variables with multiple record dimensions (MRD). ncks does
not produce syntactically correct output on MRD variables.)

zender@roulee:~$ ncdump ~/nco/data/mrd.nc
netcdf mrd {
dimensions:
        time = UNLIMITED ; // (2 currently)
        step = UNLIMITED ; // (2 currently)
variables:
        int var_mrd(time, step) ;
        int time(time) ;
data:

 var_mrd =
  {1, 2},
  {3, 4} ;

 time = 1, 2 ;
}
zender@roulee:~$ ncrcat -O -h -d time,0 ~/nco/data/mrd.nc ~/foo.nc
zender@roulee:~$ ncdump ~/foo.nc
netcdf foo {
dimensions:
        time = UNLIMITED ; // (1 currently)
        step = UNLIMITED ; // (0 currently)
variables:
        int var_mrd(time, step) ;
        int time(time) ;
data:

 time = 1 ;
}
zender@roulee:~$ 
************************************************************************
YaZ14.txt yang song

Dear Yang Song,

Here are my latest suggestions, appended/merged to my earlier ones.
I'm up to Section 3.5 now. My apologies for the delay. I had a tough
week.

Best,
Charlie

Suggested title:

Seasonal concentration, profiles, melt-scavenging, and enrichment of
black carbon, organic carbon, and dust in a Tibetan Plateau Glacier

My overall impression is that this manuscript is a well-done summary 
of an impressive campaign of sampling and analysis. 

line 27: Small initial changes in snow albedo due to BC, in
conjunction with the rapid adjustments and feedbacks that ensue, can
cause significant climate effects 

line 30 estimate should refer to Bond et al., 2013

line 31 delete "Even worse, "

line 51 delete ", which ..." to end of paragraph, replace with
something like "Much of our knowledge of the behavior and effects of
BC concentation via melt and sublimation in snowpacks comes from
earlier studies with limited snow stratigraphy and sites."

line 58: (New paragraph) More recent snow sampling has improved our
understanding of melt-scavenging of BC. Xu 2012 and Xu 2006 sampled
snow in the TP and in the Tienshan glacier. Results demonstrated
that... 

line 61: "snow. Simultanteously, BC and OC were scavenged ..."

line 63-66: break into multiple sentences.

line 70: "melt seasons" instead of "seasons of concentration"

line 77: an English semantic issue reduces this manuscript's clarity. 
the manuscript uses the word "concentration" to mean two distinct
things, sometimes in adjacent sentences. Sometime "concentration"
means, essentially, mixing ratio, i.e., the mass concentration of BC.
Other times "concentration" means "enrichment", a process of
amplifying the mass concentration and mixing ratio because of partial
scavenging. These two meanings are related yet distinct. One is a
process and one refers to a unit of measurement (kg/m3). I suggest
that the manuscript be revised to clarify which meaning is intended. 
It might be simpler to use "mass mixing ratio" or MMR when you mean
mass concentration, taking into account that the units are different, 
kg/kg vs. kg/m3. Then use "enrichment" to refer to the increase
in mixing ratio due to partial scavenging. And omit mostly or entirely
the term "concentration" because it will confuse readers.

line 103: Figure 1 is well done

line 124: charring

line 152: was used.

line 176: for enriching BC conentration was post-deposition scavenging
in which...

line 226: Up to this point the manuscript does not seem to mention the 
role, if any, of BC dry deposition, i.e., enhanced surface BC due to 
dry deposition in clear air. What do your measurements tell you about
dry deposition?

line 258: We primarily discuss the higher elevation profiles where
samples were thicker and more robust. 

line 275: OC/BC ratios

line 290: Melt-scavenging of OC appears more sensitive...

line 292: here is an example where the manuscript says "concentration"
yet the measurements reported are mixing ratios. Please be consistent.

line 354: Under the same environmental conditions on the same ... is
rooted in hydrophilic differences, i.e., ...

line 366: eliminate "and in the model call"

line 370: "surprising" instead of "eccentric"

line 373: to satisfy our assumption of asymptotic behavior to
compute scavenging efficiency, and two ...

line 374: reminded us that Equation (1) may be difficult to apply...

line 393: OC/BC ratios

line 395: this sentence is unclear. does it mean that dust, OC, and BC
all had similar enrichments? If so, say that.

line 396: The enrichment varied with snow depth, impurity species, and
elevation.

line 399: i.e., post-deposition effects increased the BC ....

line 410: OC/BC ratios

line 412: what is LAHS?

line 416: implying that OC scavenging efficiency and BC scavenging 
efficiency both remained constant.

line 445: "Most importantly, unjustified extrapolation of BC ... could
bias BC RF estimates."

line 490: "and/or previous enrichment; afterwards, mixing ratios
gradually..."

line 500: "OC and BC mixing ratios were highly variable across pits."

*************************************************************************
Re: Auto insurance
Hi,
Now that we are divorced we no longer need to pay to list eachother
as explicitly allowed to drive eachothers' cars. This will save me
$26/yr. To make this happen you need to call AAA and request to be
removed as a driver from my auto policy CAA 064566006.
They have already heard from me that I wish you to be removed, they
just need you to confirm it. You will still be covered if you drive
the car, it just doesn't won't cost me extra now. 

The number to call is 877 422 2100. 

Thanks!
*************************************************************************
Hello,

I have been experiencing a problem when using ncks to pull out records
by time with a stride from a netCDF4 file. Whenever I try to use a
stride on the time variable (e.g. ncks -d time,0,,12 <in> <out>) the
process takes a significantly longer time to complete compared to just
pulling out all the individual timesteps and concatenating the
temporary files together. 

The details of netCDF Operators on my system are:

NCO netCDF Operators version "4.4.4" last modified 2014/05/21 built Jul 16 2014 on tibicus by pnorton
ncks version 4.4.4
Linked to netCDF library version 4.3.2, compiled Jul 16 2014 11:45:23
Copyright (C) 1995--2014 Charlie Zender
NCO is free software and comes with a BIG FAT KISS and ABOLUTELY NO WARRANTY
License: GNU General Public License (GPL) Version 3
Homepage: http://nco.sf.net
User's Guide: http://nco.sf.net/nco.html
Configuration Option:   Active? Meaning or Reference:
Check _FillValue    Yes http://nco.sf.net/nco.html#mss_val
Check missing_value No  http://nco.sf.net/nco.html#mss_val
Compressed netCDF3  No  http://nco.sf.net/nco.html#znetcdf (pre-alpha)
DAP clients (libdap)    No  http://nco.sf.net/nco.html#dap
DAP clients (libnetcdf) No  http://nco.sf.net/nco.html#dap
Debugging: Custom   No  Pedantic, bounds checking (slowest execution)
Debugging: Symbols  No  Produce symbols for debuggers (e.g., dbx, gdb)
GNU Scientific Library  Yes http://nco.sf.net/nco.html#gsl
HDF4 support        Unknown http://nco.sf.net/nco.html#hdf4
Internationalization    No  http://nco.sf.net/nco.html#i18n (pre-alpha)
MPI parallelization No  http://nco.sf.net/nco.html#mpi (beta)
netCDF3 64-bit files    Yes http://nco.sf.net/nco.html#lfs
netCDF4/HDF5 available  Yes http://nco.sf.net/nco.html#nco4
netCDF4/HDF5 enabled    Yes http://nco.sf.net/nco.html#nco4
OpenMP SMP threading    No  http://nco.sf.net/nco.html#omp
Optimization: run-time  No  Fastest execution possible (slowest compilation)
Parallel netCDF3    No  http://nco.sf.net/nco.html#pnetcdf (pre-alpha)
Regular Expressions Yes http://nco.sf.net/nco.html#rx
Shared libraries built  Yes Small, dynamically linked executables
Static libraries built  Yes Large executables with private namespaces
UDUnits conversions Yes http://nco.sf.net/nco.html#udunits
UDUnits2 conversions    Yes http://nco.sf.net/nco.html#udunits

Staycation: New records set

The netCDF file I am working with contains Modis MOD16A2 (Monthly ET
and PET values for roughly North America at a 1km grid spacing for
2000-2013). This file uses the netCDF4 format with chunking and
compression. The entire dataset is 8.4G compressed. Below is the key
information for this netCDF file. I have remove attributes not
necessary for this discussion. The various chunk sizes are defaults
used when the file was created. 

ET_1km: type NC_FLOAT, 3 dimensions, 7 attributes, chunked? yes, compressed? yes, packed? no
ET_1km compression (Lempel-Ziv without shuffling) level = 2
ET_1km size (RAM) = 13*5032*9946*sizeof(NC_FLOAT) = 650627536*4 = 2602510144 bytes
ET_1km dimension 0: time, size = 13 NC_DOUBLE, chunksize = 1 (Record coordinate is time)
ET_1km dimension 1: latitude, size = 5032 NC_DOUBLE, chunksize = 719 (Coordinate is latitude)
ET_1km dimension 2: longitude, size = 9946 NC_DOUBLE, chunksize = 1421 (Coordinate is longitude)

PET_1km: type NC_FLOAT, 3 dimensions, 7 attributes, chunked? yes, compressed? yes, packed? no
PET_1km compression (Lempel-Ziv without shuffling) level = 2
PET_1km size (RAM) = 13*5032*9946*sizeof(NC_FLOAT) = 650627536*4 = 2602510144 bytes
PET_1km dimension 0: time, size = 13 NC_DOUBLE, chunksize = 1 (Record coordinate is time)
PET_1km dimension 1: latitude, size = 5032 NC_DOUBLE, chunksize = 719 (Coordinate is latitude)
PET_1km dimension 2: longitude, size = 9946 NC_DOUBLE, chunksize = 1421 (Coordinate is longitude)

latitude: type NC_DOUBLE, 1 dimension, 7 attributes, chunked? yes, compressed? yes, packed? no
latitude compression (Lempel-Ziv without shuffling) level = 2
latitude size (RAM) = 5032*sizeof(NC_DOUBLE) = 5032*8 = 40256 bytes
latitude dimension 0: latitude, size = 5032 NC_DOUBLE, chunksize = 5032 (Coordinate is latitude)

longitude: type NC_DOUBLE, 1 dimension, 7 attributes, chunked? yes, compressed? yes, packed? no
longitude compression (Lempel-Ziv without shuffling) level = 2
longitude size (RAM) = 9946*sizeof(NC_DOUBLE) = 9946*8 = 79568 bytes
longitude dimension 0: longitude, size = 9946 NC_DOUBLE, chunksize = 9946 (Coordinate is longitude)

time: type NC_DOUBLE, 1 dimension, 4 attributes, chunked? yes, compressed? yes, packed? no
time compression (Lempel-Ziv without shuffling) level = 2
time size (RAM) = 13*sizeof(NC_DOUBLE) = 13*8 = 104 bytes
time dimension 0: time, size = 13 NC_DOUBLE, chunksize = 1 (Record coordinate is time)

One of our scientists wanted to create the long term average for each
month. I figured I could quickly pull out each unique month by
iterating over the 12 months and using ncks with a stride value
specified for the time variable. 
ncks -d time,N,,12 <in.nc> <out.nc> (where N is the starting index)

I ran ncks on the first month in the file (JAN) expecting it to
complete in fairly short order. I let it run for several hours and it
never completed. At this point I killed it and tried a couple of other
tests. In the interest of time I created a subset of the dataset for
2000-01 to 2001-01. I tried the ncks command on this file and it took
an hour to complete. This seemed like a long time so for comparison I
created a script that pulled a single month at a time from the
subsetted file using ncks (e.g. just 2000-01 and 2001-01) and then
concatenated those two files together. This script took less than a
minute. Each ncks pull of a single timestep took roughly 3-6 seconds. 

My suspicion is this might have something to do with the chunking
values that were used however I'm not sure why specifying a stride
would result in such a difference in runtime. After reading a blogpost
at UniData
(http://www.unidata.ucar.edu/blogs/developer/en/entry/chunking_data_choosing_shapes)
I decided to re-chunk my subset file using nccopy from the default of
(1,719,1421) to (1,23,44) to achieve a middle ground of efficiency for
different types of access. I again ran the ncks command on the the
re-chunked subset; this time it only took around 30 minutes. This is a
better runtime but a far cry from the speed seen when reading single
timesteps into a bunch of temporary files. 

I noticed that when I use ncks it was automatically re-chunking to a
completely different set of chunk values (shown below). 

ET_1km: type NC_FLOAT, 3 dimensions, 7 attributes, chunked? yes, compressed? yes, packed? no
ET_1km compression (Lempel-Ziv without shuffling) level = 1
ET_1km size (RAM) = 2*5032*9946*sizeof(NC_FLOAT) = 100096544*4 = 400386176 bytes
ET_1km dimension 0: time, size = 2 NC_DOUBLE, chunksize = 1 (Coordinate is time)
ET_1km dimension 1: latitude, size = 5032 NC_DOUBLE, chunksize = 1007 (Coordinate is latitude)
ET_1km dimension 2: longitude, size = 9946 NC_DOUBLE, chunksize = 1990 (Coordinate is longitude)

PET_1km: type NC_FLOAT, 3 dimensions, 7 attributes, chunked? yes, compressed? yes, packed? no
PET_1km compression (Lempel-Ziv without shuffling) level = 1
PET_1km size (RAM) = 2*5032*9946*sizeof(NC_FLOAT) = 100096544*4 = 400386176 bytes
PET_1km dimension 0: time, size = 2 NC_DOUBLE, chunksize = 1 (Coordinate is time)
PET_1km dimension 1: latitude, size = 5032 NC_DOUBLE, chunksize = 1007 (Coordinate is latitude)
PET_1km dimension 2: longitude, size = 9946 NC_DOUBLE, chunksize = 1990 (Coordinate is longitude)

latitude: type NC_DOUBLE, 1 dimension, 7 attributes, chunked? yes, compressed? yes, packed? no
latitude compression (Lempel-Ziv without shuffling) level = 1
latitude size (RAM) = 5032*sizeof(NC_DOUBLE) = 5032*8 = 40256 bytes
latitude dimension 0: latitude, size = 5032 NC_DOUBLE, chunksize = 5032 (Coordinate is latitude)

longitude: type NC_DOUBLE, 1 dimension, 7 attributes, chunked? yes, compressed? yes, packed? no
longitude compression (Lempel-Ziv without shuffling) level = 1
longitude size (RAM) = 9946*sizeof(NC_DOUBLE) = 9946*8 = 79568 bytes
longitude dimension 0: longitude, size = 9946 NC_DOUBLE, chunksize = 9946 (Coordinate is longitude)

time: type NC_DOUBLE, 1 dimension, 4 attributes, chunked? yes, compressed? yes, packed? no
time compression (Lempel-Ziv without shuffling) level = 1
time size (RAM) = 2*sizeof(NC_DOUBLE) = 2*8 = 16 bytes
time dimension 0: time, size = 2 NC_DOUBLE, chunksize = 2 (Coordinate is time)

time_bnds: type NC_DOUBLE, 2 dimensions, 1 attribute, chunked? yes, compressed? yes, packed? no
time_bnds compression (Lempel-Ziv without shuffling) level = 1
time_bnds size (RAM) = 2*2*sizeof(NC_DOUBLE) = 4*8 = 32 bytes
time_bnds dimension 0: time, size = 2 NC_DOUBLE, chunksize = 2 (Coordinate is time)
time_bnds dimension 1: tbnd, size = 2, chunksize = 2 (Non-coordinate dimension)

I didn't want the chunking changed so I re-ran the ncks command and
specified 'xst' for the chunk policy and chunk map. 
ncks -d time,N,,12 --cnk_plc=xst --cnk_map=xst <in.nc> <out.nc>

Now instead of running the program exited with an error. The debug
output is shown the below. 

ncks: INFO nco_fl_open() reports nc__open() will request file buffer of default size
ncks: INFO nco_fl_open() reports nc__open() opened file with buffer size = 0 bytes
ncks: INFO Extended filetype of tmp_ET_t0thru12_RC1.nc is NC_FORMAT_HDF5, mode = 4096
ncks: INFO nco_xtr_mk() reports following groups match sub-setting and regular expressions:
ncks: INFO nco_xtr_mk() reports following variables match sub-setting and regular expressions:
ncks: nco_fl_out_open() reports sizeof(pid_t) = 4 bytes, pid = 14910, pid_sng_lng = 6 bytes, strlen(pid_sng) = 5 bytes, fl_out_tmp_lng = 41 bytes, strlen(fl_out_tmp) = 40, fl_out_tmp = crap_RC1_stride_xst.nc.pid14910.ncks.tmp
ncks: INFO nco_fl_blocksize() reports preferred output filesystem I/O block size: 4096 bytes
ncks: INFO User requested chunking or unchunking
cnk_plc, cnk_map: xst, xst
cnk_sz_scl, cnk_sz_byt: 0, 4194304
ncks: INFO nco_cnk_sz_set_trv() ET_1km must be chunked (record, compressed, or check-summed variable)
ncks: INFO nco_cnk_sz_set_trv() re-chunking ET_1km
idx dmn_nm  dmn_sz  cnk_sz:
 0 /time    2   1
 1 /latitude    5032    23
 2 /longitude   9946    44
ncks: INFO nco_aed_prc() examining variable ET_1km
ncks: INFO nco_aed_prc() reports creating, modifying, or overwriting ET_1km attribute _FillValue in netCDF4 file requires re-name trick
ncks: INFO nco_cnk_sz_set_trv() PET_1km must be chunked (record, compressed, or check-summed variable)
ncks: INFO nco_cnk_sz_set_trv() re-chunking PET_1km
idx dmn_nm  dmn_sz  cnk_sz:
 0 /time    13  1
 1 /latitude    5032    23
 2 /longitude   9946    44
ncks: INFO nco_aed_prc() examining variable PET_1km
ncks: INFO nco_aed_prc() reports creating, modifying, or overwriting PET_1km attribute _FillValue in netCDF4 file requires re-name trick
ncks: INFO nco_cnk_sz_set_trv() latitude must be chunked (record, compressed, or check-summed variable)
ncks: INFO nco_cnk_sz_set_trv() re-chunking latitude
idx dmn_nm  dmn_sz  cnk_sz:
 0 /latitude    5032    5032
ncks: INFO nco_cnk_sz_set_trv() longitude must be chunked (record, compressed, or check-summed variable)
ncks: INFO nco_cnk_sz_set_trv() re-chunking longitude
idx dmn_nm  dmn_sz  cnk_sz:
 0 /longitude   9946    9946
ncks: INFO nco_cnk_sz_set_trv() time must be chunked (record, compressed, or check-summed variable)
ncks: INFO nco_cnk_sz_set_trv() re-chunking time
idx dmn_nm  dmn_sz  cnk_sz:
 0 /time    13  13
ncks: INFO nco_cnk_sz_set_trv() time_bnds must be chunked (record, compressed, or check-summed variable)
ncks: INFO nco_cnk_sz_set_trv() re-chunking time_bnds
idx dmn_nm  dmn_sz  cnk_sz:
 0 /time    13  13
 1 /tbnd    2   2
ncks: INFO nco_cnk_sz_set_trv() yearmonth must be chunked (record, compressed, or check-summed variable)
ncks: INFO nco_cnk_sz_set_trv() re-chunking yearmonth
idx dmn_nm  dmn_sz  cnk_sz:
 0 /time    13  13
 1 /DateStrLen  7   7
NCO version 4.4.4
cvs_nm_sng nco-4_4_4
cvs_mjr_vrs_sng 4
cvs_mnr_vrs_sng 4
cvs_pch_vrs_sng 4
cvs_mjr_vrs 4
cvs_mnr_vrs 4
cvs_pch_vrs 4
ncks: INFO nco_aed_prc() examining variable Global
nco_err_exit(): ERROR Short NCO-generated message (usually name of function that triggered error): nco_enddef()
nco_err_exit(): ERROR Error code is -101. Translation into English with nc_strerror(-101) is "NetCDF: HDF error"
nco_err_exit(): ERROR NCO will now exit with system call exit(EXIT_FAILURE)

Lastly, I converted the subsetted file to netCDF3 (no chunking, no
compression) and the ncks process with a stride specified took 30
seconds. I'm unsure what to try at this point. I expected ncks with a
stride specified to perform comparable or better to the brute force
approach I tried above. This leads me to a few questions. 

1) Is the poor performance of the stride parameter a bug or am I just
doing something incorrectly? 
2) What is causing ncks to error when I specify a chunk policy and map
of 'xst' on a file with chunk values of (1,23,44)? I should note that
when I tried that same policy and map with a version of the file that
had chunk values of (1,719,1421) ncks did not fail, however, it took
over an hour to complete. 

I can supply the test files if you would like them. Note the test file
sizes for the netCDF4 files range between 700-800 meg. I appreciate
any help or insight you might provide. If I've missed something or you
require additional information feel free to contact me. 

Parker Norton
************************************************************************
http://nco.sf.net#help
************************************************************************
trv:
I'd like to start planning upcoming trips and meetings.
Have you had a chance to look at these weeks?
I need to know which work for you and which, if any, do not. 

Thanks!

Hi,

Thank you taking care of the kids when I need to travel for up to
three weeks per year. I want to give you as much advance planning
time for this as possible. I also want to repeat weeks year after year
when possible to allow a rhythm to develop.

2014:
07/21-07/27 Confirmed, Hawaii
10/13-10/20 Boulder UCAR
12/08-12/14 San Francisco DOE ARM+AGU

2015:
03/30-04/05 Washington DC NASA
07/20-07/26 TBD
12/07-12/13 San Francisco DOE ARM+AGU

We did not do this in 2013 but that ship already sailed.
The 2014 weeks fall in the second half of the year.
The 2015 weeks are spread evenly through the year.

Do the remaining 2014 dates and the 2015 dates work OK for you?

Thanks!
c
************************************************************************
Travel:
Three weeks blocks 1. normal week off, 2. dvr trv wk, 3. normal week off

Potential three week blocks in 2014 (Monday--Sunday): 
12/01-12/21 no dots
	    teaching ends 12/4
	    middle week is free
	    last week is AGU
	    following week is xms

2015:
04/06-04/12 UHS, EDHS Spring Break NYC?
04/24-04/26 Jazz Fest
04/30-05/03 Jazz Fest

Locations:

Australia:
Great Barrier Reef (GBR)
Cairns (CNS) is HQ
LAX<->CNS $1784 on UA 20131115

Bali:
LAX<->DPS $1382 on EVA 20131115

Israel:

BRC:
************************************************************************
cf
Hi CFers, 

The title of this page

http://cfconventions.org/latest.html

is 

"Lastest CF Conventions Document" 

"Latest" is the bestest spelling of this word.

c
************************************************************************
Hi,

Latest ncgen seems to fail to create netCDF4 files from this CDL file
produced by ncdump from a presumably valide netCDF4 file that 
contains multiple record dimensions (MRD): 

zender@givre:~$ ncks -O -d time,0,1 -d run,0,1 -d step,0,1 -d number,0,1 -d ngr,0,1 ${DATA}/hdf/71355.ecmf.1.nc ~/mrd.nc
zender@givre:~$ ncdump ~/mrd.nc > ~/mrd.cdl
zender@givre:~$ ncgen -k netCDF-4 -b -o ~/mrd2.nc ~/mrd.cdl
assertion failure: con->nctype == NC_COMPOUND

Aborted (core dumped)
zender@givre:~$ ncgen -v
ncgen: option requires an argument -- 'v'
Usage: ncgen [ -b ] [ -c ] [ -f ] [ -k kind ] [ -x ] [-S struct-format] [-M <name> [ -o outfile]  [ file ... ]
netcdf library version 4.3.3-rc1 of Jun  5 2014 13:50:57 $

mrd.nc and mrd.cdl are attached.

Can you reproduce the problem? If so, please let me know when there is
a fix.

Thanks!
c
************************************************************************
nco mpi pnetcdf3
http://trac.mcs.anl.gov/projects/parallel-netcdf/wiki/QuickTutorial # PnetCDF
http://cucis.ece.northwestern.edu/projects/PnetCDF/doc/pnetcdf-c/index.html # PnetCDF
https://code.google.com/p/parallelio # NCAR PIO
/data/zender/tmp/netcdf-c/nc_test4/tst_parallel.c
fedora:
sudo yum install openmpi -y # mpirun
sudo yum install openmpi-devel -y # mpicc
Compiler binaries in /usr/lib64/openmpi:
export MPI_ROOT=/usr/lib64/openmpi
export PATH=${MPI_ROOT}/bin\:${PATH}
export LD_LIBRARY_PATH=${MPI_ROOT}/lib\:${LD_LIBRARY_PATH}
Installed libraries and executables in /usr/local/parallel:
export PNETCDF_ROOT=/usr/local/parallel
export PATH=${PNETCDF_ROOT}/bin\:${PATH}
export LD_LIBRARY_PATH=${PNETCDF_ROOT}/lib\:${LD_LIBRARY_PATH}
Yellowstone:
# https://www2.cisl.ucar.edu/sites/default/files/Yellowstone_Started_Nov30-2.pdf
# module add pnetcdf/1.4.1
# module load impi
ncmpidump -v one ~/nco/data/in.nc > ~/foo # works
ncmpigen -v 5 -b -o ~/in_5.nc ~/nco/data/in.cdl # fails
mpiexec -n 1 ncks -O ~/nco/data/in.nc ~/foo.nc # fails
mpiexec -n 1 ncmpigen -v 5 -b -o ~/in_5.nc ~/nco/data/in.cdl # fails
mpirun -np 1 ncmpigen -v 5 -b -o ~/in_5.nc ~/nco/data/in.cdl # fails
poe -hostfile=~/nco/bld/host.list -node=1 -tasks_per_node=3 $MY_BIN_DIR/mpncwa -O -D 3 -p ${HOME}/nco/data in.nc ~/foo.nc
# Build parallel HDF5 and netCDF
http://www.unidata.ucar.edu/software/netcdf/docs/getting_and_building_netcdf.html
************************************************************************
Hello EEE,

Please authorize the following SSH keys to access my F14 course codes
42605 and 42090.

Thanks,
Charlie

ssh-dss AAAAB3NzaC1kc3MAAACBAKIWD+K/Rg77TI0SZwph2Za4GbX9581x1a4si0Ec2OrGf7BfQosCtezk3Yw2xkas4FCVZEVUMaFm0Es0fy1B2Kr9jyIvdTaoWxyRoSBqdBRpq1dbNw4ipsIqtArC5G1ptIXcCKFwzLVpdCaRG1MJIpnERZ0yDyvYX0Azv+DnLGHLAAAAFQCTe445Suo01KkgVO1qN+5SrUy5HwAAAIBI2IBIWdplrKArDjq2wGAXJR4ur4GSbdU6t72hApC6Cc99GC+Q85t+LQDcKagFL5uW3JkeMW5oRQ+LsBeqyz4aLbmEOLQ2VApr8K9vUWMazks2uJBGxTr+NGNEtAA3H5u4yBaBW1kFjgJS2BI9oEkp8m3iKOsfHOeog9balMjBnwAAAIBlz5yJL8uiA5o/nCqQXeUrUf60pSO/shcmL7LKBNUmhgkKMvsj3j3Rsv6JpdgMQwmL8j6w1DEUnKfH8lstBmW2o13u46bCLdt32nn6bQ/8cZN8ByNmBtjkD1Py/0H5JAPpP58bdN22JSefoB1UIDuof2nkmwDi5ZUuucXNlZ2VRQ== zender@givre

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCdT5kTBHevpNBX4qy1zpLF9IX7WawMBPVnMhpqaMPl0jB0ZU9Q2EVwWgll23Kz++RFk9N+gKUr/CBparUToakQIJkyOfdwgcuscS40axgKPQtVF6LuoZsR4h63z+65USfPZHGYVxefHLyOLGz7+KwJU1Ki6Scz6dTWgcTMshlLj1Z3a/n5yT59HkubJExeZFEbXGylx5f3WGuCNg6y73MjfEWgTIaWyPq0ODfSkrnFVhsJAa64ITlLiGpNqtGPYboFQpwE2n3ytCmjp0E3+Ku/vlhiWPgPYBNeeHST9FDuwqbk9A45WNI9ULKuldiiZKaVSPSq5N987yOYJfzyURy5 zender@givre

ssh-dss AAAAB3NzaC1kc3MAAACBALO+uSS1uavDUTGwRb+3ibPBwOu4pvVuIBghsrv3ZB7GocYXEN+Dw45SSOcoUYjzAJjQWZHhsVgHOGbhm0u+gtZqACzPCRc8PMwFJkyFWHk6LulovHxA57kVNDNnHVjrd7fv4RHycGrBciYyjAEFKJmnQnKvX5PXHg+y7YnBd+zvAAAAFQCXLWoZwofwyPnWiOQlZPbP+WpKnwAAAIBOWl2MMdZOUUlutDedoKRyeLi7ZsZBqxluLdD+UasnZeBLTlrc79bu8n/baJzkKludEtEI79Y07bNRWjJN3qD16u5tCPumI4NO4mcweg9pCE1s0GfzFyYngioYlG/JfjsNJtt0oImppoxaz/lCMMInh8lKwJjnNhsWl2/qgIDEiAAAAIBZj+ZPQL7Nj/pMRh2RsecWlPTn1cqdcz/O7h75+4MyP6wTnWdVhXobk8JMnG2B+9GrtUrpuvH06Va4tZbx9496f9QszM9UQztKPNofdgABlnot7EvRvGw1hLicmpEICaVk3ek2wIecOq0h4o6DxS+7P1fmnS0xTQmRgFHFJ8Ns6g== zender@roulee

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDCd5PygIlz7XbB9E/7amYJKsNspP85QgfTbsZ+krm0/4ig61ZYwWPI/J8NuLysqRuhZmZ8uVRpExRitwzgNoZh4La4TSkNYUDszJle4nd45CU+SThMg6auSaSLznJ5BHCk9xClQ+Fpxb3+k9CiC5oAbhteRCY/oVeehrU/zGLH30nCoLuO6kUYn0HIHMH/KS5ELrj5eBAvkmOxVaMJmAod96NjNVIxZabrY7xx7ndR0PhmL5v2XFtNWnCkgoz/3usZ8PP7uXTqf3SlOiB+JfUVJI0nlKk0eUvpaZX7BQs9fP/3ULmYlBXSpqSSOjLf/VruIc8doR+Y6Sr4mpN2VP+h zender@roulee
************************************************************************
axs reuse-readiness level (RRL):
To improve NCO compliance with NASA Reuse-Readiness Levels (RRLs),
we are reorganizing NCO's documentation regarding citation,
authorship, copyright, and licensing. We would appreciate any guidance 
on our proposed reorganization of this material into the following
files in NCO's top-level:
AUTHORS, CITATION, COPYING, COPYRIGHT, LICENSE. 

File: AUTHORS 

The User Guide documentation (nco.texi) contains the most up-to-date list  
of contributors to the NCO project, see http://nco.sf.net/nco.html#ppl

<text version of actual list>

File: CITATION 

The recommended citation for NCO software is

Zender, C. S. (2008), Analysis of Self-describing Gridded Geoscience
Data with netCDF Operators (NCO), Environ. Modell. Softw., 23(10),
1338-1342, doi:10.1016/j.envsoft.2008.03.004. 

or

Zender, C. S. (2014), netCDF Operator (NCO) User Guide, Version 4.4.3,
http://nco.sf.net/nco.pdf. 

Use the former when referring to overall design, purpose, and
optimization of NCO, and use the latter when referring to specific
features and/or the Users Guide itself.

A complete list of NCO publications and presentations is at
http://nco.sf.net#pub
This list links to the full papers and seminars themselves.

File: COPYING 

[Verbatim copy of GPL3, as per FSF guidelines]

File: COPYRIGHT 

Charlie Zender wrote most of NCO and holds all NCO copyrights. 
He licenses NCO under the GNU GPL3 (see file COPYING).
Advance permission must be sought from him for any usage or 
(re-)distribution of NCO code not compliant with the GPL. 
Contact him at <surname at uci dot edu>. His surname is zender.

All other NCO contributors either explicitly assigned their copyrights 
to Zender, or their contributions were not legally significant for
copyright purposes. These contributors are all listed under
"Contributors" in the User Guide documentation (nco.texi) at
http://nco.sf.net/nco.html#ppl  
and in the file AUTHORS

The original author of this software, Charlie Zender, seeks to improve
it with your suggestions, contributions, bug-reports, and patches.
Please contact the NCO project at http://nco.sf.net or write to
Charlie Zender
Department of Earth System Science
University of California, Irvine
Irvine, CA 92697-3100

File: LICENSE 

All source code created by the NCO project and distributed with NCO is
redistributable under the terms of the GNU General Public License
GPL) Version 3. The full license text is at 
http://www.gnu.org/copyleft/gpl.html and in the file COPYING. 

As a special exception to the terms of the GPL, you are permitted 
to link the NCO source code with the HDF, netCDF, OPeNDAP, and UDUnits
libraries and to distribute the resulting executables under the terms 
of the GPL, but in addition obeying the extra stipulations of the 
HDF, netCDF, OPeNDAP, and UDUnits licenses.
   
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  
See the GNU General Public License for more details.
   
************************************************************************
It's your day to cel-e-brate
It's your day to have'a some fun,
With sweets and treats
And your favorite eats,
And you'll have a wonderful one.
It's your day to have'a your cake.
It's your day to eat it too.
It's your day to get it your way,
'cause your birthday's all about you!
************************************************************************
axs hire:
Thank you. I will consider your application in the coming weeks.    
I will notify all applicants of the outcome once the successful
applicant has accepted our offer. The timeframe for this is uncertain,  
though I anticipate making an offer before 7/1. 
Thank you for your interest and, in advance, for your patience.
Best wishes,
Charlie
************************************************************************
nco 4.4.4

A. Rew chunking map: cnk_map=rew implements the map described by
   Russ Rew at 
   http://www.unidata.ucar.edu/staff/russ/public/chunk_shape_3D.py
   This map strives to balance 1-D (e.g., point timeseries) access  
   speeds with 2-D (e.g., lat/lon map) access speeds.
   http://nco.sf.net/nco.html#cnk fxm

************************************************************************
Coupler history files from CCSM4 MOAR mother of all runs:
b40.1850.track1.1deg.006a
b40.20th.track1.1deg.012
b40.rcp2_6.1deg.006
b40.rcp4_5.1deg.006
b40.rcp6_0.1deg.006
b40.rcp8_5.1deg.007
/glade/p/cesm/shared_outputdata/cases/ccsm4/$CASE/cpl/hist
************************************************************************
Zhangfan Xing <zhangfan.xing@jpl.nasa.gov>
Thomas Huang <Thomas.Huang@jpl.nasa.gov>

http://scifari.org/test/data/hdf5/m.he5/
ncks --jsn -M -m -g // ~/nco/data/in_grp.nc # print groups as nodes, variables as leaves, and metadata
ncks --cdl -M -m -g // ${DATA}/hdf/m.he5 # not so good
ncks --cdl -M -m -g // ${DATA}/hdf/m.nc  # ok
ncks --jsn -v Latitude -M -m -g // ${DATA}/hdf/m.he5 # not so good
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/Data%20Fields/
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/Data%20Fields/Temperature/
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/Data%20Fields/Temperature[]?output=json
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/Data%20Fields/Temperature[0:4,2:4,3:5]?output=json
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/Data%20Fields/Temperature[0:4,2:4,3:5]?output=json.indented
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/Data%20Fields/Temperature[0:4,2:4,3:5]?output=nc
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/Data%20Fields/Temperature[0:4,2:4,3:5]?output=le
http://scifari.org/test/data/hdf5/m.he5/HDFEOS/GRIDS/Temperature/Data%20Fields/Temperature[0:4,2:4,3:5]?output=be
************************************************************************
nco 4.4.3
J. The --cnk_byt option was introduced to allow users to manually
   specify the total desired chunksize (in Bytes). In the absence
   of this parameter, NCO sets the chunksize to the filesystem
   blocksize of the output file (if obtainable via stat()), or else  
   to 4096 B, the Linux default blocksize. 
   ncks -4 --cnk_byt=8192 in.nc out.nc
   Note that --cnk_dmn arguments are still in elements, not bytes. 
   Should we use bytes instead of elements for all chunk arguments?
   Send us your preference to help the decisions for 4.4.1.
   http://nco.sf.net/nco.html#cnk

NCO 4.4.1 renames ncra as ncrs and ncwa as ncws (Record Statistics and
Weighted Statistics, respectively).  This is to more accurately
reflect their purpose. Backward compatibility is guaranteed. NCO
installs by default with symbolic links from ncra->ncrs and ncwa->ncws
so calling either operator by its old name still works, and calling
either operator by its new name produces the same results. 
However, ncra and ncwa are deprecated and will eventually be removed.
Please change your scripts/habits accordingly within a few years...
************************************************************************
gvc recruitment sdn grd
http://www.grad.uci.edu/funding/fellowships-awards/diversity/index.html
decade eligible: emily hawkins
fee waiver: dianne sanchez, miguel garcia
gre-based: wendy clark not4me, adrianna trusiak yes, jayson yuter
not4me, warith abdullah not4me, shovon mukherjee, rachel silvern
not4me, sankarambadi navneet not4me, reuben demirdjian not4me, rachel
bartlett yes, siyao zhai 
cz-interested domestic applicants: jessica canet (not4me), dawn woodard
cz-interested intl applicants: rachel bartlett, hsui-hui tseng
cz-interested grad recruitment day: 
Hsui-hui Tseng <shelly19910802@gmail.com>
Dawn Woodard <dawnwoodard@gmail.com> 
Zhifeng (Bruce) Yang <yangzf01@huskers.unl.edu> (Nebraska)
Michael Wood <mhwclas@gmail.com> (UCSB math guy)
Emily Hawkins <ehawkins@oxy.edu> (Oxy physics gal)
Rachel Bartlett <r.bartlett@student.reading.ac.uk> skype raychullbee
Jialin Liu <jaln.liu@ttu.edu>, http://www.myweb.ttu.edu/jialliu/ 1-806-620-8606 ttu, pnetcdf?
Morgan Gorris <mgorris@umich.edu>
************************************************************************
On a related note, I would also like to lobby to reduce these parameters
     #define NC_LEN_TOO_BIG 65536
     #define NC_LEN_WAY_TOO_BIG 1048576
by a couple orders of magnitude. A chunk that is 65536 on a side would
never fit into the 32Mb default cache. The cache must be big enough to
hold at least ~50-100 chunks. And the cache is allocated on a
per-variable basis, so if you are forced to set the cache size large
because chunk size is large, then you're in danger of running out of
memory (unless memory is an unlimited resource on your system, which
is not the usual case). Chunks that are too small do a lot less harm
than chunks that are too big.  

Barker's proposal (lfp):
So, a proposal:

The default chunking needs to take into account all the dimensions of the
 variable. I'd propose something like:

Starting from the right-most dimension (fastest varying, yes?), have the
chunk size be equal to the dimension if it's not unlimited.

When you get to an unlimited dimension, have its chunk be whatever is
needed to add up to a defined "decent sized" chunk -- I'd say maybe 1k, but
someone other than me might have a better idea for a default. Any other
unlimited dimensions would get a 1 chunk size.

So: for a 1-d unlimited variable, you'd get a chunk size of 1k (1024)

For a small 2-d variable, say (unlimited, 2), you'd get a 2 chunk size for
the second dimension, and 512 for the first.

For Dave's example above, you'd get just what he used for the 4-d variable.

You'd also need a max total chunk size as well to cap it off -- I think the
lib has that already, though it looks like it uses the limits on a
per-dimension basis, rather than a per-whole-chunk basis.
************************************************************************
jeff lee.

HDF Web service:
http://eosdap.hdfgroup.uiuc.edu:8887/thredds/catalog/hdfdap/ftp/pub/outgoing/NASAHDF/catalog.html

two beam splitters each with 50 holes
each hole is a channel
thus 50 channels per frequency
mabel uses 8-16 holes/channels each time
actual channel number is logical thing to use as a dimension scale

ncks -D 2 -M -m --cdl ${DATA}/hdf/mabel_l2_20130927t201800_008_1.h5 | m
ncks -D 2 -M -m --cdl ${DATA}/hdf/GLAH13_633_2103_001_1317_0_01_0001.h5 | m

Summary prints that group depth reaches 10 levels.
Yet converted files have no metadata-only groups, i.e., group deeper than level ~4.
Fixed this in ncks 20131218.
h5dump indeed finds groups 10-levels deep, e.g., group "tof"
                                 ATTRIBUTE "SensorCharacteristicDataType" {
h5dump -H ${DATA}/hdf/mabel_l2_20130927t201800_008_1.h5 | grep tof
ncdump -h ${DATA}/hdf/mabel_l2_20130927t201800_008_1.h5 | grep tof
ncks -g tof ${DATA}/hdf/mabel_l2_20130927t201800_008_1.h5 | grep tof
ncks -g tof ${DATA}/hdf/mabel_l2_20130927t201800_008_1.nc | grep tof

Full conversion of these files by ncks fails with "old" libraries/NCO (e.g, grele).
Works but is slow on "new" libraries (givre/roulee).
Conversion time on givre is 20--22 minutes, on roulee is 15-17 minutes.
Be cautious about executing on local workstation!
zender@givre:~$ time ncks -O -L 1 ${DATA}/hdf/mabel_l2_20130927t201800_008_1.h5 ${DATA}/hdf/mabel_l2_20130927t201800_008_1.nc
real    22m1.268s
user    17m47.503s
sys     2m33.680s

ncks -O -D 2 -L 6 ${DATA}/hdf/mabel_l2_20130927t201800_008_1.h5 ${DATA}/hdf/mabel_l2_20130927t201800_008_1.nc
ncks -O -D 2 -L 1 ${DATA}/hdf/GLAH13_633_2103_001_1317_0_01_0001.h5 ${DATA}/hdf/GLAH13_633_2103_001_1317_0_01_0001.nc

Partial dumps work quickly:

ncks -O -D 2 -L 1 -v app_refl ${DATA}/hdf/mabel_l2_20130927t201800_008_1.h5 ~/foo_mabel.nc
ncks -O -D 2 -L 1 -g altimetry ${DATA}/hdf/mabel_l2_20130927t201800_008_1.h5 ~/foo_mabel.nc

netCDF4 compression is awful compared to HDF5. Why?
LZW1 and LZW6 netCDF4 files are both twenty-times larger than LZW6 .h5 files:
zender@givre:/data/zender/hdf$ ls -l mabel*
-rw-r--r--. 1 zender cgdcsm  118108678 17 dÃ©c.  10:39 mabel_l2_20130927t201800_008_1.h5
-rw-r--r--. 1 zender cgdcsm 2604868858 17 dÃ©c.  17:52 mabel_l2_20130927t201800_008_1.nc
-rw-r--r--. 1 zender cgdcsm 2604365401 17 dÃ©c.  18:19 /data/zender/hdf/mabel_l2_20130927t201800_008_1.nc

Dismembering:
ncdismember ~/nco/data/in.nc /data/zender/nco/tmp/hdf cf 1.5 > ~/foo 2>&1 # works
ncdismember ~/nco/data/cf.nc /data/zender/nco/tmp/hdf cf 1.5 > ~/foo 2>&1 # works
ncdismember ~/nco/data/dsm.nc /data/zender/nco/tmp/hdf cf 1.5 > ~/foo 2>&1 # works
ncdismember ~/nco/data/in_grp.nc /data/zender/nco/tmp/hdf cf 1.5 '--fix_rec_dmn=all --grp_xtr_var_xcl -v string_arr,string_rec_arr' > ~/foo 2>&1 # works except group g1/g1:g2 causes ncatted failure)
fl=3B43.070901.6A.HDF;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=acos_L2s_130101_01_Production_v150151_L2s30300_r01_PolB_130225030150.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 '--fix_rec_dmn=all --grp_xtr_var_xcl -v AncillaryDataDescriptors,acquisition_mode,AutomaticQualityFlag,GapStartTime,GapStopTime,L2FullPhysicsInputPointer,SpectralChannel' > ${dst}/${fl}.txt 2>&1 # works (two core dumps, backtrace because extensive string arrays remain in two groups)
fl=AIRS.2002.08.01.L3.RetStd_H031.v4.0.21.0.G06104133732.hdf;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # fxm
fl=AMSR_E_L2_Rain_V10_200905312326_A.hdf;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (with stderr = "H5F_locate_signature(): unable to find a valid file signature")
fl=ATL01_template.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # partially working
fl=CER_SSF_Aqua-FM3-MODIS_Edition3A_303303.2011123100;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.4) (with stderr = "H5F_locate_signature(): unable to find a valid file signature")
fl=CER_SSF1deg-Hour_Terra-MODIS_TestSuite_000000.200407D01-v1.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.4) (but neglects non-atomic object)
fl=HIRDLS-Aura_L3ZAD_v06-00-00-c02_2005d022-2008d077.he5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0) (ncatted problem on whitespace in filename)
fl=mabel_l1a_20120419t075900_809_1.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=mabel_l1b_20120419t075900_809_1.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=mabel_l2a_20120419t075900_809_1.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # borken because group quality_assessment appears ~100 times as subgroup, so flattening always creates ambiguous variables in root
fl=MCD43C1.A2006353.004.2007012185705.hdf;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=MERRA300.prod.assim.inst3_3d_asm_Cp.20130601.hdf;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=MOD10CM.A2007001.005.2007108111758.hdf;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=MSLERLSTL3zm_v01-00-2012m0925t174330.he5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=MSO3L3zm5_v01-02-2013m0907t142428.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0) (ncatted problem on whitespace in filename)
fl=MYD06_L2.A2014001.0000.006.2014013194257.hdf;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # 
fl=OMI-Aura_L2-OMAERUV_2013m1004t2338-o49057_v003-2013m1005t053932.he5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0) (ncatted problem on whitespace in filename)
fl=OMI-Aura_L2-OMIAuraSO2_2012m1222-o44888_v01-00-2014m0107t114720.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=OMI-Aura_L3-OMTO3d_2013m1003_v003-2013m1005t020103.he5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0) (ncatted problem on whitespace in filename)
fl=S1999001.L3m_DAY_CDOM_cdom_index_9km.hdf;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0) (with stderr = "H5F_locate_signature(): unable to find a valid file signature")
fl=SBUV2-NOAA17_L2-SBUV2N17L2_2006m1225_v01-01-2012m0905t144148.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 > ${dst}/${fl}.txt 2>&1 # works! (4.4.0)
fl=SMAP_L1C_S0_HIRES_00363_D_20130925T184008_R04501_004.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 --grp_xtr_var_xcl -v along_track_time_utc --grp_xtr_var_xcl -v along_track_time_utc > ${dst}/${fl}.txt 2>&1 # works (4.4.4)
fl=SMAP_L1C_S0_HIRES_00365_D_20130925T215701_R04501_004.h5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 --grp_xtr_var_xcl -v along_track_time_utc > ${dst}/${fl}.txt 2>&1 # works (4.4.4)
fl=TES-Aura_L2-O3-Nadir_r0000011015_F05_07.he5;dst=${DATA}/nco/tmp/hdf;ncdismember ${DATA}/hdf/${fl} ${dst} cf 1.5 '--grp_xtr_var_xcl -v UTCTime' > ${dst}/${fl}.txt 2>&1 # works (4.4.4)

# These HDF datasets work with default arguments
for fl in in.nc 3B43.070901.6A.HDF AIRS.2002.08.01.L3.RetStd_H031.v4.0.21.0.G06104133732.hdf AMSR_E_L2_Rain_V10_200905312326_A.hdf BUV-Nimbus04_L3zm_v01-00-2012m0203t144121.h5 CER_SSF_Aqua-FM3-MODIS_Edition3A_303303.2011123100 CER_SSF1deg-Hour_Terra-MODIS_TestSuite_000000.200407D01-v1.h5 HIRDLS-Aura_L3ZAD_v06-00-00-c02_2005d022-2008d077.he5 mabel_l1a_20120419t075900_809_1.h5 mabel_l1b_20120419t075900_809_1.h5 MCD43C1.A2006353.004.2007012185705.hdf MERRA300.prod.assim.inst3_3d_asm_Cp.20130601.hdf MOD10CM.A2007001.005.2007108111758.hdf MOP01-20121231-L1V3.34.10.hdf MSLERLSTL3zm_v01-00-2012m0925t174330.he5 MSO3L3zm5_v01-02-2013m0907t142428.h5 MYD06_L2.A2014001.0000.006.2014013194257.hdf OMI-Aura_L2-OMAERUV_2013m1004t2338-o49057_v003-2013m1005t053932.he5 OMI-Aura_L2-OMIAuraSO2_2012m1222-o44888_v01-00-2014m0107t114720.h5 OMI-Aura_L3-OMTO3d_2013m1003_v003-2013m1005t020103.he5 S1999001.L3m_DAY_CDOM_cdom_index_9km.hdf SBUV2-NOAA17_L2-SBUV2N17L2_2006m1225_v01-01-2012m0905t144148.h5 ; do
    ncdismember ${DATA}/hdf/${fl} ${DATA}/nco/tmp/hdf cf 1.5 > ${DATA}/nco/tmp/hdf/${fl}.txt 2>&1
done    
scp ${DATA}/nco/tmp/hdf/*.txt dust.ess.uci.edu:/var/www/html/hdf

# These HDF5 datasets require --fix_rec_dmn=all switch to fix multiple record dimensions in same group and/or variable
for fl in in_grp.nc ATL01_template.h5 GATMO_npp_d20100906_t1935191_e1935505_b00012_c20110707155932065809_noaa_ops.h5 GLAH13_633_2103_001_1317_0_01_0001.h5 mabel_l2_20130927t201800_008_1.h5 ; do
    ncdismember ${DATA}/hdf/${fl} ${DATA}/nco/tmp/hdf cf 1.5 --fix_rec_dmn=all > ${DATA}/nco/tmp/hdf/${fl}.txt 2>&1
done    
scp ${DATA}/nco/tmp/hdf/*.txt dust.ess.uci.edu:/var/www/html/hdf

# These HDF5 datasets _might_ require --fix_rec_dmn=all switch to fix multiple record dimensions in same group and/or variable
for fl in mabel_l2a_20120419t075900_809_1.h5 ; do
    ncdismember ${DATA}/hdf/${fl} ${DATA}/nco/tmp/hdf cf 1.5 --fix_rec_dmn=all > ${DATA}/nco/tmp/hdf/${fl}.txt 2>&1
done    
scp ${DATA}/nco/tmp/hdf/*.txt dust.ess.uci.edu:/var/www/html/hdf

# These HDF5 datasets require '--grp_xtr_var_xcl -v along_track_time_utc' switch to avert converting 1-D string variables
for fl in SMAP_L1C_S0_HIRES_00363_D_20130925T184008_R04501_004.h5 SMAP_L1C_S0_HIRES_00365_D_20130925T215701_R04501_004.h5 ; do
    ncdismember ${DATA}/hdf/${fl} ${DATA}/nco/tmp/hdf cf 1.5 '--grp_xtr_var_xcl -v along_track_time_utc' > ${DATA}/nco/tmp/hdf/${fl}.txt 2>&1
done    
scp ${DATA}/nco/tmp/hdf/*.txt dust.ess.uci.edu:/var/www/html/hdf

# These HDF5-EOS2 datasets require '--grp_xtr_var_xcl -v UTCTime' switch to avert converting 1-D string variable
for fl in TES-Aura_L2-O3-Nadir_r0000011015_F05_07.he5 ; do
    ncdismember ${DATA}/hdf/${fl} ${DATA}/nco/tmp/hdf cf 1.5 '--grp_xtr_var_xcl -v UTCTime' > ${DATA}/nco/tmp/hdf/${fl}.txt 2>&1
done    
scp ${DATA}/nco/tmp/hdf/*.txt dust.ess.uci.edu:/var/www/html/hdf

# These HDF5 datasets require --fix_rec_dmn=all switch and --grp_xtr_var_xcl switch 
for fl in acos_L2s_130101_01_Production_v150151_L2s30300_r01_PolB_130225030150.h5 ; do
    ncdismember ${DATA}/hdf/${fl} ${DATA}/nco/tmp/hdf cf 1.5 '--fix_rec_dmn=all --grp_xtr_var_xcl -v AncillaryDataDescriptors,acquisition_mode,AutomaticQualityFlag,GapStartTime,GapStopTime,L2FullPhysicsInputPointer,SpectralChannel' > ${DATA}/nco/tmp/hdf/${fl}.txt 2>&1
done    
scp ${DATA}/nco/tmp/hdf/*.txt dust.ess.uci.edu:/var/www/html/hdf

# Fails to convert HDF4
ncdismember ${DATA}/hdf/MOP01-20121231-L1V3.34.10.hdf /data/zender/nco/tmp/hdf cf 1.5 --fix_rec_dmn=all
ncks --cdl --hdf4 MOP01-20121231-L1V3.34.10.hdf
../src/nco/nco_grp_utl.c:1108: nco_xtr_crd_ass_add: Assertion `nbr_dmn_var == var_trv.nbr_dmn' failed.

# Fails to get ID of dimension delta_time in group "/quality_assessment/summary" 20140408 ("NetCDF: Invalid dimension ID or name")
ncdismember ${DATA}/hdf/mabel_l2a_20120419t075900_809_1.h5 /data/zender/nco/tmp/hdf cf 1.5 # fxm
ncks -O -D 2 -3 -G : ${DATA}/hdf/mabel_l2a_20120419t075900_809_1.h5 ~/foo_mabel.nc # borken
ncks -O -D 2 -3 -G : -g /summary/ ${DATA}/hdf/mabel_l2a_20120419t075900_809_1.h5 ~/foo_mabel.nc # borken
ncks --cdl -m ${DATA}/hdf/mabel_l2a_20120419t075900_809_1.h5 # borken
ncdump -h ${DATA}/hdf/mabel_l2a_20120419t075900_809_1.h5 # borken
ncdump -h -g quality_assessment/summary ${DATA}/hdf/mabel_l2a_20120419t075900_809_1.h5 # borken
h5dump -H ${DATA}/hdf/mabel_l2a_20120419t075900_809_1.h5
************************************************************************
olivia, it is important to be honest and to treat others with respect.
i'm disappointed and, yes, exasperated at how you treat me.
while you're becoming a young woman, you expect me to clean up after
you like you're still a baby. your behavior is surprisingly immature
for one who seems to take pride in her sophistication.

although you treat me like your servant, you may not intend to.
i don't mind cooking and cleaning for you when you appreciate it.
i do resent your messes and ingratitude when i clean them up. 
this morning you brought down your crusty dishes and then skulked out
to school knowing that you had left me dirty dishes without a
"please" or a "thank you". is this behavior what you intend?

do you feel that your actions guilty. 
feeling guilty is not healthy.  

be honest and to treat others with respect.
************************************************************************
diwg.txt

A long thread ensued from the initial posting of this proposal.
Consensus to adopt "group-aware" features into CF did/does not exist.
Yet motivations to pursue such a convention continue to strengthen:
organization, synergy, simplicity, and user-friendliness.

As of November, 2013 the DIWG (nÃ©e HDF5WG) will codify "group-aware"
conventions as CF extensions called "Group-aware CF" (GACF).
What is or will be GACF?
GACF is not endorsed by CF.
GACF attempts to codify the recommended and required elements of
data and metadata organization such that GACF-compliant datasets
may utilize "group-aware" features of the netCDF API. 
GACF embraces all flat-file conventions of CF.

Organization interested in adopting GACF are encouraged to contribute
feedback, ideas, and changes to GACF maintainers.
 
GACF conventions are written in CF style, relative to the
(forthcoming) CF-1.7 convention where possible. 
Currently all GACF contributors are associated with NASA ESDSWG DIWG.
Write-access is available for individuals from other institutions too. 

Ensembles:
http://mailman.cgd.ucar.edu/pipermail/cf-metadata/2007/046103.html
http://mailman.cgd.ucar.edu/pipermail/cf-metadata/2013/057010.html
http://mailman.cgd.ucar.edu/pipermail/cf-metadata/2013/057013.html
************************************************************************
************************************************************************
ncea.txt
\csznote{ % Implement cell_methods attribute CF list 20131104
In the cell_methods attribute, the entries have the form "name: method", as
you know. The name usually identifies a dimension (see first para of 7.3), and
it doesn't matter if the dimension doesn't have a coordinate variable. I agree
with you that cell_methods is the best way to describe an ensemble statistic,
such as ensemble mean, and there have been previous discussions about doing
this over previous years. I think your second example should look something
like this.

dimensions:
  ensemble = 1 ;
variables:
  int realization(ensemble);
    realization:long_name = "Number of the simulation in the ensemble" ;
  char experiment_id(ensemble,STRINGLENGTH);
    experiment_id:long_name = "Experiment identifier" ;
  char source(ensemble,STRINGLENGTH);
    source:long_name = "Method of production of the data" ;
  char institution(ensemble,STRINGLENGTH);
    institution:long_name = "Institution responsible for the forecast system" ;
  float sh_sd(ensemble, time, latitude, longitude) ;
    sh_sd:units = "1" ;
    sh_sd:standard_name = "specific_humidity" ;
    sh_sd:cell_methods = "leadtime: mean (interval 6 h) ensemble: mean";
    sh_sd:coordinates = "experiment_id source realization institution" ;

CF doesn't give any guidance about the contents of the aux coord vars after
aggregation; the data processing software will have to decide how best to
represent the ranges of the collapsed ensemble axis. For numerical aux coord
vars such as realization above you could provide bounds.
} % end cell_methods
***********************************************************************
Hi,

Deep in a library I need to use strcasestr() to search for one string
in another, neglecting case. Neither string nor its pointer changes
due to this comparison, so I would to use a function prototyped as 

char * strcasestr(const char *sng_1,const char sng_2);

However, the stand
***********************************************************************
fb:
Me: "Ruby, here's some home-made whipped cream!
Ruby: "Can I have some un-home-made whipped cream instead?"
************************************************************************
fb:
Me: That looks like a pretty sexy TV show.
Ruby: Yep, it's pretty much got a lot of everything.
***********************************************************************
Greg, the emerging "preparedness/adaptation" consensus is, I hope, the
necessary third stage of climate change grief, if one applies
Elisabeth KÃ¼bler-Ross' model of the five stages of grief. 
From our playtime outside as kids to our scarcer yet more treasured
moments in nature as adults, the environmental experience is personal.
Much of the mitigation we all want to pre-empt geoengineering can only 
take place once policymakers and consumers understand their personal
loss, i.e., the end of the Nature that was once our birthright and
lifelong companion.

The movement of some to "preparedness/adaptation" shows that they are
beyond the stages of Denial and Anger about loss of Nature. Now they
Bargain---plan new sea walls, crops, and drought response measures.  
Bargaining will forestall some damage, and ease some suffering.
Yet after a few more decades of intensifying weather extremes---
catastrophes like Sandy, superdroughts, and unprecedented wildfires---
planners and people are likely to recognize how terribly expensive and
ineffective all this reactive spending is. The futility of Bargaining
will pave the way for Acceptance, i.e., long-term mitigation.

The road to Acceptance traverses (or circumvents) Depression in the
EKR model. Not sure there is a straightforward climate science analogy
to this stage. Most climate-scientists I know are jazzed about the
science, which is cool, though its findings for Nature are bleak. 
The wider Public is likelier to experience an analogue of Depression
than we are because, quite simply, we are inured to bad climate news.
Much of the Public has until recently secretly hoped that Climate
Denialists and Dissenters are right, that no tipping point has been
passed, and that scientists are alarmists. Understanding the extent to
which Denialists and their paymasters have manipulated public opinion
by sowing doubt about the science will take people many more years.

************************************************************************
Hi Pedro,

Here is an 8-line block of debugging code I just removed from
ncflint.c and replaced by one line. I find it much easier to follow a
screenful of actually used if densely written code rather than
debugging statements with a lot of whitespace that cause me to have to
page up and page down until I find what important code surrounds
it. Please be concise :) A one statement loop in an if-clause is a
single idea. It should occupy a single line. 
Anything else make it appear more complex than it is.

c
************************************************************************

    if(dbg_lvl_get() >= nco_dbg_dev){
      var_sct *v=var_prc_out[idx];
      for(int idx_dmn=0;idx_dmn<v->nbr_dim;idx_dmn++){
        (void)fprintf(stdout,"%s: DEBUG output count for dim %d=%d\n",prg_nm_get(),idx_dmn,v->cnt[idx_dmn]);     
      } 
    }

************************************************************************

    if(dbg_lvl_get() >= nco_dbg_dev) for(int idx_dmn=0;idx_dmn<var_prc_out[idx]->nbr_dim;idx_dmn++) (void)fprintf(stdout,"%s: DEBUG output count for dim %d=%d\n",prg_nm_get(),idx_dmn,var_prc_out[idx]->cnt[idx_dmn]);     

************************************************************************
fb:
Tuesday I went to a PTA meeting for which I had prepared for months. I usually enjoy PTA meetings because the women (and yes, they are all women) support and listen to and care for eachother so well. Not so this meeting, where everything that could go wrong did. When I left I was soured on volunteering, to teach two classes I felt unpreared for, dreading the three hours ahead. The 
************************************************************************
20130517: 1 bonfire = 1 truck 562 miles
Later in life Pauline struggled to breathe. Michael had difficulty remembering. Tessa struggled to understand the meaning of the day's headlines. Were their debilitating symptoms related? All are symptomatic of neuro-degenerative processes  
They had once lived in Newport Beach, where smoke from bonfires
polluted her lungs.
*******************************************************************
This has not worked for me since NYT changed things in March 2013. To be careful, I removed the old bookmark and installed the revised version in April 2013. No dice.
************************************************************************
"when we have compared everything in the world to everything else in the world" 
***********************************************************************
nco: timeseries extraction of nldas data teng.txt
William.L.Teng@nasa.gov Goddard Earth Sciences DISC (301) 614-5164
# Soil moisture directory and typical file
ftp://hydro1.sci.gsfc.nasa.gov/data/s4pa/WAOB/LPRM_AMSRE_D_SOILM3.002
ftp://hydro1.sci.gsfc.nasa.gov/data/s4pa/WAOB/LPRM_AMSRE_D_SOILM3.002/2002/06/LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T111931Z_20020619.nc
ncks -O -v ts -d Latitude,40.0 -d Longitude,-105.0 -m -M LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T111931Z_20020619.nc ~/foo.nc
DAP server:      
http://hydro1.sci.gsfc.nasa.gov/opendap/LPRM_AMSRE_D_SOILM3
ncecat -O -u time -v ts -d Latitude,40.0 -d Longitude,-105.0 -p http://hydro1.sci.gsfc.nasa.gov/opendap/LPRM_AMSRE_D_SOILM3 2002/06/LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T111931Z_20020619.nc ~/foo.nc
ncks -d Time, -d lat,40.0 -d lon,-105.0 http://hydro1.gsfc.nasa.gov/dods/fxm
# roulee ncdump, ncks, ncecat all fail with "NetCDF: Index exceeds dimension bound"! 
# DAP to GDS via .ncml file produces ~/amsre.nc file
ncks -O -v ts -d time,0,1 -d Latitude,40.0 -d Longitude,-105.0 http://hydro1.sci.gsfc.nasa.gov/opendap/hyrax/ncml/LPRM_AMSRE_D_SOILM3_timeSeries.ncml ~/amsre.nc
# DAP directly to .nc files produces ~/amsre.nc file that works with ncks and ncdump: 
ncecat -O -u time -v ts -d Latitude,40.0 -d Longitude,-105.0 -p http://hydro1.sci.gsfc.nasa.gov/opendap/LPRM_AMSRE_D_SOILM3/2002/06 LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T111931Z_20020619.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T114533Z_20020620.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T121204Z_20020621.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T123806Z_20020622.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T130407Z_20020623.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T133009Z_20020624.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T135610Z_20020625.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T142241Z_20020626.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T154045Z_20020629.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T160716Z_20020630.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T221651Z_20020627.nc LPRM-AMSR_E_L3_D_SOILM3_V002-20120512T221751Z_20020628.nc ~/amsre.nc
************************************************************************
ncap2 -O -v -s 'mss_val_nbr=three_dmn_var_dbl.number_miss();print("Missing value for three_dmn_var_dbl is ");print(three_dmn_var_dbl.get_miss(),"%d\n");print("Number of missing values in three_dmn_var_dbl: ");print(mss_val_nbr,"%d");' ~/nco/data/in.nc ~/foo.nc
************************************************************************
Dangers of CO2

1896 Swedish scientist
CO2 doubling, accurately 
Swede, protracter, predicted

No GCM, 
ad hominem,
math and physics

30 years later 
Human calculator

1941 orbital forcing
eccentricity precession obliquity
cycle 

1957 CO2 measured
Keeling detector
Mauna Loa, Seasonal cyculator

1988 Mississippi drought
Hansen testimony
Magazine notoriety

IPCC founded

1992 UCED Rio Summit FCCC
Bush signed stil apolitical

1997 
************************************************************************
fb: 
To my FB friends:

I'm always leery of posting things to FB because I do not trust FB Inc.
FB follows the usual tactics of Apple and Microsoft to lock-in us users and keep us from using competing products, from reclaiming our data, from owning, in a meaningful way, our digital identities.

Some of you know I am an long-time advocate of and contributor to the Free Software movement, aka, the FSF or Open Source.
One reason the Internet is still partially free is its origins in free software.
A free Internet, for example, allows users to legally distribute, copy, and enjoy their own music and videos without paying surcharges every step of the way.
My preference for a free software ecosystem is why I eschew Windows and Apple and run Linux instead.

What you do on Windows or Mac OS on your home or office computers I do on Linux with obscure programs that are less pretty, though just as (or more) functional.
The point of social media is to connect with friends so I use FB because my friends do.
I am fascinated by, proud of, and humored by your FB postings.
It irritates me that there's nothing as "connected" as FB in world of free software.

A truly free social media site would enable users to exchange all their postings with other social media sites.
Our postings are, after all, an expression of ourselves it's silly to restrict our expressions to a single company.
The free social media software that does exist is so obscure even I've never heard of it (elgg, Xoops, ...).

Could the social media site Google Plus be a pragmatic solution to my dilemma?
I prefer Google+ over FB for ethical reasons.
G+ is just as technically non-intuitive to me as FB.
But G+ is integrated into Android, which is Linux for the masses.
And Android is owned by Google, which literally promises to "do no evil".
It's naÃ¯ve to think Google always does good, yet I think their corporate principles, structure, and ethos leads to better choices (that preserve their users' digital freedoms) more than Apple, Microsoft, Facebook, and Amazon. 

What's more, my older daughter and all her friends use Google plus, because they are too young for FB, not for G+ or Gmail.
Her generation is happy with G+ on its own terms, without any ethical considerations.
This gives me hope that in ten years or so G+ may have as many users as FB.
I want to encourage and not delay that day.

So this is a brief explanation of why I'm transitioning away from FB to G+.
************************************************************************
fb:
Today I first sensed that the physical sciences at UC Irvine, which have fared relatively well through the last 5 years of system-wide (and CA-wide) budgetary bleed-out, are becoming irrevocably damaged. Administration is canceling important, integrative programs that prepared students for a future with complex energy, business, and environmental constraints. They have little choice. Students pay record tuitions each new year, for an education of diminishing quality.

Once upon a time (in the misty past when I turned 18), all you had do to gain admittance to UC, and eventually to graduate, was prove you were smart. Fortunately that hasn't changed much. But nowadays your family better have cushy upper middle-class resources too.

Deterioration state-wide. 
Short-sighted edu-cide.
When in doubt, educate. 
Vote for 30 not 38.

WARNING: I will keep rapping until 30 passes and is enacted.
************************************************************************
Olivia chores:
Lunch Box in kitchen
Put-away backpack/purse/bag
Put-away shoes put
Food put away after cooking
Put-back what what I take out
Vacuum 2x per week (thu-fri)
************************************************************************
fb:
For TC, or anyone looking for a sleeper on Netflix: The Singing Detective is my favorite-ever unknown mini-series. Six episodes first broadcast by the BBC in the 1980s. Produced long before the surreal works of Charlie Kaufman, this mini-series traces the hallucinations of a pulp-mystery writer as he heals from a grotesque disfiguration stemming from childhood trauma. Think "The English Patient" meets Phillip Marlowe in "Top Hat". Six episodes take-up two Netflix DVDs.
************************************************************************
Havn't seen this new NRC report on CCS mentioned on the newsgroup.
Disappointing news. Physics Today summary from 8/2012 issue, pbinde. 22:

c
************************************************************************
fb:
The water cleared-up at Crystal Cove. It was clear enough to see all
the cool and unsettling things you're swimming with but can't see the
rest of the year. So there's this family of, I swear, a dozen
stingrays all bedded down in the sand beneath me. At 1-3 feet wide
they're relatively small compared to the monster rays I've seen in the 
Caribbean. Still, those stingers kinda' spook me because of what
happened to Steve Irwin, the "Crocodile Hunter". Fins and tails
rippling imperceptibly until ... my shadow passes over. This spooks  
a few of them into launching. As I swam away I could not help but
remember the scene in Aliens where not one, but dozens of the monsters
that most haunt your dreams are swarming towards you.
************************************************************************
Hi Hardip,

If you're suggesting we share a meal together, that sounds good.
I enjoy meeting new people, and prefer that to eating alone.
************************************************************************
avatar disc
Panasonic DMP-BD70V Blu-ray Disc/VHS Multimedia Player upconconverter
foxconnect ticket # 235028
http://panasonic.jp/support/global/cs/bd/download/bd60/bd60_na.html
************************************************************************
singing.txt
youtube clear tone voice lesson 1, 2
singing-made-simple
************************************************************************
FYI the current NCO snapshot now uses nc__open() and nc__create()
in order to support non-default file buffer sizes:

A. All operators allow specification of buffer size for reading and
   writing files. Specify size with --bfr_sz_hnt or --buffer_size_hint: 
   # Request 2 MB file buffer instead of default 8 kB buffer
   ncks -O -D 3 --bfr_sz=2097152 ~/nco/data/in.nc ~/foo.nc
   Larger sizes can increase access speed by reducing the number of 
   system calls netCDF makes to read/write data from/to disk.
   http://nco.sf.net/nco.html#bfr

I tested the efficacy of this in ncks with the commands below. 
I did not notice that this causes any gain in speed on top of the
existing MM3 patches already in NCO. Whether the buffer size patch
improves performance on operators that do not yet have MM3 patches
(ncea, ncecat, ncbo, ncpdq, ncflint) is not yet tested/known.
Perhaps Andy Mai can use my binaries to run some of his own tests.

In any case, the patches suggested by Andy Mai/Rob Latham were
worthwhile since they increase opportunities to optimize other cases, 
and they pave the road to support diskless files.

time /glade/home/zender/bin/LINUXAMD64/ncks -O -D 3 -x -v TH ~/gary.nc ~/out3_mrg_4.2.0.nc
time /glade/home/zender/bin/LINUXAMD64/ncks -O -D 3 -x -v TH --bfr_sz=2097152 ~/gary.nc ~/out3_mrg_4.2.0.nc

time /glade/home/zender/bin/AIX/ncks -O -D 3 -x -v TH ~/gary.nc ~/out3_blf_4.2.0.nc
time /glade/home/zender/bin/AIX/ncks -O -D 3 -x -v TH --bfr_sz=2097152 ~/gary.nc ~/out3_blf_4.2.0.nc
************************************************************************
fb: 
Not counting cereal and self-prepared/foraged food, here is what I
prepared (and cleaned up) for the kids during one five-day (Wednesday
afternoon-Monday morning) stretch: 
Wed lunch: tomato soup with oyster crackers
Wed dinner: margharita/pesto veggie and cheese pizza
Thu lunch: macaroni and cheese with succotash (corn, lima beans, tomatoes)
Thu dinner1: green beans sautÃ©ed with diced bacon and walnuts
Thu dinner2: veggie burgers with grilled tomatoes, red onions, mushrooms
Fri snack: yet another macaroni/succotash
Fri dinner: macaroni/succotash
Sat brunch: crÃªpes with nutella, fresh berries, jam, and whip cream
Sat snack: peanut butter sandwiches
Sat dinner: leftovers,toasted bread with nutella
Sun brunch: crÃªpes and biscuits with butter
Sun dinner: poached salmon fillets
Mon breakfast: hot chocolate, cambric, biscuits with butter, crÃªpes
Mon lunch: crÃªpes, grapes, edamame, goldfish crackers
************************************************************************
Funny, riveting post in humanitarian intervention:
http://www.jadaliyya.com/pages/index/4987/stuff-white-people-like-n.135-humanitarian-interve
How to respond?
Wicked fun. Can't wait for the prequel to roast those who favored 

intervention to prevent genocide in the Rwandan civil war. 
************************************************************************
groomsmen: curtis, john z., mark t., patrick, ???
************************************************************************
Wanted: Used bike helmets and rollerblade pads for kids

Does anyone have serviceable bike helmets and/or rollerblade pads
(to protect knees, wrists, elbows) that fit kids aged 6 and 12?

On that note, if you want a used pink bike helmet now too small for
the 6 year-old free to first person who asks. Expect she will outgrow
her current bike within the year and will post again once she migrates
to a larger one.  

Please respond off-line.

Thanks,
Charlie
************************************************************************
cat > ~/ncap2_foo.nco << 'EOF'
vertical_sum=three_dmn_rec_var.total($lat);
//print(three_dmn_rec_var);
//print(vertical_sum);
EOF
ncap2 -O -v -S ~/ncap2_foo.nco ~/nco/data/in.nc ~/foo.nc
ncks -O -C -H ~/foo.nc
***********************************************************************
Here's a simpler test case that indicates that newest NCO gives correct
answers when transposition is required to correctly weight 2-D variables.
This surprises me because I don't remember implementing this functionality.
Yet I verified the results manually with 'bc' (see below).
I would be pleasantly surprised if you are unable to post a similar test 
case where ncwa is _demonstrably_ incorrect (yes, that's a challenge).
For now I'll assume that ncwa works, even in cases where I thought it 
should fail.

cat > ~/foo.cdl << EOF
netcdf foo {
dimensions:
    two = 2 ;
    three = 3 ;
variables:
    float a(two) ;
    float b(two,three) ;
    float c(three,two) ;
data:

    a =  1, 2 ;
    b =  1, 2, 3, 4, 5, 6 ;
    c =  1, 2, 3, 4, 5, 6 ;
}
EOF
ncgen -b -o ~/foo.nc ~/foo.cdl
ncwa -O -w b ~/foo.nc ~/ncwa_foo.nc
ncks -H ~/ncwa_foo.nc

yields

zender@roulee:~$ ncgen -b -o ~/foo.nc ~/foo.cdl
zender@roulee:~$ ncwa -O -w b ~/foo.nc ~/ncwa_foo.nc
zender@roulee:~$ ncks -H ~/ncwa_foo.nc
a = 1.5 
b = 4.33333 
c = 4.09524 

which appears correct according to 'bc':

zender@roulee:~$ bc
(1*1+2*2+3*3+4*4+5*5+6*6)/(1+2+3+4+5+6)
4.33333333333333333333
b should be 4.333 so ncwa is correct

zender@roulee:~$ bc
(1*1+2*4+3*2+4*5+5*3+6*6)/(1+2+3+4+5+6)
4.09523809523809523809
c should be 4.095 so ncwa is correct
************************************************************************
bk: 
rlz bk opens with vows xmas morning. how did we get there? what happened after?
************************************************************************
pumas.txt kahn

weighted averages: use example of 0.5 vis + 0.5 NIR contributions to albedo?
fractions and earthquakes 6-8 level
************************************************************************
peltason.txt
Dear Jack and Suzie,

We have not met, though we have been neighbors (3 Whistler) since 2000.
Recently I met your friend Mary Roosevelt and we talked about UCI, 
chancellors, and the subject turned to you because we're neighbors.  
She said Jack is battling health problems and might enjoy company.
I occasionally have free evenings when I would be happy to come and
talk, even if the conversation is one-sided. I could even bring one of 
my daughters (6, 11). Let me know if you would welcome any of this.
I hate to think that you might be lonely just up the street when there
is so much life around here.

My phone number is 949 891-2429 and my email is zender@uci.edu.

Charlie
************************************************************************
COMET Geostationary Satellite Course

Course Homepage:
http://courses.comet.ucar.edu/course/view.php?id=76

METED modules:
http://meted.ucar.edu/

Environmental Science Resource Center:
http://meted.ucar.edu/esrc

Sources for GNG/CNT aerosol presentation:
METED Weather and Health Module: Everyday Weather
KML files for Google Earth:
Air quality: airnow.gov
Wildfires: NAAPS NRL FlambÃ©
Google Earth Weather Bundle

GNG Marine Stratocumulus Brightening:
1. How Sunlight Heats/Cools Earth
2. Clouds/Aerosol Interactions with Sunlight
3. Volcanic cooling of Climate
4. Marine Stratocumulus

Google discussion group:
https://docs.google.com/present/view?id=ddtq3rzw_279hd9bm4gx

IDV Repository:
http://motherlode.ucar.edu/repository

Stitched satellite imagery:
http://nhc.noaa.gov
 http://opc.noaa.gov 
http://disc.sci.gsfc.nasa.gov # A-train

GE SAL Aerosol product?
sudo aptitude install lsb-core
************************************************************************
fb status-update character limit: 420
************************************************************************
fb:
on growing older and more humorous: My mother's opening when she calls a customer support line goes like this: "Hi, I'm wondering if you can help me. I'm 77-years old, fat, and ugly. I've been on your website for an hour and I can't figure out...". It's her birthday today and we're spending it together. She told me I'm her favorite.
************************************************************************
fb: Dating site correspondence between me and a self-described conservative illustrates why Mary Matalin and James Carville don't talk politics at home:
me: "While I read your profile I chuckled a few times at how opposite we are. Imagine being stuck in an elevator together!"
her: "I could show you pics of my gas guzzling truck, talk about which Republican must defeat Obama.....and you could explain to me why it's so cold outside with all the global "warming" while you try to figure out how to bust us out of the elevator while wearing those wimpie Beirkenstocks! :)"
me: "it would be quicker, and better for my Birkenstocks, if you shot the door open with your concealed weapon."
************************************************************************
fb: 
SRIs in school. They still use them.
************************************************************************
fb: 
As you probably know, my wife and I separated and are divorcing. In addition to introductions from friends (come on friends, the pickings have been pretty slim so far, up your game), I search on-line for compatible dates. Sometimes e-romance delivers lines that deserve water-cooler discussion. Such as "What do you like to read? I usually read Dental Hygiene journals".
************************************************************************
fb: iz "over the rainbow"
http://www.youtube.com/watch?v=V1bFr2SWP1I&NR=1&feature=fvwp
************************************************************************
fb: 
Set a PR for longest-expired refrigerator item. Before squirting the red chili sauce (Tu'o'ng Ã³'t Sriracha) from my fridge onto some leftovers (Korean stone-pot tofu Bim-Bim-Bop from Koba Tofu Grill), I noticed the expiration date (barely legible in photo): 12/2001.
photo
************************************************************************
fb: Our high school (go Trojans!) placed 8th in Newsweek's list of Best (public) High Schools in America, although its student/teacher ratio is 34.5, nearly twice the ratio at many other schools in the list. Limited resources (thanks to Prop. 13) don't necessarily produce mediocrity everywhere, yet does CA have to test the limits?
http://www.newsweek.com/feature/2011/americas-best-high-schools.html
************************************************************************
fb:
Not so psyched for the 11-hr drive each way to Burning Man. Must beef-up my favorite playlists which last only an hour. May add Grateful Dead concerts from the Internet Archive. Any suggestions? Wish I knew the dates of the shows I saw. Especially that one I saw and missed at the same time due to catatonia :)
************************************************************************
fb:
Need to bedeck my bicycle with lights and tinfoil for Burning Man. I doubt my peddling will power enough LEDs so I'm looking into battery-powered luminescent strings that last a few days on AAs: http://www.bikerumor.com/2010/10/15/bike-glow-luminescent-string-light-wraps-around-your-bicycle/
************************************************************************
fb: Article
Thanks for that link, Danielle. The RunBare summary and interview are much clearer than the Nature article. All my toes are now accustomed to minimalist running shoes. Now I can comfortably run "barefoot" the same trails I ran shod. Here "barefoot" means Vibram FiveFinger Bikila shoes. Ran 23.5 km this AM and feel great.

My seventh grade X-country coach, Ken Israel, told us to heel-strike. This I obeyed until hearing, and then running with, classmate and barefoot professor Daniel Lieberman at our 25th college reunion in May 2011. At a seminar called "The Biology of Middle Age" he explained why cushioned running shoes make us more accident-prone, and are a redundant, inferior, substitute for bare feet and brains, which co-evolved Homo into the best long-distance runners of any mammal.

I adopted the Bikilas in June 2011, and my gait changed and finally stabilized in November. Slapping the ground barefoot at fullspeed is harsh, especially down cold, morning pavement, so I begin slowly, with a bouncy walk, then a prissy jog. Imagine a tipsy dancer stumbling forward on hot coals. The first uphill relaxes me because the shoes weigh almost nothing, have no snaggy edges, and make my feet feel light and grippy. Then my newly strengthened feet and calves seem to wake-up and do the job they evolved for.

After an uphill, I run lightly, and adjust to the surface's contours. On streets my outside mid-sole strikes with the forefoot, absorbing the impact over a wider area. On uneven trails my forefoot dominates, as my splayed toes seek to use their individual grips to land and launch on and off the often barely perceptible surface features. I'm still learning to take more, choppier steps so that my shins strike when vertical, i.e., not to run with the breaks on.  Humorous to me that landing my foot beneath my body takes conscious training, aka, unlearning.

Knowing that "barefoot running" is seen as a fad and I'm still new to it, here are my impressions: My hips no longer bother me on long (> 1 hr) runs. My feet and the body they support are much more aware of and balanced on the terrain. No more cheating by sloppy landing on cushy, oversized, foot prophylactics. My calves are healthier. However, I do run more slowly, take a few km to hit my stride, and my triathlon T2 transition times (bike shoes to FiveFingers) are, well, a running joke :)
************************************************************************
fb:
Two weeks ago my friend Ron and I couldn't run together because he injured his knee many months ago. While it heals, he sometimes rollerblades, which he said takes only about ten sessions to learn. Sports that were in the hunter-gatherer Olympics (running, swimming, climbing, ... fleeing) come easily to me yet sports with technology (skateboards, rollerblades, skates, skis, snowboards, bikes, guns) intimidate me. So as not to propagate my shameful weakness to my girls, I decided on the spot to try learn to rollerblade. 

I just finished my tenth rollerblading session in ~20 days.  Taking Ron at his word, I should be at least competent now. fxm
************************************************************************
fb: 
A friend and I have debated the gender of the seasons: 
Me: "I am reclaiming Summer as Male. Winter can be Female." 
Her: "Haven't you heard of Old Man Winter? And did not Shakespeare compare a woman to a summer's day? Sorry buddy, but summer's a chick." 
Me: "I have it on good authority from various merchandisers dedicated to selling fine wares, that Summer is a blonde beach dude and Winter is a jazzy chick in an elf outfit. So who knows more about Summer: tanned marketers or pale writers? Keep your dog in this fight and it's gonna' get bit." 
Her: "Summer will be signaled by my little blond nieces running around in swimsuits. You wouldn't dream of biting them, would you?"
What do you think, is Summer Male or Female?
************************************************************************
fb:
Took two kids used-clothes shopping. To preserve their anonymity, let's call them Dracula and Werewolf. I saw and heard their fangs and gutteral growling in their backseat cage. Werewolf can be exquisitely merciless to Dracula, mocking the toothlessness of her banter: 
"You don't even know what money is"  
"Yes, I do"
"Prove it!"
"You prove it"
"I already did"
"Wanna bet on it"
"I have a sister I hate".
Dracula responds to this by sharpening her fangs: "You're a meanie".
Drifting back from the front seat comes a plea for peace:
"Girls, if you were kinder to eachother, you'd both be less upset". 
The futility of that position becomes clear a second later:
"Oh yeah, well tell her to stop copying me", "I'm not copying you, you're copying me"...
************************************************************************
# TODO nco1049. ncrcat Generates warnings like "ERROR: Conversion between user specified unit "days" and file units "" is meaningless"
ncgen -o ~/foo_1.nc << EOF
netcdf foo_1 { dimensions: time = UNLIMITED ; // (2 currently) 
variables: 
double time(time) ; 
time:units = "days" ; 
float SST(time) ; 
data: time = 1 ; 
SST = 27; } 
EOF
ncgen -o ~/foo_2.nc << EOF 
netcdf foo_2 { dimensions: time = UNLIMITED ; // (2 currently)
variables: 
double time(time) ; 
time:units = "days" ; 
float SST(time) ;
data: time = 3; 
SST = 29; }
EOF
ncrcat -O -h ~/foo_1.nc ~/foo_2.nc ~/foo.nc
************************************************************************
Middleton 20110426:
LAS from PMEL allows ferret analysis
steve hankin
NCL and CDAT also backends
GDS accepts full analysis scripts
DRS Data Retrieval Syntax
LAS handles security
ESG servers use read-only mode on GLADE servers 
DRS Data Retrieval Syntax 
************************************************************************
Charlie,
I've been told that you are frustrated by lack of usage of your tools (NCO and SWAMP) here at NCAR (or is it only for ESG? I guess you know that NCO is very widely used on our HPC and viz clusters, right? It's a great tool, and we discussed about it on sourceforge in the past)

I investigated the issue a little further and I've learned what follows:

1) NCO is not used by ESG yet, but it is certainly a possibility for the future

2) SWAMP is based on GridEngine (right?) so it'll be a little tricky to have it in our HPC environment (however I'm very interested in it, there are some users who are looking for something like this, and they suggested if we can try swift, which has similar "it does not support my scheduler" issues). Is there a lightweight way to make SWAMP LSF-friendly?
My understanding is that ESG has serious concerns about security issues and SWAMP in their environment will make their security worse.

Thanks and Regards,
Davide Del Vento,
NCAR Computational & Information Services Laboratory
Consulting Services Software Engineer
http://www.cisl.ucar.edu/hss/csg/
SEA Chair http://sea.ucar.edu/
office: Mesa Lab, Room 55G
phone:  (303) 497-1233
mobile: (303) 720-6338
************************************************************************
ncar.txt
Hello Davide,

Thanks for your note about NCO/SWAMP support at NCAR.

> I've been told that you are frustrated by lack of usage of your
> tools (NCO and SWAMP) here at NCAR (or is it only for ESG? I guess
> you know that NCO is very widely used on our HPC and viz clusters,
> right? 

I am grateful to Anke for listening and reporting my frustrations to
CISL. However, my concerns may have been changed in translation.
So let's divide the subject into two areas, NCO and SWAMP.

First, I can dispense with the SWAMP-related issues:
CISL and ESG gave us little traction or opportunity with SWAMP.
This frustrated me for years, since the need for such a system was,
and is, real, and I had funded personnel eager to implement a SWAMP
pilot on the NCAR Community Data Portal. I recently discussed this
with Don Middleton. He seems receptive to trying something SWAMP-like,
if CISL receives funding to help implementation. We are considering
next steps. He has heard what I want to say about SWAMP. 

I am not frustrated by the lack of users of NCO on CISL machines.
There are plenty of users, as you note. The CISL Technical Consulting
Group (TCG) has kept its word to support NCO like other community
software packages such as lapack. In other words, install it and
forget about it. They could do better for their users. It would
benefit their users, and NCO, if they tried.

Here's an example:
The default NCO on bluefire, i.e., that in /usr/local/bin, is version
3.9.6 which you, Davide, built on 20090121. It is 12 stable versions
out-of-date. Only savvy users will find the /usr/local/apps/nco-4.0.6
executables. The naive user, including most new users, will simply use
the executables in /usr/local/bin forever. 

Later versions of NCO contain useful features and bug fixes; many that
were spurred by suggestions or reports from NCO users on CISL
machines. I think it is fair to say that most users of NCO on CISL
machines have the impression that NCO is a static tool with a fixed
set of features, as opposed to a dynamically evolving toolset with 
regular releases of new features and bug-fixes. CISL surveys its users
occasionally and I presume that CISL would comply if its users asked
for more frequent NCO updates, so the NCO users seem to be quiet,
happy, or both.

Contrast this with NCL. There have been four updates of
/usr/local/bin/ncl since the last update of /usr/local/bin/NCO.
Clearly CISL upgrades its own software more regularly than NCO.
There's nothing surprising with that. It's just a fact. 

I have put bluefire executables on the NCO homepage and in my
personal bluefire directories for years. Yet CISL does not seem to
take advantage of either. I don't know why.

> It's a great tool, and we discussed about it on sourceforge in the past) 

I investigated the issue a little further and I've learned what follows:

1) NCO is not used by ESG yet, but it is certainly a possibility for the future

2) SWAMP is based on GridEngine (right?) so it'll be a little tricky to have it in our HPC environment (however I'm very interested in it, there are some users who are looking for something like this, and they suggested if we can try swift, which has similar "it does not support my scheduler" issues). Is there a lightweight way to make SWAMP LSF-friendly?
My understanding is that ESG has serious concerns about security issues and SWAMP in their environment will make their security worse.
************************************************************************
prp wnd sgs phz ocn cld capps (or prp wnd slr ese gng ?)
1. necessity of sgs wnd in atm when scale >~ 100 km
2. hypotheses:
   remediate drake passage throughput
   roughen ocean in high winds, affect sea-ice transport
   improve zonal jets
3. tasks: 
   response in uncoupled models of atm/ocn
   response in coupled model
4. implementation in cesm for
************************************************************************
scott fourth paper:
coastal windpower
power as function of depth, distance
use (& compare) rated power curves not analytic functions
************************************************************************
prp wnd efs
4968. prp: Energy for Sustainability ? due 8/15/2010-9/23/2010
http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=501026&org=NSF PD 10-7644 
Gregory Rorrer <grorrer@nsf.gov>  	(703) 292-8320
Geoffrey Prentice <gprentic@nsf.gov> 	(703) 292-8320 

Dear Drs. Rorrer and Prentice,

This is a request for guidance on whether our research interests are
suitable for a full proposal to the Energy for Sustainability program.
My collaborator (Dr. Scott Capps) and I work at the interface of Earth
System Modeling and the quantification of renewable energy potential,
e.g., the characterization of available wind power and solar energy. 
We predict resource availability (e.g., wind speed, surface
insolation) in regional-to-global scale models, and convolve these
predictions with the necessary technical information (e.g., turbine
type, capacity factors) to estimate renewable energy potential.
This paper shows some of our work on offshore wind energy:

http://dust.ess.uci.edu/ppr/ppr_CaZ10.pdf

We would like to extend our methods to estimate the renewable energy
available from all fast-timescale meteorologically-related sources
wind, wave, and solar. Climate scientists (and NSF GEO AGS programs)
may consider our work too applied, because we consider technical
limitations to renewables, while engineers (and NSF ENG CBET programs)
may consider our work too theoretical, because we use climate models
and meteorological analyses/assimilations to estimate power, rather
than building or designing system components.  
However, the shift to renewable power sources entails load-balancing
multiple renewables with non-renewable baseload. 
This requires harvesting renewable energy over geographic regions
of sufficient size to reduce meteorological intermittency effects
(e.g., cloud, wind, and wave variability) to acceptable levels.
Hence the climatological and technical problems are linked.

To our knowledge there is no single objective source where one can
find accurate and self-consistent estimates of wind, wave, and solar
power potential. We can integrate the meteorological, geographic, and
technical requirements and conditions to produce a global atlas of
wind, wave, and solar power potential. This would provide, for the
first time, a high level, objective view of the relative constraints
on renewable power resources that energy planners would value. 
Questions like: "How far afield would we need to harvest wind and
solar energy to supply 10% and 5%, respectively of summertime peak
demand in Los Angeles with 99.99% reliability?" could be explored.
Our work could be extended to include other renewables, too.

Would such a proposal fit the Energy for Sustainability mandate?
Is there a more appropriate NSF program for such work?
Please feel free to call (949-891-2429) or email with some guidance. 

Sincerely,
Charlie
************************************************************************
Dear fxm,

I've looked through my digital photograph collection and found
some pictures that you might want. You can view and download them
directly from the websites listed. Hope you enjoy them!

Charlie

http://dust.ess.uci.edu/pix/zen/200809_irvine_annecy_halloween_thanksgiving/img_
http://dust.ess.uci.edu/pix/zen/200812_agu_san_miguel/img_
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_.jpg
************************************************************************
Manuel:
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0637.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0639.jpg

Anna:
http://dust.ess.uci.edu/pix/zen/200809_irvine_annecy_halloween_thanksgiving/img_0049.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0643.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0644.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0647.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0647.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0650.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0651.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0652.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0653.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0654.jpg

Brian + Suzanne:
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0789.jpg

Ken + Shelley:
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0664.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0665.jpg

Marina:
http://dust.ess.uci.edu/pix/zen/200809_irvine_annecy_halloween_thanksgiving/img_0049.jpg
http://dust.ess.uci.edu/pix/zen/200809_irvine_annecy_halloween_thanksgiving/img_0050.jpg
http://dust.ess.uci.edu/pix/zen/200809_irvine_annecy_halloween_thanksgiving/img_0066.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0354.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0365.jpg

Luna:
http://dust.ess.uci.edu/pix/zen/200809_irvine_annecy_halloween_thanksgiving/img_0050.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0358.jpg
http://dust.ess.uci.edu/pix/zen/200905_g1/1241020834061.jpg
http://dust.ess.uci.edu/pix/zen/200905_g1/1242835505692.jpg

Sophie:
http://dust.ess.uci.edu/pix/zen/200809_irvine_annecy_halloween_thanksgiving/img_0050.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0361.jpg
http://dust.ess.uci.edu/pix/zen/200905_g1/1242835851547.jpg

Sandra and Roy:
http://dust.ess.uci.edu/pix/zen/200812_agu_san_miguel/img_6534.jpg
http://dust.ess.uci.edu/pix/zen/200812_agu_san_miguel/img_6569.jpg
http://dust.ess.uci.edu/pix/zen/200812_agu_san_miguel/img_6572.jpg

Mike and Ellen:
http://dust.ess.uci.edu/pix/zen/200812_agu_san_miguel/img_6536.jpg

Carolyn...:
http://dust.ess.uci.edu/pix/zen/200812_agu_san_miguel/img_6530.jpg

Walla Walla people:
http://dust.ess.uci.edu/pix/zen/200812_agu_san_miguel/img_6536.jpg

Veronica:
http://dust.ess.uci.edu/pix/zen/200812_agu_san_miguel/img_6595.jpg

Alessia:
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/1235577261696.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0363.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0540.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0541.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0544.jpg
http://dust.ess.uci.edu/pix/zen/200905_g1/1241020681004.jpg
http://dust.ess.uci.edu/pix/zen/200905_g1/1241020695402.jpg
http://dust.ess.uci.edu/pix/zen/200905_g1/
http://dust.ess.uci.edu/pix/zen/200905_g1/
http://dust.ess.uci.edu/pix/zen/200905_g1/

Harry and Marian:
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0343.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0341.jpg
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_0661.jpg

Emily:
http://dust.ess.uci.edu/pix/zen/200904_idyllwild_caribbean/img_0357.jpg
http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/09-07-29 08.31.37.jpg


http://dust.ess.uci.edu/pix/zen/200907_beaches_kiya/img_.jpg
************************************************************************
NY Times essay on what I do with this UCS theme:
For centuries science has made the world better for all of us. It's
made our food, our air, and our water safer. It's made our lives
healthier, more productive, and efficient. Science has brought us many
of the conveniences we take for granted in our day-to-day lives. 

But recently, science, and especially climate science, has become a
political football. Corporations, front groups, media pundits, and
legislators seeking to delay desperately-needed action to reduce
global warming emissions have manufactured controversies and
misrepresented the facts. 

They want to sow confusion and lull the public into a dangerous
complacency. 

Why? Because they're scared. In the last few years we've made
substantial progress toward reining in harmful emissions and
establishing a cleaner approach to our nation's energy system. 

Change is coming, and these attacks on science are simply a last ditch
attempt by polluting industries and their allies in media and Congress
to continue to operate business as usual. 
************************************************************************
books i have checked out this year and may check out again:
AUTHOR:  Klein, Stefan,
The science of happiness : how our brains make us happy--and what we can do 
Call #: 152.42 KLEIN

AUTHOR:  Lerner, Harriet Goldhor.
The dance of connection : how to talk to someone when you're mad, hurt, scar
Call #: 153.6 LERNER
Very good, especially parts that describe negative dynamics
between two otherwise positive people

AUTHOR:  Krane, Gary.
Simple fun for busy people : 333 free ways to enjoy your loved ones more in 
Call #: 306.85 KRANE

AUTHOR:  Rutter, Virginia Beane.
TITLE:   Celebrating girls : nurturing and empowering our daughters
Call #: 306.8743 RUTTER

AUTHOR:  Kendall-Tackett, Kathleen A.
The hidden feelings of motherhood : coping with stress, depression, and burn
Call #: 306.8743 KENDALL

AUTHOR:  Blauner, Susan Rose,
How I stayed alive when my brain was trying to kill me : one person's guide 
Call #: 362.287 BLAUNER
************************************************************************
florent.txt
Florent,

> However, there was this abstract of him and others, of which you're
> probably aware. Which raises the question: what shall we do with the
> work we did while you were here ? If we dont do anything soon, it will
> probably be too late.

Sorry for the late response. Thanks for Kirchstetter's abstract.
I don't know if our data can produce a paper.
I have not given up, and, in fact, just
worked on the model today.

The main impediment (besides me!) is that, 
as I showed you shortly before leaving Grenoble, when
using the so-called "fractional reflectance calibration" (which I
devised for EGU and also showed at IGAC) to convert
DUFISSS reflectance into equivalent plane parallel
reflectance, the doped snow agrees well with the model.
But when we follow the procedure we used in the DUFISS
paper (generalized to include BC effects), the agreement 
is terrible. There are some indications that our snow
had much less BC than we thought (presumably due to 
stickiness).
************************************************************************
Dear Colleagues,

Our fifth peer-reviewed netCDF Operator (NCO) related paper is out.
Let me take this opportunity to summarize for you our published NCO
research on large-scale and distributed data reduction and analysis. 
Following is the citation info., URL, and one sentence summary of
these five papers. 

Best,
Charlie

Zender, C. S., and H. J. Mangalam (2007), Scaling Properties of Common
Statistical Operators for Gridded Datasets, Int. J. High
Perform. Comput. Appl., 21(4), 485-498, doi:10.1177/1094342007083802. 
http://dust.ess.uci.edu/ppr/ppr_ZeM07_ijhpca.pdf
Documents actual and theoretical scaling of common statistical
algorithms (as implemented in NCO) with dataset size and complexity.  

Zender, C. S. (2008), Analysis of Self-describing Gridded Geoscience
Data with netCDF Operators (NCO), Environ. Modell. Softw., 23(10),
1338-1342, doi:10.1016/j.envsoft.2008.03.004. 
http://dust.ess.uci.edu/ppr/ppr_Zen08_ems.pdf
Summarizes design principles and functionality of NCO (except ncap2).

Wang, D. L., C. S. Zender, and S. F. Jenks (2007), Server-side
parallel data reduction and analysis, in Advances in Grid and
Pervasive Computing, Second International Conference, GPC 2007, Paris,
France, May 2-4, 2007, Proceedings. IEEE Lecture Notes in Computer
Science, vol. 4459, edited by C. Cerin and K.-C. Li, pp. 744-750,
Springer-Verlag, Berlin/Heidelberg,
doi:10.1007/978-3-540-72360-8_67.
http://dust.ess.uci.edu/ppr/ppr_WZJ073.pdf
Introduces SWAMP and shows results of simple tests.

Wang, D. L., C. S. Zender and S. F. Jenks (2008), Cluster Workflow
Execution of Retargeted Data Analysis Scripts, in Cluster Computing
and the Grid, 2008. CCGRID '08. 8th&nbsp;IEEE International Symposium
on. Lyon, France, 19-22 May 2008. IEEE Computer Society, 449-458,
doi:10.1109/CCGRID.2008.69.
http://dust.ess.uci.edu/ppr/ppr_WZJ081.pdf
Describes SWAMP design and quantifies performance.

Wang, D. L., C. S. Zender, and S. F. Jenks (2009), Efficient Clustered
Server-side Data Analysis Workflows using SWAMP, Earth Sci. Inform.,
2(3), 141-155, doi:10.1007/s12145-009-0021-z. 
************************************************************************
editorial.txt
An Anti-Climatic Election

Point: The candidates ignore the harsher impacts of climate change

The 2008 presidential campaign has not, as many environmentalists
had hoped, become a referendum on climate policy. 
Instead, candidates have talked about "energy policy", presumably 
because voters want to hear what actions will be taken to lower,
or at least stabilize, fuel prices.
Energy policy also includes security aspects such as considering
whether our country's economy should continue to depend on oil-rich
OPEC countries.
Yet the hand-wringing about gasoline prices neglects a fundamental
truth about our environment---that it is melting.

In 2001 scientists writing for the Intergovenmental Panel on Climate
Change (IPCC) solidified the consensus that global warming imminently
threatens the temperature patterns and sea levels that have nurtured
societal growth for thousands of years. 
Clinton and Gore had just left the White House, and with them went the 
chance of enacting federally support climate mitigation policies for
the next eight years.
The latest IPCC report was released in 2007, with plenty of time to
affect the 2008 elections 
it unequivocally strengthened the earlier warnings 

Since 2007 Earth's polar regions have melted even more quickly than
the IPCC anticipated.  
The Bush administration has received with remarkable aplomb the
the dire evidence and forecasts collected by our nation's leading
climate scientists.

So it was with high hopes that many scientists and environmentalists
welcomed the nominations of two climate realists, Obama and McCain, 
to head their party's tickets.
Obama had...
McCain had co-sponsored legislation to enact a cap-and-trade system
for carbon emissions, a first step recommended by numerous studies.

************************************************************************
Hi Florent,

Here is a draft of the dirty snow manuscript.
I would appreciate your comments and suggestions before I send
it to the others for comments before submission.

The author ordering is me, JC, you, ... only because that's what we 
did for EGU. Do you still want to be after JC?
I will also offer (but havn't yet) co-authorship to Ghislain 
and Mark Flanner, for their contributions to the modeling.

I attached the manuscript as LaTeX and PDF. 
You can make comments however you like.

Charlie
************************************************************************
Hi All,

Robynn and I have discussed and agreed to a final marital settlement.
The process has been extremely stressful and disappointing for me.
I no longer consider Robynn to be an ethical person for many reasons
(some recent ones given below). Unfortunately, this means I have no
more hope that we can restore/maintain a positive relationship.

The upshot is that I request that all of you refrain from CC'ing her
on an email that also includes me. I don't want to receive any
internet jokes, family pictures, or helpful hints, or responses to
same, on the same email that she does.

1. She charged me rent for living our house for the last 2.5 years
while she dragged her feet on a settlement. Her argument is that the
kids and I could have moved out and rented the house. 
Forget about the fact that would have further disrupted the kids'
lives, or that UCI doesn't permit rentals for more than 12 months.
Or that we had previously agreed I would stay and buy her out.

2. Remember the years she was suicidal? The times she kicked-in the
wall, destroyed household items, slammed doors, slept-in, left
messes, all in front of the kids? The months she was virtually
incapacitated by medications? I lovingly supported her. 
Never threatened or called for a restraining order despite 
professional advice from my therapist that I do that.
Now I get give her half my retirement from that period.
So much for being a faithful, loyal husband and good samaritan.

3. Her globe-trotting, millionaire parents are willing to her their
awesome house in Mexico, complete with endowed maid. That's not
enough for her. She's taking me to the cleaners for all I've got.
Maybe this way she'll never have to get a full-time job.

4. I do not know if she was visciously greedy until we separated. 
Because I gave her everything I could. Now she takes it.
It's all legal, though in my mind it's not ethical.
It's sad because now I don't think our rift will ever heal.
And that's not what I wanted for us or for the kids.
************************************************************************
Carol,

Here is my belated response to your letter of 3/2/13:

> *Your thoughts and perceptions are illuminated by the clarity of your
> hindsight.  Perhaps mine are too but this is what I believe was true at
> the time and remains so today.*

> *You were welcomed into our family with open arms.  My hope and
> anticipation was for a long and happy life together for you and Robynn.
>  Your letter indicates your belief that I plotted against you from the
> very beginning with lies and deceptions.  

I didn't say nor do I believe that you plotted against me consciously.
I did say and I do believe that you lied by omission to me.

> Nothing could be further from
> the truth.  However, in all honesty, you were not on my radar screen
> regarding family issues at that time as I was immersed in dealing with
> my son, whom I had failed and my daughter, who I was determined not to
> fail.  That does not mean that you were not considered as part of the
> family - only that my attention was necessarily focused on Randy and
> Robynn.*

> *Unless I have misunderstood, it seems that your time line of events is
> a big skewed.  Your indication is that Randy's behavior only became
> apparent to you after your marriage.  However, I recall clearly - after
> we learned about Randy's physical abuse of Karla - you saying to Robynn
> that you were not sure you could marry into a family where physical
> abuse occurred.  That would have been an excellent time for you to voice
> your concerns or questions to me.  Of course, this is hindsight speaking
> again.*

You recall correctly but you misunderstand the implications.
Before proposing to Robynn I knew Randy had been jailed for wife abuse.
So I made certain to ask what their current relation was like.
Robynn told me that he had reformed, that there was no repeated abuse
after his release, and that now they were happily running a candy
store. In other words, that it was over and done with. In the past.
Everyone deserves a second chance and he has made the most of his
thought I. Robynn knew nothing about his subsequent violence or
suicidal tendencies because apparently you concealed that from her.

The point is that what little I knew of the truth did concern me!
Robynn unwittingly allayed my concerns by repeating your falsehoods.

> *I will do my best to overlook your narcissistic comments "How did it
> feel to lie to your new son in law" and "Was Randy's death a relief in
> part because your deception could end?" It sounds as though you believe
> that I spent my days and nights contemplating the best way to deceive you.

Please do not overlook these questions---they are not rhetorical.
A wedding is all about family. Your own son was in prison.
You were gaining a son-in-law who wasn't. I'm curious how that felt. 
What did you think would happen when I found out you had deceived
Robynn and me about him? I can't believe it never occurred to you.
The first thing we do when we lie is evaluate the consequences.
What did you think the consequences would be?

> These events at this time were not about you.  I assure you that
> families with these problems have much bigger issues to think about! 

Robynn was suicidal for many years. 
You didn't live with her then. 
Getting help for her and me was absolutely important.
When I reached out to you for support I got nada.

> I stand firm on my reasons for withholding information from Robynn at that
> time.  If you felt cheated, lied to or betrayed, you were a victim of
> unintended consequences.*

Yes, I feel all three. Unintended but foreseeable on your part.
Foreseeable in that deceiving your family is not a great strategy.
Clearly your reasons for deceiving Robynn and me are precious to you.
Until you see, admit, apologize and try to make amends for deceiving
me, any relationship I could have with you would be poisoned by your
deception. 

> *When I married into an alcoholic family it never occurred to me to
> accuse my mother in law - especially after the fact - of lying to me or
> deceiving me by not telling me that their family carried the gene for
> alcoholism.  Had I made this accusation I believe she would have had
> every right to tell me that if I had questions it was my job to ask.
> If the future was predictable we would probably re-think many of our
> decisions...and ask the right questions....which would make it more
> accurate as to who the finger of blame should be pointed toward.*

Neither you nor Rodel's mother are role models for parental success.
Yet if she concealed Rodel's behavior from you then she has no right
to your respect or loyalty and you would have every right to be angry.
Rodel caused a nightmare for you, your family, and mine! Yet I suspect
the circumstances were different, and that Rodel's mother did not
deceive you, so the analogy to my situation with you is not apt.

> *As I said, you were welcomed into our family and regardless of the
> relationship between you and Robynn, we have hoped to continue a good
> relationship with you.  Obviously the feelings between you and I are
> pretty intense at the moment.  But perhaps if we are each willing to
> hear the other and get past the anger, we may be able to bridge the gap.
> I was blind sided by your letter and it has been very difficult to
> respond to it, as it was for you to write, I'm sure. There are many
> years ahead of us as we enjoy being a part of the growing up of our
> beloved girls.  You have always been a devoted father and Cliff and I
> are grateful for the opportunity to be loving and supportive
> grandparents.  It would be a blessing for all of us if we were able to
> let go of this anger in order to find a neutral common ground. Since you
> have opened this door the decision will be yours.*

************************************************************************
